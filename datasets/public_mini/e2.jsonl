{"question": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-dummy\n  namespace: default\nspec:\n  containers:\n  - name: nginx-dummy\n    image: nginx\n    volumeMounts:\n    - name: dummy\n      mountPath: /data\n    ports:\n    - containerPort: 80\n  volumes:\n  - name: dummy\n    flexVolume:\n      driver: \"k8s/dummy\"\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/flexvolume/nginx-dummy.yaml:6"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container nginx-dummy is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/flexvolume/nginx-dummy.yaml:8"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/volumes/flexvolume/nginx-dummy.yaml", "hash": "9962e93c9e45d4c8", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nginxsvc\n  labels:\n    app: nginx\nspec:\n  type: NodePort\n  ports:\n  - port: 80\n    protocol: TCP\n    name: http\n  - port: 443\n    protocol: TCP\n    name: https\n  selector:\n    app: nginx\n---\napiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: my-nginx\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      volumes:\n      - name: secret-volume\n        secret:\n          secretName: nginxsecret\n      - name: configmap-volume\n        configMap:\n          name: nginxconfigmap\n      containers:\n      - name: nginxhttps\n        image: ymqytw/nginxhttps:1.5\n        command: [\"/home/auto-reload-nginx.sh\"]\n        ports:\n        - containerPort: 443\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /index.html\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 1\n        volumeMounts:\n        - mountPath: /etc/nginx/ssl\n          name: secret-volume\n        - mountPath: /etc/nginx/conf.d\n          name: configmap-volume\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/https-nginx/nginx-app.yaml:29"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container nginxhttps is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/https-nginx/nginx-app.yaml:38"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/https-nginx/nginx-app.yaml", "hash": "96192cd566fb77a0", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "kind: ReplicationController\napiVersion: v1\nmetadata:\n  name: zeppelin-controller\nspec:\n  replicas: 1\n  selector:\n    component: zeppelin\n  template:\n    metadata:\n      labels:\n        component: zeppelin\n    spec:\n      containers:\n        - name: zeppelin\n          image: registry.k8s.io/zeppelin:v0.5.6_v1\n          ports:\n            - containerPort: 8080\n          resources:\n            requests:\n              cpu: 100m\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/spark/zeppelin-controller.yaml:13"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container zeppelin is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/spark/zeppelin-controller.yaml:15"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/spark/zeppelin-controller.yaml", "hash": "fd2d65bada615482", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: flex-ds\nspec:\n  template:\n    metadata:\n      name: flex-deploy\n      labels:\n        app: flex-deploy\n    spec:\n      containers:\n        # TODO Change to your container registry.\n        - image: \"<image_url>\"\n          name: flex-deploy\n          securityContext:\n            privileged: true\n          volumeMounts:\n            - mountPath: /flexmnt\n              name: flexvolume-mount\n      volumes:\n        - name: flexvolume-mount\n          hostPath:\n            # TODO Change to the Flexvolume plugin directory of your cluster.\n            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "no-extensions-v1beta", "severity": "medium", "msg": "Migrate using the apps/v1 API versions for the objects. Refer to https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/ for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/flexvolume/deploy/ds.yaml:11"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.privileged-container.privileged-container", "severity": "WARNING", "msg": "Container or pod is running in privileged mode. This grants the container the equivalent of root capabilities on the host machine. This can lead to container escapes, privilege escalation, and other security concerns. Remove the 'privileged' key to disable this capability.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/flexvolume/deploy/ds.yaml:14"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container flex-deploy is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/flexvolume/deploy/ds.yaml:15"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.allow-privilege-escalation.allow-privilege-escalation", "severity": "WARNING", "msg": "In Kubernetes, each pod runs in its own isolated environment with its own set of security policies. However, certain container images may contain `setuid` or `setgid` binaries that could allow an attacker to perform privilege escalation and gain access to sensitive resources. To mitigate this risk, it's recommended to add a `securityContext` to the container in the pod, with the parameter `allowPrivilegeEscalation` set to `false`. This will prevent the container from running any privileged processes and limit the impact of any potential attacks. By adding the `allowPrivilegeEscalation` parameter to your the `securityContext`, you can help to ensure that your containerized applications are more secure and less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/flexvolume/deploy/ds.yaml:16"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/volumes/flexvolume/deploy/ds.yaml", "hash": "3a48a83af8b08a83", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "# Default PostgreSQL\noutput \"default_postgres_option_group_id\" {\n  description = \"The ID of the default PostgreSQL option group (should be blank)\"\n  value       = module.default_postgres.db_option_group_id\n}\n\noutput \"default_postgres_option_group_arn\" {\n  description = \"The ARN of the default PostgreSQL option group (should be blank)\"\n  value       = module.default_postgres.db_option_group_arn\n}\n\noutput \"default_postgres_parameter_group_id\" {\n  description = \"The db parameter group id\"\n  value       = module.default_postgres.db_parameter_group_id\n}\n\noutput \"default_postgres_parameter_group_arn\" {\n  description = \"The ARN of the db parameter group\"\n  value       = module.default_postgres.db_parameter_group_arn\n}\n\n# Default MySQL\noutput \"default_mysql_option_group_id\" {\n  description = \"The ID of the default MySQL option group\"\n  value       = module.default_mysql.db_option_group_id\n}\n\noutput \"default_mysql_option_group_arn\" {\n  description = \"The ARN of the default MySQL option group\"\n  value       = module.default_mysql.db_option_group_arn\n}\n\noutput \"default_mysql_parameter_group_id\" {\n  description = \"The db parameter group id\"\n  value       = module.default_mysql.db_parameter_group_id\n}\n\noutput \"default_mysql_parameter_group_arn\" {\n  description = \"The ARN of the db parameter group\"\n  value       = module.default_mysql.db_parameter_group_arn\n}\n\n# Default MySQL name\noutput \"default_mysql_name_option_group_id\" {\n  description = \"The ID of the default MySQL option group using `name`\"\n  value       = module.default_mysql_name.db_option_group_id\n}\n\noutput \"default_mysql_name_option_group_arn\" {\n  description = \"The ARN of the default MySQL option group using `name`\"\n  value       = module.default_mysql_name.db_option_group_arn\n}\n\noutput \"default_mysql_name_parameter_group_id\" {\n  description = \"The db parameter group id\"\n  value       = module.default_mysql_name.db_parameter_group_id\n}\n\noutput \"default_mysql_name_parameter_group_arn\" {\n  description = \"The ARN of the db parameter group\"\n  value       = module.default_mysql_name.db_parameter_group_arn\n}\n\n# Default MySQL default AWS groups\noutput \"default_mysql_default_aws_option_group_id\" {\n  description = \"The ID of the default MySQL option group\"\n  value       = module.default_mysql_default_aws.db_option_group_id\n}\n\noutput \"default_mysql_default_aws_option_group_arn\" {\n  description = \"The ARN of the default MySQL option group\"\n  value       = module.default_mysql_default_aws.db_option_group_arn\n}\n\noutput \"default_mysql_default_aws_parameter_group_id\" {\n  description = \"The db parameter group id\"\n  value       = module.default_mysql_default_aws.db_parameter_group_id\n}\n\noutput \"default_mysql_default_aws_parameter_group_arn\" {\n  description = \"The ARN of the db parameter group\"\n  value       = module.default_mysql_default_aws.db_parameter_group_arn\n}\n\n# BYO MySQL\noutput \"byo_mysql_option_group_id\" {\n  description = \"The ID of the BYO MySQL option group (should be blank)\"\n  value       = module.byo_mysql.db_option_group_id\n}\n\noutput \"byo_mysql_option_group_arn\" {\n  description = \"The ARN of the BYO MySQL option group (should be blank)\"\n  value       = module.byo_mysql.db_option_group_arn\n}\n\noutput \"byo_mysql_parameter_group_id\" {\n  description = \"The db parameter group id\"\n  value       = module.byo_mysql.db_parameter_group_id\n}\n\noutput \"byo_mysql_parameter_group_arn\" {\n  description = \"The ARN of the db parameter group\"\n  value       = module.byo_mysql.db_parameter_group_arn\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-rds/examples/groups/outputs.tf", "hash": "2fb41a076c60701e", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "---\n# https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: jobs-timetolive-job\nspec:\n  ttlSecondsAfterFinished: 100\n  template:\n    spec:\n      containers:\n        - command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n          image: perl\n          name: jobs-timetolive-container\n      restartPolicy: Never\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Job/spec.ttlSecondsAfterFinished/timetolive.yaml:10"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container jobs-timetolive-container is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Job/spec.ttlSecondsAfterFinished/timetolive.yaml:14"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/Job/spec.ttlSecondsAfterFinished/timetolive.yaml", "hash": "20aa0c71974a489e", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "kind: ReplicationController\napiVersion: v1\nmetadata:\n  name: spark-ui-proxy-controller\nspec:\n  replicas: 1\n  selector:\n    component: spark-ui-proxy\n  template:\n    metadata:\n      labels:\n        component: spark-ui-proxy\n    spec:\n      containers:\n        - name: spark-ui-proxy\n          image: elsonrodriguez/spark-ui-proxy:1.0\n          ports:\n            - containerPort: 80\n          resources:\n            requests:\n              cpu: 100m\n          args:\n            - spark-master:8080\n          livenessProbe:\n              httpGet:\n                path: /\n                port: 80\n              initialDelaySeconds: 120\n              timeoutSeconds: 5\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/spark/spark-ui-proxy-controller.yaml:13"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container spark-ui-proxy is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/spark/spark-ui-proxy-controller.yaml:15"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/spark/spark-ui-proxy-controller.yaml", "hash": "d2728421a6c8863f", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nfs-server\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: nfs-server\n  template:\n    metadata:\n      labels:\n        role: nfs-server\n    spec:\n      containers:\n      - name: nfs-server\n        image: registry.k8s.io/volume-nfs:0.8\n        ports:\n          - name: nfs\n            containerPort: 2049\n          - name: mountd\n            containerPort: 20048\n          - name: rpcbind\n            containerPort: 111\n        securityContext:\n          privileged: true\n        volumeMounts:\n          - mountPath: /exports\n            name: mypvc\n      volumes:\n        - name: mypvc\n          persistentVolumeClaim:\n            claimName: nfs-pv-provisioning-demo\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "privilege-escalation-container", "severity": "medium", "msg": "Ensure containers do not allow privilege escalation by setting allowPrivilegeEscalation=false, privileged=false and removing CAP_SYS_ADMIN capability. See https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for more details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "privileged-container", "severity": "medium", "msg": "Do not run your container as privileged unless it is required.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/nfs/nfs-server-deployment.yaml:14"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.privileged-container.privileged-container", "severity": "WARNING", "msg": "Container or pod is running in privileged mode. This grants the container the equivalent of root capabilities on the host machine. This can lead to container escapes, privilege escalation, and other security concerns. Remove the 'privileged' key to disable this capability.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/nfs/nfs-server-deployment.yaml:16"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container nfs-server is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/nfs/nfs-server-deployment.yaml:16"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.allow-privilege-escalation.allow-privilege-escalation", "severity": "WARNING", "msg": "In Kubernetes, each pod runs in its own isolated environment with its own set of security policies. However, certain container images may contain `setuid` or `setgid` binaries that could allow an attacker to perform privilege escalation and gain access to sensitive resources. To mitigate this risk, it's recommended to add a `securityContext` to the container in the pod, with the parameter `allowPrivilegeEscalation` set to `false`. This will prevent the container from running any privileged processes and limit the impact of any potential attacks. By adding the `allowPrivilegeEscalation` parameter to your the `securityContext`, you can help to ensure that your containerized applications are more secure and less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/nfs/nfs-server-deployment.yaml:25"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/volumes/nfs/nfs-server-deployment.yaml", "hash": "2f9e3b88c2d377f9", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "---\n# https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-dns-debug\nspec:\n  containers:\n    - command:\n        - sleep\n        - \"3600\"\n      image: gcr.io/kubernetes-e2e-test-images/dnsutils:1.3\n      name: dnsutils\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Pod/dns-debug.yaml:7"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container dnsutils is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Pod/dns-debug.yaml:13"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/Pod/dns-debug.yaml", "hash": "31b336c69d8f1587", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "variable \"create\" {\n  description = \"Determines whether to create Fargate profile or not\"\n  type        = bool\n  default     = true\n  nullable    = false\n}\n\nvariable \"tags\" {\n  description = \"A map of tags to add to all resources\"\n  type        = map(string)\n  default     = {}\n  nullable    = false\n}\n\nvariable \"region\" {\n  description = \"Region where the resource(s) will be managed. Defaults to the Region set in the provider configuration\"\n  type        = string\n  default     = null\n}\n\nvariable \"partition\" {\n  description = \"The AWS partition - pass through value to reduce number of GET requests from data sources\"\n  type        = string\n  default     = \"\"\n}\n\nvariable \"account_id\" {\n  description = \"The AWS account ID - pass through value to reduce number of GET requests from data sources\"\n  type        = string\n  default     = \"\"\n}\n\n################################################################################\n# IAM Role\n################################################################################\n\nvariable \"create_iam_role\" {\n  description = \"Determines whether an IAM role is created or to use an existing IAM role\"\n  type        = bool\n  default     = true\n  nullable    = false\n}\n\nvariable \"cluster_ip_family\" {\n  description = \"The IP family used to assign Kubernetes pod and service addresses. Valid values are `ipv4` (default) and `ipv6`\"\n  type        = string\n  default     = \"ipv4\"\n}\n\nvariable \"iam_role_arn\" {\n  description = \"Existing IAM role ARN for the Fargate profile. Required if `create_iam_role` is set to `false`\"\n  type        = string\n  default     = null\n}\n\nvariable \"iam_role_name\" {\n  description = \"Name to use on IAM role created\"\n  type        = string\n  default     = \"\"\n}\n\nvariable \"iam_role_use_name_prefix\" {\n  description = \"Determines whether the IAM role name (`iam_role_name`) is used as a prefix\"\n  type        = bool\n  default     = true\n  nullable    = false\n}\n\nvariable \"iam_role_path\" {\n  description = \"IAM role path\"\n  type        = string\n  default     = null\n}\n\nvariable \"iam_role_description\" {\n  description = \"Description of the role\"\n  type        = string\n  default     = \"Fargate profile IAM role\"\n  nullable    = false\n}\n\nvariable \"iam_role_permissions_boundary\" {\n  description = \"ARN of the policy that is used to set the permissions boundary for the IAM role\"\n  type        = string\n  default     = null\n}\n\nvariable \"iam_role_attach_cni_policy\" {\n  description = \"Whether to attach the `AmazonEKS_CNI_Policy`/`AmazonEKS_CNI_IPv6_Policy` IAM policy to the IAM IAM role. WARNING: If set `false` the permissions must be assigned to the `aws-node` DaemonSet pods via another method or nodes will not be able to join the cluster\"\n  type        = bool\n  default     = true\n  nullable    = false\n}\n\nvariable \"iam_role_additional_policies\" {\n  description = \"Additional policies to be added to the IAM role\"\n  type        = map(string)\n  default     = {}\n  nullable    = false\n}\n\nvariable \"iam_role_tags\" {\n  description = \"A map of additional tags to add to the IAM role created\"\n  type        = map(string)\n  default     = {}\n  nullable    = false\n}\n\n################################################################################\n# IAM Role Policy\n################################################################################\n\nvariable \"create_iam_role_policy\" {\n  description = \"Determines whether an IAM role policy is created or not\"\n  type        = bool\n  default     = true\n  nullable    = false\n}\n\nvariable \"iam_role_policy_statements\" {\n  description = \"A list of IAM policy [statements](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document#statement) - used for adding specific IAM permissions as needed\"\n  type = list(object({\n    sid           = optional(string)\n    actions       = optional(list(string))\n    not_actions   = optional(list(string))\n    effect        = optional(string)\n    resources     = optional(list(string))\n    not_resources = optional(list(string))\n    principals = optional(list(object({\n      type        = string\n      identifiers = list(string)\n    })))\n    not_principals = optional(list(object({\n      type        = string\n      identifiers = list(string)\n    })))\n    condition = optional(list(object({\n      test     = string\n      values   = list(string)\n      variable = string\n    })))\n  }))\n  default = null\n}\n\n################################################################################\n# Fargate Profile\n################################################################################\n\nvariable \"cluster_name\" {\n  description = \"Name of the EKS cluster\"\n  type        = string\n  default     = \"\"\n}\n\nvariable \"name\" {\n  description = \"Name of the EKS Fargate Profile\"\n  type        = string\n  default     = \"\"\n  nullable    = false\n}\n\nvariable \"subnet_ids\" {\n  description = \"A list of subnet IDs for the EKS Fargate Profile\"\n  type        = list(string)\n  default     = []\n  nullable    = false\n}\n\nvariable \"selectors\" {\n  description = \"Configuration block(s) for selecting Kubernetes Pods to execute with this Fargate Profile\"\n  type = list(object({\n    labels    = optional(map(string))\n    namespace = string\n  }))\n  default = null\n}\n\nvariable \"timeouts\" {\n  description = \"Create and delete timeout configurations for the Fargate Profile\"\n  type = object({\n    create = optional(string)\n    delete = optional(string)\n  })\n  default = null\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-eks/modules/fargate-profile/variables.tf", "hash": "2b6915659468a253", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "apiVersion: apps/v1 #  for k8s versions before 1.9.0 use apps/v1beta2  and before 1.8.0 use extensions/v1beta1\nkind: StatefulSet\nmetadata:\n  name: minio\n  labels:\n     app: minio\nspec:\n  selector:\n    matchLabels:\n      app: minio\n  serviceName: minio\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        app: minio\n    spec:\n      containers:\n      - name: minio\n        env:\n        - name: MINIO_ACCESS_KEY\n          value: \"minio\"\n        - name: MINIO_SECRET_KEY\n          value: \"minio123\"\n        image: minio/minio:latest\n        args:\n        - server\n        - http://minio-0.minio.default.svc.cluster.local/data\n        - http://minio-1.minio.default.svc.cluster.local/data\n        - http://minio-2.minio.default.svc.cluster.local/data\n        - http://minio-3.minio.default.svc.cluster.local/data\n        ports:\n        - containerPort: 9000\n          hostPort: 9000\n        # These volume mounts are persistent. Each pod in the StatefulSet\n        # gets a volume mounted based on this field.\n        volumeMounts:\n        - name: data\n          mountPath: /data\n  # These are converted to volume claims by the controller\n  # and mounted at the paths mentioned above.\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n        - ReadWriteOnce\n      storageClassName: standard\n      resources:\n        requests:\n          storage: 10Gi\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "env-var-secret", "severity": "medium", "msg": "Do not use raw secrets in environment variables. Instead, either mount the secret as a file or use a secretKeyRef. Refer to https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-anti-affinity", "severity": "medium", "msg": "Specify anti-affinity in your pod specification to ensure that the orchestrator attempts to schedule replicas on different nodes. Using podAntiAffinity, specify a labelSelector that matches pods for the deployment, and set the topologyKey to kubernetes.io/hostname. Refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/storage/minio/minio-distributed-statefulset.yaml:17"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container minio is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/storage/minio/minio-distributed-statefulset.yaml:19"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/storage/minio/minio-distributed-statefulset.yaml", "hash": "638dba0072a2ce36", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "# Evaluates error rates for each endpoint in the pool against the settings here, ejecting them if they cross the thresholds.\n---\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: circuit-breaker\nspec:\n  host: service-a\n  trafficPolicy:\n    outlierDetection:\n      consecutive5xxErrors: 7  # Default 5\n      interval: 5m  # Interval over which errors are counted and compared to the threshold. This is a periodic check, not a rolling one.\n      baseEjectionTime: 10s  # Initial period for which the endpoint is ejected from the endpoint pool. Repeated ejections are longer each time. Default 30s\n      maxEjectionPercent: 50  # Max % of endpoints that can ejected from the endpoint pool. Default 10\n      minHealthPercent: 50  # Min % of endpoints in the endpoint pool that must be healthy for circuit-breaking to activate. Default 0\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/Istio/DestinationRule/circuit-breaker.yaml", "hash": "e66a2ff698abe7ee", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "terraform {\n  required_version = \">= 1.5.7\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \">= 6.15\"\n    }\n  }\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-eks/tests/eks-managed-node-group/versions.tf", "hash": "03f4815f943d3da8", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "apiVersion: v1\nkind: Pod\nmetadata:\n name: azure-2\nspec:\n containers:\n  - image: kubernetes/pause\n    name: azure-2\n    volumeMounts:\n      - name: azure\n        mountPath: /mnt/azure\n volumes:\n  - name: azure\n    persistentVolumeClaim:\n      claimName: sample-storage-claim\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/azure_file/azure-2.yaml:5"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container azure-2 is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/azure_file/azure-2.yaml:8"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/volumes/azure_file/azure-2.yaml", "hash": "0e30779a197da6d8", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "data \"aws_region\" \"current\" {\n  count = var.create ? 1 : 0\n\n  region = var.region\n}\ndata \"aws_partition\" \"current\" {\n  count = var.create ? 1 : 0\n}\ndata \"aws_caller_identity\" \"current\" {\n  count = var.create ? 1 : 0\n}\n\nlocals {\n  account_id = try(data.aws_caller_identity.current[0].account_id, \"\")\n  dns_suffix = try(data.aws_partition.current[0].dns_suffix, \"\")\n  partition  = try(data.aws_partition.current[0].partition, \"\")\n  region     = try(data.aws_region.current[0].region, \"\")\n}\n\n################################################################################\n# Karpenter controller IAM Role\n################################################################################\n\nlocals {\n  create_iam_role = var.create && var.create_iam_role\n}\n\ndata \"aws_iam_policy_document\" \"controller_assume_role\" {\n  count = local.create_iam_role ? 1 : 0\n\n  override_policy_documents = var.iam_role_override_assume_policy_documents\n  source_policy_documents   = var.iam_role_source_assume_policy_documents\n\n  # Pod Identity\n  statement {\n    sid = \"PodIdentity\"\n    actions = [\n      \"sts:AssumeRole\",\n      \"sts:TagSession\",\n    ]\n\n    principals {\n      type        = \"Service\"\n      identifiers = [\"pods.eks.amazonaws.com\"]\n    }\n  }\n}\n\nresource \"aws_iam_role\" \"controller\" {\n  count = local.create_iam_role ? 1 : 0\n\n  name        = var.iam_role_use_name_prefix ? null : var.iam_role_name\n  name_prefix = var.iam_role_use_name_prefix ? \"${var.iam_role_name}-\" : null\n  path        = var.iam_role_path\n  description = var.iam_role_description\n\n  assume_role_policy    = data.aws_iam_policy_document.controller_assume_role[0].json\n  max_session_duration  = var.iam_role_max_session_duration\n  permissions_boundary  = var.iam_role_permissions_boundary_arn\n  force_detach_policies = true\n\n  tags = merge(var.tags, var.iam_role_tags)\n}\n\nresource \"aws_iam_policy\" \"controller\" {\n  count = local.create_iam_role ? 1 : 0\n\n  name        = var.iam_policy_use_name_prefix ? null : var.iam_policy_name\n  name_prefix = var.iam_policy_use_name_prefix ? \"${var.iam_policy_name}-\" : null\n  path        = var.iam_policy_path\n  description = var.iam_policy_description\n  policy      = data.aws_iam_policy_document.controller[0].json\n\n  tags = var.tags\n}\n\nresource \"aws_iam_role_policy_attachment\" \"controller\" {\n  count = local.create_iam_role ? 1 : 0\n\n  role       = aws_iam_role.controller[0].name\n  policy_arn = aws_iam_policy.controller[0].arn\n}\n\nresource \"aws_iam_role_policy_attachment\" \"controller_additional\" {\n  for_each = { for k, v in var.iam_role_policies : k => v if local.create_iam_role }\n\n  role       = aws_iam_role.controller[0].name\n  policy_arn = each.value\n}\n\n################################################################################\n# Pod Identity Association\n################################################################################\n\nresource \"aws_eks_pod_identity_association\" \"karpenter\" {\n  count = local.create_iam_role && var.create_pod_identity_association ? 1 : 0\n\n  region = var.region\n\n  cluster_name    = var.cluster_name\n  namespace       = var.namespace\n  service_account = var.service_account\n  role_arn        = aws_iam_role.controller[0].arn\n\n  tags = var.tags\n}\n\n################################################################################\n# Node Termination Queue\n################################################################################\n\nlocals {\n  enable_spot_termination = var.create && var.enable_spot_termination\n\n  queue_name = coalesce(var.queue_name, \"Karpenter-${var.cluster_name}\")\n}\n\nresource \"aws_sqs_queue\" \"this\" {\n  count = local.enable_spot_termination ? 1 : 0\n\n  region = var.region\n\n  name                              = local.queue_name\n  message_retention_seconds         = 300\n  sqs_managed_sse_enabled           = var.queue_managed_sse_enabled ? var.queue_managed_sse_enabled : null\n  kms_master_key_id                 = var.queue_kms_master_key_id\n  kms_data_key_reuse_period_seconds = var.queue_kms_data_key_reuse_period_seconds\n\n  tags = var.tags\n}\n\ndata \"aws_iam_policy_document\" \"queue\" {\n  count = local.enable_spot_termination ? 1 : 0\n\n  statement {\n    sid       = \"SqsWrite\"\n    actions   = [\"sqs:SendMessage\"]\n    resources = [aws_sqs_queue.this[0].arn]\n\n    principals {\n      type = \"Service\"\n      identifiers = [\n        \"events.amazonaws.com\",\n        \"sqs.amazonaws.com\",\n      ]\n    }\n  }\n  statement {\n    sid    = \"DenyHTTP\"\n    effect = \"Deny\"\n    actions = [\n      \"sqs:*\"\n    ]\n    resources = [aws_sqs_queue.this[0].arn]\n    condition {\n      test     = \"Bool\"\n      variable = \"aws:SecureTransport\"\n      values = [\n        \"false\"\n      ]\n    }\n    principals {\n      type = \"*\"\n      identifiers = [\n        \"*\"\n      ]\n    }\n  }\n}\n\nresource \"aws_sqs_queue_policy\" \"this\" {\n  count = local.enable_spot_termination ? 1 : 0\n\n  region = var.region\n\n  queue_url = aws_sqs_queue.this[0].url\n  policy    = data.aws_iam_policy_document.queue[0].json\n}\n\n################################################################################\n# Node Termination Event Rules\n################################################################################\n\nlocals {\n  events = {\n    health_event = {\n      name        = \"HealthEvent\"\n      description = \"Karpenter interrupt - AWS health event\"\n      event_pattern = {\n        source      = [\"aws.health\"]\n        detail-type = [\"AWS Health Event\"]\n      }\n    }\n    spot_interrupt = {\n      name        = \"SpotInterrupt\"\n      description = \"Karpenter interrupt - EC2 spot instance interruption warning\"\n      event_pattern = {\n        source      = [\"aws.ec2\"]\n        detail-type = [\"EC2 Spot Instance Interruption Warning\"]\n      }\n    }\n    instance_rebalance = {\n      name        = \"InstanceRebalance\"\n      description = \"Karpenter interrupt - EC2 instance rebalance recommendation\"\n      event_pattern = {\n        source      = [\"aws.ec2\"]\n        detail-type = [\"EC2 Instance Rebalance Recommendation\"]\n      }\n    }\n    instance_state_change = {\n      name        = \"InstanceStateChange\"\n      description = \"Karpenter interrupt - EC2 instance state-change notification\"\n      event_pattern = {\n        source      = [\"aws.ec2\"]\n        detail-type = [\"EC2 Instance State-change Notification\"]\n      }\n    }\n  }\n}\n\nresource \"aws_cloudwatch_event_rule\" \"this\" {\n  for_each = { for k, v in local.events : k => v if local.enable_spot_termination }\n\n  region = var.region\n\n  name_prefix   = \"${var.rule_name_prefix}${each.value.name}-\"\n  description   = each.value.description\n  event_pattern = jsonencode(each.value.event_pattern)\n\n  tags = merge(\n    { \"ClusterName\" : var.cluster_name },\n    var.tags,\n  )\n}\n\nresource \"aws_cloudwatch_event_target\" \"this\" {\n  for_each = { for k, v in local.events : k => v if local.enable_spot_termination }\n\n  region = var.region\n\n  rule      = aws_cloudwatch_event_rule.this[each.key].name\n  target_id = \"KarpenterInterruptionQueueTarget\"\n  arn       = aws_sqs_queue.this[0].arn\n}\n\n################################################################################\n# Node IAM Role\n# This is used by the nodes launched by Karpenter\n################################################################################\n\nlocals {\n  create_node_iam_role = var.create && var.create_node_iam_role\n\n  node_iam_role_name          = coalesce(var.node_iam_role_name, \"Karpenter-${var.cluster_name}\")\n  node_iam_role_policy_prefix = \"arn:${local.partition}:iam::aws:policy\"\n\n  ipv4_cni_policy = { for k, v in {\n    AmazonEKS_CNI_Policy = \"${local.node_iam_role_policy_prefix}/AmazonEKS_CNI_Policy\"\n  } : k => v if var.node_iam_role_attach_cni_policy && var.cluster_ip_family == \"ipv4\" }\n  ipv6_cni_policy = { for k, v in {\n    AmazonEKS_CNI_IPv6_Policy = \"arn:${local.partition}:iam::${local.account_id}:policy/AmazonEKS_CNI_IPv6_Policy\"\n  } : k => v if var.node_iam_role_attach_cni_policy && var.cluster_ip_family == \"ipv6\" }\n}\n\ndata \"aws_iam_policy_document\" \"node_assume_role\" {\n  count = local.create_node_iam_role ? 1 : 0\n\n  statement {\n    sid     = \"EKSNodeAssumeRole\"\n    actions = [\"sts:AssumeRole\"]\n\n    principals {\n      type        = \"Service\"\n      identifiers = [\"ec2.${local.dns_suffix}\"]\n    }\n  }\n}\n\nresource \"aws_iam_role\" \"node\" {\n  count = local.create_node_iam_role ? 1 : 0\n\n  name        = var.node_iam_role_use_name_prefix ? null : local.node_iam_role_name\n  name_prefix = var.node_iam_role_use_name_prefix ? \"${local.node_iam_role_name}-\" : null\n  path        = var.node_iam_role_path\n  description = var.node_iam_role_description\n\n  assume_role_policy    = data.aws_iam_policy_document.node_assume_role[0].json\n  max_session_duration  = var.node_iam_role_max_session_duration\n  permissions_boundary  = var.node_iam_role_permissions_boundary\n  force_detach_policies = true\n\n  tags = merge(var.tags, var.node_iam_role_tags)\n}\n\n# Policies attached ref https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/eks_node_group\nresource \"aws_iam_role_policy_attachment\" \"node\" {\n  for_each = { for k, v in merge(\n    {\n      AmazonEKSWorkerNodePolicy          = \"${local.node_iam_role_policy_prefix}/AmazonEKSWorkerNodePolicy\"\n      AmazonEC2ContainerRegistryPullOnly = \"${local.node_iam_role_policy_prefix}/AmazonEC2ContainerRegistryPullOnly\"\n    },\n    local.ipv4_cni_policy,\n    local.ipv6_cni_policy\n  ) : k => v if local.create_node_iam_role }\n\n  policy_arn = each.value\n  role       = aws_iam_role.node[0].name\n}\n\nresource \"aws_iam_role_policy_attachment\" \"node_additional\" {\n  for_each = { for k, v in var.node_iam_role_additional_policies : k => v if local.create_node_iam_role }\n\n  policy_arn = each.value\n  role       = aws_iam_role.node[0].name\n}\n\n################################################################################\n# Access Entry\n################################################################################\n\nresource \"aws_eks_access_entry\" \"node\" {\n  count = var.create && var.create_access_entry ? 1 : 0\n\n  region = var.region\n\n  cluster_name  = var.cluster_name\n  principal_arn = var.create_node_iam_role ? aws_iam_role.node[0].arn : var.node_iam_role_arn\n  type          = var.access_entry_type\n\n  tags = var.tags\n\n  depends_on = [\n    # If we try to add this too quickly, it fails. So .... we wait\n    aws_sqs_queue_policy.this,\n  ]\n}\n\n################################################################################\n# Node IAM Instance Profile\n# This is used by the nodes launched by Karpenter\n# Starting with Karpenter 0.32 this is no longer required as Karpenter will\n# create the Instance Profile\n################################################################################\n\nlocals {\n  external_role_name = try(replace(var.node_iam_role_arn, \"/^(.*role/)/\", \"\"), null)\n}\n\nresource \"aws_iam_instance_profile\" \"this\" {\n  count = var.create && var.create_instance_profile ? 1 : 0\n\n  name        = var.node_iam_role_use_name_prefix ? null : local.node_iam_role_name\n  name_prefix = var.node_iam_role_use_name_prefix ? \"${local.node_iam_role_name}-\" : null\n  path        = var.node_iam_role_path\n  role        = var.create_node_iam_role ? aws_iam_role.node[0].name : local.external_role_name\n\n  tags = merge(var.tags, var.node_iam_role_tags)\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-eks/modules/karpenter/main.tf", "hash": "ee50cbfd2475c256", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "apiVersion: apps/v1 #  for k8s versions before 1.9.0 use apps/v1beta2  and before 1.8.0 use extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: guestbook\n      tier: frontend\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: guestbook\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google-samples/gb-frontend:v5\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n          # If your cluster config does not include a dns service, then to\n          # instead access environment variables to find service host\n          # info, comment out the 'value: dns' line above, and uncomment the\n          # line below:\n          # value: env\n        ports:\n        - containerPort: 80\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "no-anti-affinity", "severity": "medium", "msg": "Specify anti-affinity in your pod specification to ensure that the orchestrator attempts to schedule replicas on different nodes. Using podAntiAffinity, specify a labelSelector that matches pods for the deployment, and set the topologyKey to kubernetes.io/hostname. Refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/web/guestbook/frontend-deployment.yaml:16"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container php-redis is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/web/guestbook/frontend-deployment.yaml:18"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/web/guestbook/frontend-deployment.yaml", "hash": "e59514404887fd30", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: pv-dd-shared-ssd-5g\n  annotations:\n    volume.beta.kubernetes.io/storage-class: sharedssd\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/volumes/azure_disk/claim/blob-based-disk/shared-ssd/pvc-on-shared-ssd.yaml", "hash": "98946e416c8aec9c", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: v1\nkind: Service\nmetadata:\n  name: elasticsearch-discovery\n  labels:\n    component: elasticsearch\n    role: master\nspec:\n  selector:\n    component: elasticsearch\n    role: master\n  ports:\n  - name: transport\n    port: 9300\n    protocol: TCP\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "dangling-service", "severity": "medium", "msg": "Confirm that your service's selector correctly matches the labels on one of your deployments.", "loc": ""}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/elasticsearch/production_cluster/es-discovery-svc.yaml", "hash": "0b77067f333331fc", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "terraform {\n  required_version = \">= 1.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \">= 5.92\"\n    }\n  }\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-rds/modules/db_instance_role_association/versions.tf", "hash": "6212b8f03764bcee", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "kind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: thin-disk\nprovisioner: kubernetes.io/vsphere-volume\nparameters:\n    diskformat: thin\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/volumes/vsphere/simple-storageclass.yaml", "hash": "53c6068ed2bf2f19", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: rbac-role-role\nrules:\n  - apiGroups: [\"\"]  # \"\" indicates the core API group\n    resources: [\"pods\"]\n    verbs: [\"get\", \"watch\", \"list\"]\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/Role/role.yaml", "hash": "052f136af0adaadc", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: redis\n    role: master\n  name: test-storageos-redis-pvc\nspec:\n  containers:\n    - name: master\n      image: kubernetes/redis:v1\n      env:\n        - name: MASTER\n          value: \"true\"\n      ports:\n        - containerPort: 6379\n      resources:\n        limits:\n          cpu: \"0.1\"\n      volumeMounts:\n        - mountPath: /redis-master-data\n          name: redis-data\n  volumes:\n    - name: redis-data\n      persistentVolumeClaim:\n        claimName: pvc0001\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/storageos/storageos-pvcpod.yaml:8"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container master is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/storageos/storageos-pvcpod.yaml:10"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/volumes/storageos/storageos-pvcpod.yaml", "hash": "7837744c7ea4796c", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "locals {\n\n}\n\n################################################################################\n# Cluster\n################################################################################\n\noutput \"cluster_arn\" {\n  description = \"The Amazon Resource Name (ARN) of the cluster\"\n  value       = try(aws_eks_cluster.this[0].arn, null)\n\n  depends_on = [\n    aws_eks_access_entry.this,\n    aws_eks_access_policy_association.this,\n  ]\n}\n\noutput \"cluster_certificate_authority_data\" {\n  description = \"Base64 encoded certificate data required to communicate with the cluster\"\n  value       = try(aws_eks_cluster.this[0].certificate_authority[0].data, null)\n\n  depends_on = [\n    aws_eks_access_entry.this,\n    aws_eks_access_policy_association.this,\n  ]\n}\n\noutput \"cluster_endpoint\" {\n  description = \"Endpoint for your Kubernetes API server\"\n  value       = try(aws_eks_cluster.this[0].endpoint, null)\n\n  depends_on = [\n    aws_eks_access_entry.this,\n    aws_eks_access_policy_association.this,\n  ]\n}\n\noutput \"cluster_id\" {\n  description = \"The ID of the EKS cluster. Note: currently a value is returned only for local EKS clusters created on Outposts\"\n  value       = try(aws_eks_cluster.this[0].cluster_id, \"\")\n}\n\noutput \"cluster_name\" {\n  description = \"The name of the EKS cluster\"\n  value       = try(aws_eks_cluster.this[0].name, \"\")\n\n  depends_on = [\n    aws_eks_access_entry.this,\n    aws_eks_access_policy_association.this,\n  ]\n}\n\noutput \"cluster_oidc_issuer_url\" {\n  description = \"The URL on the EKS cluster for the OpenID Connect identity provider\"\n  value       = try(aws_eks_cluster.this[0].identity[0].oidc[0].issuer, null)\n}\n\noutput \"cluster_dualstack_oidc_issuer_url\" {\n  description = \"Dual-stack compatible URL on the EKS cluster for the OpenID Connect identity provider\"\n  # https://github.com/aws/containers-roadmap/issues/2038#issuecomment-2278450601\n  value = try(replace(replace(aws_eks_cluster.this[0].identity[0].oidc[0].issuer, \"https://oidc.eks.\", \"https://oidc-eks.\"), \".amazonaws.com/\", \".api.aws/\"), null)\n}\n\noutput \"cluster_version\" {\n  description = \"The Kubernetes version for the cluster\"\n  value       = try(aws_eks_cluster.this[0].version, null)\n}\n\noutput \"cluster_platform_version\" {\n  description = \"Platform version for the cluster\"\n  value       = try(aws_eks_cluster.this[0].platform_version, null)\n}\n\noutput \"cluster_status\" {\n  description = \"Status of the EKS cluster. One of `CREATING`, `ACTIVE`, `DELETING`, `FAILED`\"\n  value       = try(aws_eks_cluster.this[0].status, null)\n}\n\noutput \"cluster_primary_security_group_id\" {\n  description = \"Cluster security group that was created by Amazon EKS for the cluster. Managed node groups use this security group for control-plane-to-data-plane communication. Referred to as 'Cluster security group' in the EKS console\"\n  value       = try(aws_eks_cluster.this[0].vpc_config[0].cluster_security_group_id, null)\n}\n\noutput \"cluster_service_cidr\" {\n  description = \"The CIDR block where Kubernetes pod and service IP addresses are assigned from\"\n  value       = var.ip_family == \"ipv6\" ? try(aws_eks_cluster.this[0].kubernetes_network_config[0].service_ipv6_cidr, null) : try(aws_eks_cluster.this[0].kubernetes_network_config[0].service_ipv4_cidr, null)\n}\n\noutput \"cluster_ip_family\" {\n  description = \"The IP family used by the cluster (e.g. `ipv4` or `ipv6`)\"\n  value       = try(aws_eks_cluster.this[0].kubernetes_network_config[0].ip_family, null)\n}\n\n################################################################################\n# Access Entry\n################################################################################\n\noutput \"access_entries\" {\n  description = \"Map of access entries created and their attributes\"\n  value       = aws_eks_access_entry.this\n}\n\noutput \"access_policy_associations\" {\n  description = \"Map of eks cluster access policy associations created and their attributes\"\n  value       = aws_eks_access_policy_association.this\n}\n\n################################################################################\n# KMS Key\n################################################################################\n\noutput \"kms_key_arn\" {\n  description = \"The Amazon Resource Name (ARN) of the key\"\n  value       = module.kms.key_arn\n}\n\noutput \"kms_key_id\" {\n  description = \"The globally unique identifier for the key\"\n  value       = module.kms.key_id\n}\n\noutput \"kms_key_policy\" {\n  description = \"The IAM resource policy set on the key\"\n  value       = module.kms.key_policy\n}\n\n################################################################################\n# Cluster Security Group\n################################################################################\n\noutput \"cluster_security_group_arn\" {\n  description = \"Amazon Resource Name (ARN) of the cluster security group\"\n  value       = try(aws_security_group.cluster[0].arn, null)\n}\n\noutput \"cluster_security_group_id\" {\n  description = \"ID of the cluster security group\"\n  value       = try(aws_security_group.cluster[0].id, null)\n}\n\n################################################################################\n# Node Security Group\n################################################################################\n\noutput \"node_security_group_arn\" {\n  description = \"Amazon Resource Name (ARN) of the node shared security group\"\n  value       = try(aws_security_group.node[0].arn, null)\n}\n\noutput \"node_security_group_id\" {\n  description = \"ID of the node shared security group\"\n  value       = try(aws_security_group.node[0].id, null)\n}\n\n################################################################################\n# IRSA\n################################################################################\n\noutput \"oidc_provider\" {\n  description = \"The OpenID Connect identity provider (issuer URL without leading `https://`)\"\n  value       = try(replace(aws_eks_cluster.this[0].identity[0].oidc[0].issuer, \"https://\", \"\"), null)\n}\n\noutput \"oidc_provider_arn\" {\n  description = \"The ARN of the OIDC Provider if `enable_irsa = true`\"\n  value       = try(aws_iam_openid_connect_provider.oidc_provider[0].arn, null)\n}\n\noutput \"cluster_tls_certificate_sha1_fingerprint\" {\n  description = \"The SHA1 fingerprint of the public key of the cluster's certificate\"\n  value       = try(data.tls_certificate.this[0].certificates[0].sha1_fingerprint, null)\n}\n\n################################################################################\n# IAM Role\n################################################################################\n\noutput \"cluster_iam_role_name\" {\n  description = \"Cluster IAM role name\"\n  value       = try(aws_iam_role.this[0].name, null)\n}\n\noutput \"cluster_iam_role_arn\" {\n  description = \"Cluster IAM role ARN\"\n  value       = try(aws_iam_role.this[0].arn, null)\n}\n\noutput \"cluster_iam_role_unique_id\" {\n  description = \"Stable and unique string identifying the IAM role\"\n  value       = try(aws_iam_role.this[0].unique_id, null)\n}\n\n################################################################################\n# EKS Auto Node IAM Role\n################################################################################\n\noutput \"node_iam_role_name\" {\n  description = \"EKS Auto node IAM role name\"\n  value       = try(aws_iam_role.eks_auto[0].name, null)\n}\n\noutput \"node_iam_role_arn\" {\n  description = \"EKS Auto node IAM role ARN\"\n  value       = try(aws_iam_role.eks_auto[0].arn, null)\n}\n\noutput \"node_iam_role_unique_id\" {\n  description = \"Stable and unique string identifying the IAM role\"\n  value       = try(aws_iam_role.eks_auto[0].unique_id, null)\n}\n\n################################################################################\n# EKS Addons\n################################################################################\n\noutput \"cluster_addons\" {\n  description = \"Map of attribute maps for all EKS cluster addons enabled\"\n  value       = merge(aws_eks_addon.this, aws_eks_addon.before_compute)\n}\n\n################################################################################\n# EKS Identity Provider\n################################################################################\n\noutput \"cluster_identity_providers\" {\n  description = \"Map of attribute maps for all EKS identity providers enabled\"\n  value       = aws_eks_identity_provider_config.this\n}\n\n################################################################################\n# CloudWatch Log Group\n################################################################################\n\noutput \"cloudwatch_log_group_name\" {\n  description = \"Name of cloudwatch log group created\"\n  value       = try(aws_cloudwatch_log_group.this[0].name, null)\n}\n\noutput \"cloudwatch_log_group_arn\" {\n  description = \"Arn of cloudwatch log group created\"\n  value       = try(aws_cloudwatch_log_group.this[0].arn, null)\n}\n\n################################################################################\n# Fargate Profile\n################################################################################\n\noutput \"fargate_profiles\" {\n  description = \"Map of attribute maps for all EKS Fargate Profiles created\"\n  value       = module.fargate_profile\n}\n\n################################################################################\n# EKS Managed Node Group\n################################################################################\n\noutput \"eks_managed_node_groups\" {\n  description = \"Map of attribute maps for all EKS managed node groups created\"\n  value       = module.eks_managed_node_group\n}\n\noutput \"eks_managed_node_groups_autoscaling_group_names\" {\n  description = \"List of the autoscaling group names created by EKS managed node groups\"\n  value       = compact(flatten([for group in module.eks_managed_node_group : group.node_group_autoscaling_group_names]))\n}\n\n################################################################################\n# Self Managed Node Group\n################################################################################\n\noutput \"self_managed_node_groups\" {\n  description = \"Map of attribute maps for all self managed node groups created\"\n  value       = module.self_managed_node_group\n}\n\noutput \"self_managed_node_groups_autoscaling_group_names\" {\n  description = \"List of the autoscaling group names created by self-managed node groups\"\n  value       = compact([for group in module.self_managed_node_group : group.autoscaling_group_name])\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-eks/outputs.tf", "hash": "8e2843beeae6562d", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secrets-simple-secret-pod\nspec:\n  containers:\n    - command:\n        - sleep\n        - \"3600\"\n      image: busybox\n      name: secrets-simple-secret-container\n      volumeMounts:\n        - name: secrets-simple-secret-volume\n          mountPath: \"/etc/simple-secret\"\n  volumes:\n    - name: secrets-simple-secret-volume\n      secret:\n        secretName: secrets-simple-secret-secret-doesnotexist\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/broken-Secret/simple-secret.yaml:6"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container secrets-simple-secret-container is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/broken-Secret/simple-secret.yaml:12"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/broken-Secret/simple-secret.yaml", "hash": "0669a456ad7a308b", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\n  labels:\n    app: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: \"true\"\n    spec:\n      serviceAccountName: loadgenerator\n      terminationGracePeriodSeconds: 5\n      restartPolicy: Always\n      securityContext:\n        fsGroup: 1000\n        runAsGroup: 1000\n        runAsNonRoot: true\n        runAsUser: 1000\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - |\n          MAX_RETRIES=12\n          RETRY_INTERVAL=10\n          for i in $(seq 1 $MAX_RETRIES); do\n            echo \"Attempt $i: Pinging frontend: ${FRONTEND_ADDR}...\"\n            STATUSCODE=$(wget --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print $2}')\n            if [ $STATUSCODE -eq 200 ]; then\n                echo \"Frontend is reachable.\"\n                exit 0\n            fi\n            echo \"Error: Could not reach frontend - Status code: ${STATUSCODE}\"\n            sleep $RETRY_INTERVAL\n          done\n          echo \"Failed to reach frontend after $MAX_RETRIES attempts.\"\n          exit 1\n        name: frontend-check\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n              - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n        image: busybox:latest\n        env:\n        - name: FRONTEND_ADDR\n          value: \"frontend:80\"\n      containers:\n      - name: main\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n              - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n        image: us-central1-docker.pkg.dev/google-samples/microservices-demo/loadgenerator:v0.10.3\n        env:\n        - name: FRONTEND_ADDR\n          value: \"frontend:80\"\n        - name: USERS\n          value: \"10\"\n        - name: RATE\n          value: \"1\"\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: loadgenerator\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/microservices-demo/kustomize/base/loadgenerator.yaml", "hash": "b54ca39f4df15b1c", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: network-policy-default-deny-egress\nspec:\n  podSelector: {}\n  policyTypes:\n    - Egress\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/NetworkPolicy/default-deny-egress.yaml", "hash": "fcddbe81161e59b8", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "# https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: nginx\n  name: nginx\nspec:\n  clusterIP: None\n  ports:\n    - name: web\n      port: 80\n  selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: simple-stateful-set\nspec:\n  replicas: 3  # the default is 1\n  selector:\n    matchLabels:\n      app: nginx  # has to match .spec.template.metadata.labels\n  serviceName: \"nginx\"\n  template:\n    metadata:\n      labels:\n        app: nginx  # has to match .spec.selector.matchLabels\n    spec:\n      terminationGracePeriodSeconds: 10\n      containers:\n        - image: nginx\n          name: nginx\n          ports:\n            - containerPort: 80\n              name: web\n          volumeMounts:\n            - mountPath: /usr/share/nginx/html\n              name: www\n  volumeClaimTemplates:\n    - metadata:\n        name: www\n      spec:\n        accessModes: [\"ReadWriteOnce\"]\n        resources:\n          requests:\n            storage: 1Gi\n        storageClassName: \"my-storage-class\"\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-anti-affinity", "severity": "medium", "msg": "Specify anti-affinity in your pod specification to ensure that the orchestrator attempts to schedule replicas on different nodes. Using podAntiAffinity, specify a labelSelector that matches pods for the deployment, and set the topologyKey to kubernetes.io/hostname. Refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/StatefulSet/simple-stateful-set.yaml:31"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container nginx is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/StatefulSet/simple-stateful-set.yaml:35"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/StatefulSet/simple-stateful-set.yaml", "hash": "764545b7d37a6499", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: slow\nprovisioner: kubernetes.io/glusterfs\nparameters:\n  resturl: \"http://127.0.0.1:8081\"\n  clusterid: \"630372ccdc720a92c681fb928f27b53f\"\n  restuser: \"admin\"\n  secretNamespace: \"default\"\n  secretName: \"heketi-secret\"\n  gidMin: \"40000\"\n  gidMax: \"50000\"\n  volumetype: \"replicate:3\"\n  volumeoptions: \"client.ssl on, server.ssl on\"\n  volumenameprefix: \"dept-dev\"\n  snapfactor: \"10\"\n  customepnameprefix: \"dbstorage\"\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/persistent-volume-provisioning/glusterfs/glusterfs-storageclass.yaml", "hash": "f717b0320db41470", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pvpod\nspec:\n  containers:\n  - name: test-container\n    image: registry.k8s.io/test-webserver\n    volumeMounts:\n    - name: test-volume\n      mountPath: /test-vmdk\n  volumes:\n  - name: test-volume\n    persistentVolumeClaim:\n      claimName: pvc0001\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/vsphere/vsphere-volume-pvcpod.yaml:5"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container test-container is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/vsphere/vsphere-volume-pvcpod.yaml:7"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/volumes/vsphere/vsphere-volume-pvcpod.yaml", "hash": "96c62a36fcd67578", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "---\n# https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\nkind: Pod\napiVersion: v1\nmetadata:\n  name: topology-spread-constraints-with-node-affinity-pod\n  labels:\n    label1: value1\nspec:\n  topologySpreadConstraints:\n    - labelSelector:\n        matchLabels:\n          label1: value1\n      maxSkew: 1\n      topologyKey: zone\n      whenUnsatisfiable: DoNotSchedule\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n          - matchExpressions:\n              - key: zone\n                operator: NotIn\n                values:\n                  - zoneC\n  containers:\n    - name: pause\n      image: k8s.gcr.io/pause:3.1\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Pod/spec.topologySpreadConstraints/topology-spread-constraints-with-node-affinity.yaml:9"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container pause is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Pod/spec.topologySpreadConstraints/topology-spread-constraints-with-node-affinity.yaml:27"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/Pod/spec.topologySpreadConstraints/topology-spread-constraints-with-node-affinity.yaml", "hash": "696072a27d927f6c", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: es-data\n  labels:\n    component: elasticsearch\n    role: data\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: elasticsearch\n        role: data\n    spec:\n      serviceAccount: elasticsearch\n      containers:\n      - name: es-data\n        securityContext:\n          capabilities:\n            add:\n              - IPC_LOCK\n        image: quay.io/pires/docker-elasticsearch-kubernetes:1.7.1-4\n        env:\n        - name: KUBERNETES_CA_CERTIFICATE_FILE\n          value: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: \"CLUSTER_NAME\"\n          value: \"myesdb\"\n        - name: NODE_MASTER\n          value: \"false\"\n        - name: HTTP_ENABLE\n          value: \"false\"\n        ports:\n        - containerPort: 9300\n          name: transport\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /data\n          name: storage\n      volumes:\n      - name: storage\n        emptyDir: {}\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "deprecated-service-account-field", "severity": "medium", "msg": "Use the serviceAccountName field instead. If you must specify serviceAccount, ensure values for serviceAccount and serviceAccountName match.", "loc": ""}, {"tool": "kube-linter", "rule_id": "drop-net-raw-capability", "severity": "medium", "msg": "NET_RAW makes it so that an application within the container is able to craft raw packets, use raw sockets, and bind to any address. Remove this capability in the containers under containers security contexts.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "non-existent-service-account", "severity": "medium", "msg": "Create the missing service account, or refer to an existing service account.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/elasticsearch/production_cluster/es-data-rc.yaml:15"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container es-data is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/elasticsearch/production_cluster/es-data-rc.yaml:18"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.allow-privilege-escalation.allow-privilege-escalation", "severity": "WARNING", "msg": "In Kubernetes, each pod runs in its own isolated environment with its own set of security policies. However, certain container images may contain `setuid` or `setgid` binaries that could allow an attacker to perform privilege escalation and gain access to sensitive resources. To mitigate this risk, it's recommended to add a `securityContext` to the container in the pod, with the parameter `allowPrivilegeEscalation` set to `false`. This will prevent the container from running any privileged processes and limit the impact of any potential attacks. By adding the `allowPrivilegeEscalation` parameter to your the `securityContext`, you can help to ensure that your containerized applications are more secure and less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/elasticsearch/production_cluster/es-data-rc.yaml:19"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/elasticsearch/production_cluster/es-data-rc.yaml", "hash": "6d627f69ad27a483", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "module \"wrapper\" {\n  source = \"../../modules/db_subnet_group\"\n\n  for_each = var.items\n\n  create          = try(each.value.create, var.defaults.create, true)\n  description     = try(each.value.description, var.defaults.description, null)\n  name            = try(each.value.name, var.defaults.name, \"\")\n  subnet_ids      = try(each.value.subnet_ids, var.defaults.subnet_ids, [])\n  tags            = try(each.value.tags, var.defaults.tags, {})\n  use_name_prefix = try(each.value.use_name_prefix, var.defaults.use_name_prefix, true)\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-rds/wrappers/db_subnet_group/main.tf", "hash": "e7b2cf0e88b8a458", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "# To test this, run\n#   kubectl run -i --tty load-generator --rm --image=nginx --restart=Never -- /bin/sh -c \"while sleep 0.001; do curl -v $(kubectl get service -o yaml hpa-simple-service -o=jsonpath={.spec.clusterIP}); done\"\n# Wait 5 minutes for the default 'cool-down period' to take effect.\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hpa-simple-deployment\nspec:\n  selector:\n    matchLabels:\n      app: hpa-simple-deployment-app\n  template:\n    metadata:\n      labels:\n        app: hpa-simple-deployment-app\n    spec:\n      containers:\n        - name: hpa-simple-deployment-container\n          image: registry.k8s.io/hpa-example\n          ports:\n            - containerPort: 80\n          resources:\n            limits:\n              cpu: 500m\n            requests:\n              cpu: 200m\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: hpa-simple-deployment-service\nspec:\n  ports:\n    - port: 80\n  selector:\n    app: hpa-simple-deployment-app\n---\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: hpa-simple-hpa\nspec:\n  maxReplicas: 10\n  metrics:\n    - resource:\n        name: cpu\n        target:\n          averageUtilization: 1\n          type: Utilization\n      type: Resource\n  minReplicas: 1\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: hpa-simple-deployment\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/HorizontalPodAutoscaler/simple.yaml:17"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container hpa-simple-deployment-container is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/HorizontalPodAutoscaler/simple.yaml:19"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/HorizontalPodAutoscaler/simple.yaml", "hash": "56e473a33f93aa74", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: apps/v1 #  for k8s versions before 1.9.0 use apps/v1beta2  and before 1.8.0 use extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: redis-replica\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      role: replica\n      tier: backend\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: replica\n        tier: backend\n    spec:\n      containers:\n      - name: slave\n        image: gcr.io/google_samples/gb-redisslave:v1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n          # If your cluster config does not include a dns service, then to\n          # instead access an environment variable to find the master\n          # service's host, comment out the 'value: dns' line above, and\n          # uncomment the line below:\n          # value: env\n        ports:\n        - containerPort: 6379\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "no-anti-affinity", "severity": "medium", "msg": "Specify anti-affinity in your pod specification to ensure that the orchestrator attempts to schedule replicas on different nodes. Using podAntiAffinity, specify a labelSelector that matches pods for the deployment, and set the topologyKey to kubernetes.io/hostname. Refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/web/guestbook/redis-replica-deployment.yaml:18"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container slave is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/web/guestbook/redis-replica-deployment.yaml:20"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/web/guestbook/redis-replica-deployment.yaml", "hash": "0879871ddb0ec34d", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "terraform {\n  required_version = \">= 1.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \">= 6.0\"\n    }\n  }\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-vpc/examples/ipv6-only/versions.tf", "hash": "81f3ddad3f2697f7", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "kind: Service\napiVersion: v1\nmetadata:\n  name: nfs-server\nspec:\n  ports:\n    - name: nfs\n      port: 2049\n    - name: mountd\n      port: 20048\n    - name: rpcbind\n      port: 111\n  selector:\n    role: nfs-server\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "dangling-service", "severity": "medium", "msg": "Confirm that your service's selector correctly matches the labels on one of your deployments.", "loc": ""}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/volumes/nfs/nfs-server-service.yaml", "hash": "d2eb12f55acdafcb", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: broken-init-container-pod\nspec:\n  containers:\n    - name: broken-init-container-container\n      image: busybox\n      command: ['sh', '-c', 'echo The app is running! && sleep 3600']\n  initContainers:\n    - name: broken-init-container-init-container\n      image: busybox\n      command: ['sh', '-c', \"until nslookup pods-init-container-service-nonexistent.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\"]\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/broken-Pod/spec.initContainers/init-container.yaml:6"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container broken-init-container-container is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/broken-Pod/spec.initContainers/init-container.yaml:8"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/broken-Pod/spec.initContainers/init-container.yaml", "hash": "1b05d35f2351c99c", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "kind: Service\napiVersion: v1\nmetadata:\n  name: vtgate\n  labels:\n    component: vtgate\n    app: vitess\nspec:\n  ports:\n    - port: 15001\n  selector:\n    component: vtgate\n    app: vitess\n  type: LoadBalancer\n\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "dangling-service", "severity": "medium", "msg": "Confirm that your service's selector correctly matches the labels on one of your deployments.", "loc": ""}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/storage/vitess/vtgate-service.yaml", "hash": "a1725a998b8ae752", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "---\n# https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\nkind: Pod\napiVersion: v1\nmetadata:\n  name: topology-spread-constraints-pod\n  labels:\n    label1: value1\nspec:\n  topologySpreadConstraints:\n    - maxSkew: 1\n      topologyKey: zone\n      whenUnsatisfiable: DoNotSchedule\n      labelSelector:\n        matchLabels:\n          label1: value1\n  containers:\n    - name: pause\n      image: k8s.gcr.io/pause:3.1\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Pod/spec.topologySpreadConstraints/topology-spread-constraints.yaml:9"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container pause is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Pod/spec.topologySpreadConstraints/topology-spread-constraints.yaml:18"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/Pod/spec.topologySpreadConstraints/topology-spread-constraints.yaml", "hash": "0b05a8bab7f23458", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: fast0001\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: fast\n  resources:\n    requests:\n      storage: 5Gi\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/volumes/storageos/storageos-sc-pvc.yaml", "hash": "4c00f73296eadee6", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: broken-pods-no-command-pod\nspec:\n  containers:\n    # this container has no command or entrypoint specified\n    - image: mstormo/suse\n      name: broken-pods-no-command-container\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/broken-Pod/broken-pods/no-command.yaml:6"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container broken-pods-no-command-container is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/broken-Pod/broken-pods/no-command.yaml:10"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/broken-Pod/broken-pods/no-command.yaml", "hash": "6484d2d9d62c946b", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: broken-pods-bad-command-pod\nspec:\n  containers:\n    - command:\n        - thiscommanddoesnotexist\n      image: busybox\n      name: broken-pods-bad-command-container\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/broken-Pod/broken-pods/bad-command.yaml:6"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container broken-pods-bad-command-container is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/broken-Pod/broken-pods/bad-command.yaml:11"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/broken-Pod/broken-pods/bad-command.yaml", "hash": "bee80566acabea5c", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "terraform {\n  required_version = \">= 1.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \">= 5.92\"\n    }\n  }\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-rds/examples/cross-region-replica-postgres/versions.tf", "hash": "6212b8f03764bcee", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: checkoutservice\n  labels:\n    app: checkoutservice\nspec:\n  selector:\n    matchLabels:\n      app: checkoutservice\n  template:\n    metadata:\n      labels:\n        app: checkoutservice\n    spec:\n      serviceAccountName: checkoutservice\n      securityContext:\n        fsGroup: 1000\n        runAsGroup: 1000\n        runAsNonRoot: true\n        runAsUser: 1000\n      containers:\n        - name: server\n          securityContext:\n            allowPrivilegeEscalation: false\n            capabilities:\n              drop:\n                - ALL\n            privileged: false\n            readOnlyRootFilesystem: true\n          image: checkoutservice\n          ports:\n          - containerPort: 5050\n          readinessProbe:\n            grpc:\n              port: 5050\n          livenessProbe:\n            grpc:\n              port: 5050\n          env:\n          - name: PORT\n            value: \"5050\"\n          - name: PRODUCT_CATALOG_SERVICE_ADDR\n            value: \"productcatalogservice:3550\"\n          - name: SHIPPING_SERVICE_ADDR\n            value: \"shippingservice:50051\"\n          - name: PAYMENT_SERVICE_ADDR\n            value: \"paymentservice:50051\"\n          - name: EMAIL_SERVICE_ADDR\n            value: \"emailservice:5000\"\n          - name: CURRENCY_SERVICE_ADDR\n            value: \"currencyservice:7000\"\n          - name: CART_SERVICE_ADDR\n            value: \"cartservice:7070\"\n          resources:\n            requests:\n              cpu: 100m\n              memory: 64Mi\n            limits:\n              cpu: 200m\n              memory: 128Mi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: checkoutservice\n  labels:\n    app: checkoutservice\nspec:\n  type: ClusterIP\n  selector:\n    app: checkoutservice\n  ports:\n  - name: grpc\n    port: 5050\n    targetPort: 5050\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: checkoutservice\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/microservices-demo/kubernetes-manifests/checkoutservice.yaml", "hash": "84441514b468118a", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "locals {\n  len_public_subnets      = max(length(var.public_subnets), length(var.public_subnet_ipv6_prefixes))\n  len_private_subnets     = max(length(var.private_subnets), length(var.private_subnet_ipv6_prefixes))\n  len_database_subnets    = max(length(var.database_subnets), length(var.database_subnet_ipv6_prefixes))\n  len_elasticache_subnets = max(length(var.elasticache_subnets), length(var.elasticache_subnet_ipv6_prefixes))\n  len_redshift_subnets    = max(length(var.redshift_subnets), length(var.redshift_subnet_ipv6_prefixes))\n  len_intra_subnets       = max(length(var.intra_subnets), length(var.intra_subnet_ipv6_prefixes))\n  len_outpost_subnets     = max(length(var.outpost_subnets), length(var.outpost_subnet_ipv6_prefixes))\n\n  max_subnet_length = max(\n    local.len_private_subnets,\n    local.len_public_subnets,\n    local.len_elasticache_subnets,\n    local.len_database_subnets,\n    local.len_redshift_subnets,\n  )\n\n  # Use `local.vpc_id` to give a hint to Terraform that subnets should be deleted before secondary CIDR blocks can be free!\n  vpc_id = try(aws_vpc_ipv4_cidr_block_association.this[0].vpc_id, aws_vpc.this[0].id, \"\")\n\n  create_vpc = var.create_vpc && var.putin_khuylo\n}\n\n################################################################################\n# VPC\n################################################################################\n\nresource \"aws_vpc\" \"this\" {\n  count = local.create_vpc ? 1 : 0\n\n  region = var.region\n\n  cidr_block          = var.use_ipam_pool ? null : var.cidr\n  ipv4_ipam_pool_id   = var.ipv4_ipam_pool_id\n  ipv4_netmask_length = var.ipv4_netmask_length\n\n  assign_generated_ipv6_cidr_block     = var.enable_ipv6 && !var.use_ipam_pool ? true : null\n  ipv6_cidr_block                      = var.ipv6_cidr\n  ipv6_ipam_pool_id                    = var.ipv6_ipam_pool_id\n  ipv6_netmask_length                  = var.ipv6_netmask_length\n  ipv6_cidr_block_network_border_group = var.ipv6_cidr_block_network_border_group\n\n  instance_tenancy                     = var.instance_tenancy\n  enable_dns_hostnames                 = var.enable_dns_hostnames\n  enable_dns_support                   = var.enable_dns_support\n  enable_network_address_usage_metrics = var.enable_network_address_usage_metrics\n\n  tags = merge(\n    { \"Name\" = var.name },\n    var.tags,\n    var.vpc_tags,\n  )\n}\n\nresource \"aws_vpc_ipv4_cidr_block_association\" \"this\" {\n  count = local.create_vpc && length(var.secondary_cidr_blocks) > 0 ? length(var.secondary_cidr_blocks) : 0\n\n  region = var.region\n\n  # Do not turn this into `local.vpc_id`\n  vpc_id = aws_vpc.this[0].id\n\n  cidr_block = element(var.secondary_cidr_blocks, count.index)\n}\n\nresource \"aws_vpc_block_public_access_options\" \"this\" {\n  count = local.create_vpc && length(keys(var.vpc_block_public_access_options)) > 0 ? 1 : 0\n\n  region = var.region\n\n  internet_gateway_block_mode = try(var.vpc_block_public_access_options[\"internet_gateway_block_mode\"], null)\n}\n\nresource \"aws_vpc_block_public_access_exclusion\" \"this\" {\n  for_each = { for k, v in var.vpc_block_public_access_exclusions : k => v if local.create_vpc }\n\n  region = var.region\n\n  vpc_id = try(each.value.exclude_vpc, false) ? local.vpc_id : null\n\n  subnet_id = try(each.value.exclude_subnet, false) ? lookup(\n    {\n      private     = aws_subnet.private[*].id,\n      public      = aws_subnet.public[*].id,\n      database    = aws_subnet.database[*].id,\n      redshift    = aws_subnet.redshift[*].id,\n      elasticache = aws_subnet.elasticache[*].id,\n      intra       = aws_subnet.intra[*].id,\n      outpost     = aws_subnet.outpost[*].id\n    },\n    each.value.subnet_type,\n    null\n  )[each.value.subnet_index] : null\n\n  internet_gateway_exclusion_mode = each.value.internet_gateway_exclusion_mode\n\n  tags = merge(\n    var.tags,\n    try(each.value.tags, {}),\n  )\n}\n\n################################################################################\n# DHCP Options Set\n################################################################################\n\nresource \"aws_vpc_dhcp_options\" \"this\" {\n  count = local.create_vpc && var.enable_dhcp_options ? 1 : 0\n\n  region = var.region\n\n  domain_name                       = var.dhcp_options_domain_name\n  domain_name_servers               = var.dhcp_options_domain_name_servers\n  ntp_servers                       = var.dhcp_options_ntp_servers\n  netbios_name_servers              = var.dhcp_options_netbios_name_servers\n  netbios_node_type                 = var.dhcp_options_netbios_node_type\n  ipv6_address_preferred_lease_time = var.dhcp_options_ipv6_address_preferred_lease_time\n\n  tags = merge(\n    { \"Name\" = var.name },\n    var.tags,\n    var.dhcp_options_tags,\n  )\n}\n\nresource \"aws_vpc_dhcp_options_association\" \"this\" {\n  count = local.create_vpc && var.enable_dhcp_options ? 1 : 0\n\n  region = var.region\n\n  vpc_id          = local.vpc_id\n  dhcp_options_id = aws_vpc_dhcp_options.this[0].id\n}\n\n################################################################################\n# Public Subnets\n################################################################################\n\nlocals {\n  create_public_subnets = local.create_vpc && local.len_public_subnets > 0\n\n  num_public_route_tables = var.create_multiple_public_route_tables ? local.len_public_subnets : 1\n}\n\nresource \"aws_subnet\" \"public\" {\n  count = local.create_public_subnets && (!var.one_nat_gateway_per_az || local.len_public_subnets >= length(var.azs)) ? local.len_public_subnets : 0\n\n  region = var.region\n\n  assign_ipv6_address_on_creation                = var.enable_ipv6 && var.public_subnet_ipv6_native ? true : var.public_subnet_assign_ipv6_address_on_creation\n  availability_zone                              = length(regexall(\"^[a-z]{2}-\", element(var.azs, count.index))) > 0 ? element(var.azs, count.index) : null\n  availability_zone_id                           = length(regexall(\"^[a-z]{2}-\", element(var.azs, count.index))) == 0 ? element(var.azs, count.index) : null\n  cidr_block                                     = var.public_subnet_ipv6_native ? null : element(concat(var.public_subnets, [\"\"]), count.index)\n  enable_dns64                                   = var.enable_ipv6 && var.public_subnet_enable_dns64\n  enable_resource_name_dns_aaaa_record_on_launch = var.enable_ipv6 && var.public_subnet_enable_resource_name_dns_aaaa_record_on_launch\n  enable_resource_name_dns_a_record_on_launch    = !var.public_subnet_ipv6_native && var.public_subnet_enable_resource_name_dns_a_record_on_launch\n  ipv6_cidr_block                                = var.enable_ipv6 && length(var.public_subnet_ipv6_prefixes) > 0 ? cidrsubnet(aws_vpc.this[0].ipv6_cidr_block, 8, var.public_subnet_ipv6_prefixes[count.index]) : null\n  ipv6_native                                    = var.enable_ipv6 && var.public_subnet_ipv6_native\n  map_public_ip_on_launch                        = var.map_public_ip_on_launch\n  private_dns_hostname_type_on_launch            = var.public_subnet_private_dns_hostname_type_on_launch\n  vpc_id                                         = local.vpc_id\n\n  tags = merge(\n    {\n      Name = try(\n        var.public_subnet_names[count.index],\n        format(\"${var.name}-${var.public_subnet_suffix}-%s\", element(var.azs, count.index))\n      )\n    },\n    var.tags,\n    var.public_subnet_tags,\n    lookup(var.public_subnet_tags_per_az, element(var.azs, count.index), {})\n  )\n}\n\nresource \"aws_route_table\" \"public\" {\n  count = local.create_public_subnets ? local.num_public_route_tables : 0\n\n  region = var.region\n\n  vpc_id = local.vpc_id\n\n  tags = merge(\n    {\n      \"Name\" = var.create_multiple_public_route_tables ? format(\n        \"${var.name}-${var.public_subnet_suffix}-%s\",\n        element(var.azs, count.index),\n      ) : \"${var.name}-${var.public_subnet_suffix}\"\n    },\n    var.tags,\n    var.public_route_table_tags,\n  )\n}\n\nresource \"aws_route_table_association\" \"public\" {\n  count = local.create_public_subnets ? local.len_public_subnets : 0\n\n  region = var.region\n\n  subnet_id      = element(aws_subnet.public[*].id, count.index)\n  route_table_id = element(aws_route_table.public[*].id, var.create_multiple_public_route_tables ? count.index : 0)\n}\n\nresource \"aws_route\" \"public_internet_gateway\" {\n  count = local.create_public_subnets && var.create_igw ? local.num_public_route_tables : 0\n\n  region = var.region\n\n  route_table_id         = aws_route_table.public[count.index].id\n  destination_cidr_block = \"0.0.0.0/0\"\n  gateway_id             = aws_internet_gateway.this[0].id\n\n  timeouts {\n    create = \"5m\"\n  }\n}\n\nresource \"aws_route\" \"public_internet_gateway_ipv6\" {\n  count = local.create_public_subnets && var.create_igw && var.enable_ipv6 ? local.num_public_route_tables : 0\n\n  region = var.region\n\n  route_table_id              = aws_route_table.public[count.index].id\n  destination_ipv6_cidr_block = \"::/0\"\n  gateway_id                  = aws_internet_gateway.this[0].id\n}\n\n################################################################################\n# Public Network ACLs\n################################################################################\n\nresource \"aws_network_acl\" \"public\" {\n  count = local.create_public_subnets && var.public_dedicated_network_acl ? 1 : 0\n\n  region = var.region\n\n  vpc_id     = local.vpc_id\n  subnet_ids = aws_subnet.public[*].id\n\n  tags = merge(\n    { \"Name\" = \"${var.name}-${var.public_subnet_suffix}\" },\n    var.tags,\n    var.public_acl_tags,\n  )\n}\n\nresource \"aws_network_acl_rule\" \"public_inbound\" {\n  count = local.create_public_subnets && var.public_dedicated_network_acl ? length(var.public_inbound_acl_rules) : 0\n\n  region = var.region\n\n  network_acl_id = aws_network_acl.public[0].id\n\n  egress          = false\n  rule_number     = var.public_inbound_acl_rules[count.index][\"rule_number\"]\n  rule_action     = var.public_inbound_acl_rules[count.index][\"rule_action\"]\n  from_port       = lookup(var.public_inbound_acl_rules[count.index], \"from_port\", null)\n  to_port         = lookup(var.public_inbound_acl_rules[count.index], \"to_port\", null)\n  icmp_code       = lookup(var.public_inbound_acl_rules[count.index], \"icmp_code\", null)\n  icmp_type       = lookup(var.public_inbound_acl_rules[count.index], \"icmp_type\", null)\n  protocol        = var.public_inbound_acl_rules[count.index][\"protocol\"]\n  cidr_block      = lookup(var.public_inbound_acl_rules[count.index], \"cidr_block\", null)\n  ipv6_cidr_block = lookup(var.public_inbound_acl_rules[count.index], \"ipv6_cidr_block\", null)\n}\n\nresource \"aws_network_acl_rule\" \"public_outbound\" {\n  count = local.create_public_subnets && var.public_dedicated_network_acl ? length(var.public_outbound_acl_rules) : 0\n\n  region = var.region\n\n  network_acl_id = aws_network_acl.public[0].id\n\n  egress          = true\n  rule_number     = var.public_outbound_acl_rules[count.index][\"rule_number\"]\n  rule_action     = var.public_outbound_acl_rules[count.index][\"rule_action\"]\n  from_port       = lookup(var.public_outbound_acl_rules[count.index], \"from_port\", null)\n  to_port         = lookup(var.public_outbound_acl_rules[count.index], \"to_port\", null)\n  icmp_code       = lookup(var.public_outbound_acl_rules[count.index], \"icmp_code\", null)\n  icmp_type       = lookup(var.public_outbound_acl_rules[count.index], \"icmp_type\", null)\n  protocol        = var.public_outbound_acl_rules[count.index][\"protocol\"]\n  cidr_block      = lookup(var.public_outbound_acl_rules[count.index], \"cidr_block\", null)\n  ipv6_cidr_block = lookup(var.public_outbound_acl_rules[count.index], \"ipv6_cidr_block\", null)\n}\n\n################################################################################\n# Private Subnets\n################################################################################\n\nlocals {\n  create_private_subnets = local.create_vpc && local.len_private_subnets > 0\n}\n\nresource \"aws_subnet\" \"private\" {\n  count = local.create_private_subnets ? local.len_private_subnets : 0\n\n  region = var.region\n\n  assign_ipv6_address_on_creation                = var.enable_ipv6 && var.private_subnet_ipv6_native ? true : var.private_subnet_assign_ipv6_address_on_creation\n  availability_zone                              = length(regexall(\"^[a-z]{2}-\", element(var.azs, count.index))) > 0 ? element(var.azs, count.index) : null\n  availability_zone_id                           = length(regexall(\"^[a-z]{2}-\", element(var.azs, count.index))) == 0 ? element(var.azs, count.index) : null\n  cidr_block                                     = var.private_subnet_ipv6_native ? null : element(concat(var.private_subnets, [\"\"]), count.index)\n  enable_dns64                                   = var.enable_ipv6 && var.private_subnet_enable_dns64\n  enable_resource_name_dns_aaaa_record_on_launch = var.enable_ipv6 && var.private_subnet_enable_resource_name_dns_aaaa_record_on_launch\n  enable_resource_name_dns_a_record_on_launch    = !var.private_subnet_ipv6_native && var.private_subnet_enable_resource_name_dns_a_record_on_launch\n  ipv6_cidr_block                                = var.enable_ipv6 && length(var.private_subnet_ipv6_prefixes) > 0 ? cidrsubnet(aws_vpc.this[0].ipv6_cidr_block, 8, var.private_subnet_ipv6_prefixes[count.index]) : null\n  ipv6_native                                    = var.enable_ipv6 && var.private_subnet_ipv6_native\n  private_dns_hostname_type_on_launch            = var.private_subnet_private_dns_hostname_type_on_launch\n  vpc_id                                         = local.vpc_id\n\n  tags = merge(\n    {\n      Name = try(\n        var.private_subnet_names[count.index],\n        format(\"${var.name}-${var.private_subnet_suffix}-%s\", element(var.azs, count.index))\n      )\n    },\n    var.tags,\n    var.private_subnet_tags,\n    lookup(var.private_subnet_tags_per_az, element(var.azs, count.index), {})\n  )\n}\n\n# There are as many routing tables as the number of NAT gateways\nresource \"aws_route_table\" \"private\" {\n  count = local.create_private_subnets && local.max_subnet_length > 0 ? local.nat_gateway_count : 0\n\n  region = var.region\n\n  vpc_id = local.vpc_id\n\n  tags = merge(\n    {\n      \"Name\" = var.single_nat_gateway ? \"${var.name}-${var.private_subnet_suffix}\" : format(\n        \"${var.name}-${var.private_subnet_suffix}-%s\",\n        element(var.azs, count.index),\n      )\n    },\n    var.tags,\n    var.private_route_table_tags,\n  )\n}\n\nresource \"aws_route_table_association\" \"private\" {\n  count = local.create_private_subnets ? local.len_private_subnets : 0\n\n  region = var.region\n\n  subnet_id = element(aws_subnet.private[*].id, count.index)\n  route_table_id = element(\n    aws_route_table.private[*].id,\n    var.single_nat_gateway ? 0 : count.index,\n  )\n}\n\n################################################################################\n# Private Network ACLs\n################################################################################\n\nlocals {\n  create_private_network_acl = local.create_private_subnets && var.private_dedicated_network_acl\n}\n\nresource \"aws_network_acl\" \"private\" {\n  count = local.create_private_network_acl ? 1 : 0\n\n  region = var.region\n\n  vpc_id     = local.vpc_id\n  subnet_ids = aws_subnet.private[*].id\n\n  tags = merge(\n    { \"Name\" = \"${var.name}-${var.private_subnet_suffix}\" },\n    var.tags,\n    var.private_acl_tags,\n  )\n}\n\nresource \"aws_network_acl_rule\" \"private_inbound\" {\n  count = local.create_private_network_acl ? length(var.private_inbound_acl_rules) : 0\n\n  region = var.region\n\n  network_acl_id = aws_network_acl.private[0].id\n\n  egress          = false\n  rule_number     = var.private_inbound_acl_rules[count.index][\"rule_number\"]\n  rule_action     = var.private_inbound_acl_rules[count.index][\"rule_action\"]\n  from_port       = lookup(var.private_inbound_acl_rules[count.index], \"from_port\", null)\n  to_port         = lookup(var.private_inbound_acl_rules[count.index], \"to_port\", null)\n  icmp_code       = lookup(var.private_inbound_acl_rules[count.index], \"icmp_code\", null)\n  icmp_type       = lookup(var.private_inbound_acl_rules[count.index], \"icmp_type\", null)\n  protocol        = var.private_inbound_acl_rules[count.index][\"protocol\"]\n  cidr_block      = lookup(var.private_inbound_acl_rules[count.index], \"cidr_block\", null)\n  ipv6_cidr_block = lookup(var.private_inbound_acl_rules[count.index], \"ipv6_cidr_block\", null)\n}\n\nresource \"aws_network_acl_rule\" \"private_outbound\" {\n  count = local.create_private_network_acl ? length(var.private_outbound_acl_rules) : 0\n\n  region = var.region\n\n  network_acl_id = aws_network_acl.private[0].id\n\n  egress          = true\n  rule_number     = var.private_outbound_acl_rules[count.index][\"rule_number\"]\n  rule_action     = var.private_outbound_acl_rules[count.index][\"rule_action\"]\n  from_port       = lookup(var.private_outbound_acl_rules[count.index], \"from_port\", null)\n  to_port         = lookup(var.private_outbound_acl_rules[count.index], \"to_port\", null)\n  icmp_code       = lookup(var.private_outbound_acl_rules[count.index], \"icmp_code\", null)\n  icmp_type       = lookup(var.private_outbound_acl_rules[count.index], \"icmp_type\", null)\n  protocol        = var.private_outbound_acl_rules[count.index][\"protocol\"]\n  cidr_block      = lookup(var.private_outbound_acl_rules[count.index], \"cidr_block\", null)\n  ipv6_cidr_block = lookup(var.private_outbound_acl_rules[count.index], \"ipv6_cidr_block\", null)\n}\n\n################################################################################\n# Database Subnets\n################################################################################\n\nlocals {\n  create_database_subnets     = local.create_vpc && local.len_database_subnets > 0\n  create_database_route_table = local.create_database_subnets && var.create_database_subnet_route_table\n}\n\nresource \"aws_subnet\" \"database\" {\n  count = local.create_database_subnets ? local.len_database_subnets : 0\n\n  region = var.region\n\n  assign_ipv6_address_on_creation                = var.enable_ipv6 && var.database_subnet_ipv6_native ? true : var.database_subnet_assign_ipv6_address_on_creation\n  availability_zone                              = length(regexall(\"^[a-z]{2}-\", element(var.azs, count.index))) > 0 ? element(var.azs, count.index) : null\n  availability_zone_id                           = length(regexall(\"^[a-z]{2}-\", element(var.azs, count.index))) == 0 ? element(var.azs, count.index) : null\n  cidr_block                                     = var.database_subnet_ipv6_native ? null : element(concat(var.database_subnets, [\"\"]), count.index)\n  enable_dns64                                   = var.enable_ipv6 && var.database_subnet_enable_dns64\n  enable_resource_name_dns_aaaa_record_on_launch = var.enable_ipv6 && var.database_subnet_enable_resource_name_dns_aaaa_record_on_launch\n  enable_resource_name_dns_a_record_on_launch    = !var.database_subnet_ipv6_native && var.database_subnet_enable_resource_name_dns_a_record_on_launch\n  ipv6_cidr_block                                = var.enable_ipv6 && length(var.database_subnet_ipv6_prefixes) > 0 ? cidrsubnet(aws_vpc.this[0].ipv6_cidr_block, 8, var.database_subnet_ipv6_prefixes[count.index]) : null\n  ipv6_native                                    = var.enable_ipv6 && var.database_subnet_ipv6_native\n  private_dns_hostname_type_on_launch            = var.database_subnet_private_dns_hostname_type_on_launch\n  vpc_id                                         = local.vpc_id\n\n  tags = merge(\n    {\n      Name = try(\n        var.database_subnet_names[count.index],\n        format(\"${var.name}-${var.database_subnet_suffix}-%s\", element(var.azs, count.index), )\n      )\n    },\n    var.tags,\n    var.database_subnet_tags,\n  )\n}\n\nresource \"aws_db_subnet_group\" \"database\" {\n  count = local.create_database_subnets && var.create_database_subnet_group ? 1 : 0\n\n  region = var.region\n\n  name        = lower(coalesce(var.database_subnet_group_name, var.name))\n  description = \"Database subnet group for ${var.name}\"\n  subnet_ids  = aws_subnet.database[*].id\n\n  tags = merge(\n    {\n      \"Name\" = lower(coalesce(var.database_subnet_group_name, var.name))\n    },\n    var.tags,\n    var.database_subnet_group_tags,\n  )\n}\n\nresource \"aws_route_table\" \"database\" {\n  count = local.create_database_route_table ? var.single_nat_gateway || var.create_database_internet_gateway_route ? 1 : local.len_database_subnets : 0\n\n  region = var.region\n\n  vpc_id = local.vpc_id\n\n  tags = merge(\n    {\n      \"Name\" = var.single_nat_gateway || var.create_database_internet_gateway_route ? \"${var.name}-${var.database_subnet_suffix}\" : format(\n        \"${var.name}-${var.database_subnet_suffix}-%s\",\n        element(var.azs, count.index),\n      )\n    },\n    var.tags,\n    var.database_route_table_tags,\n  )\n}\n\nresource \"aws_route_table_association\" \"database\" {\n  count = local.create_database_subnets ? local.len_database_subnets : 0\n\n  region = var.region\n\n  subnet_id = element(aws_subnet.database[*].id, count.index)\n  route_table_id = element(\n    coalescelist(aws_route_table.database[*].id, aws_route_table.private[*].id),\n    var.create_database_subnet_route_table ? var.single_nat_gateway || var.create_database_internet_gateway_route ? 0 : count.index : count.index,\n  )\n}\n\nresource \"aws_route\" \"database_internet_gateway\" {\n  count = local.create_database_route_table && var.create_igw && var.create_database_internet_gateway_route && !var.create_database_nat_gateway_route ? 1 : 0\n\n  region = var.region\n\n  route_table_id         = aws_route_table.database[0].id\n  destination_cidr_block = \"0.0.0.0/0\"\n  gateway_id             = aws_internet_gateway.this[0].id\n\n  timeouts {\n    create = \"5m\"\n  }\n}\n\nresource \"aws_route\" \"database_nat_gateway\" {\n  count = local.create_database_route_table && !var.create_database_internet_gateway_route && var.create_database_nat_gateway_route && var.enable_nat_gateway ? var.single_nat_gateway ? 1 : local.len_database_subnets : 0\n\n  region = var.region\n\n  route_table_id         = element(aws_route_table.database[*].id, count.index)\n  destination_cidr_block = \"0.0.0.0/0\"\n  nat_gateway_id         = element(aws_nat_gateway.this[*].id, count.index)\n\n  timeouts {\n    create = \"5m\"\n  }\n}\n\nresource \"aws_route\" \"database_dns64_nat_gateway\" {\n  count = local.create_database_route_table && !var.create_database_internet_gateway_route && var.create_database_nat_gateway_route && var.enable_nat_gateway && var.enable_ipv6 && var.private_subnet_enable_dns64 ? var.single_nat_gateway ? 1 : local.len_database_subnets : 0\n\n  region = var.region\n\n  route_table_id              = element(aws_route_table.database[*].id, count.index)\n  destination_ipv6_cidr_block = \"64:ff9b::/96\"\n  nat_gateway_id              = element(aws_nat_gateway.this[*].id, count.index)\n\n  timeouts {\n    create = \"5m\"\n  }\n}\n\nresource \"aws_route\" \"database_ipv6_egress\" {\n  count = local.create_database_route_table && var.create_egress_only_igw && var.enable_ipv6 && var.create_database_internet_gateway_route ? 1 : 0\n\n  region = var.region\n\n  route_table_id              = aws_route_table.database[0].id\n  destination_ipv6_cidr_block = \"::/0\"\n  egress_only_gateway_id      = aws_egress_only_internet_gateway.this[0].id\n\n  timeouts {\n    create = \"5m\"\n  }\n}\n\n################################################################################\n# Database Network ACLs\n################################################################################\n\nlocals {\n  create_database_network_acl = local.create_database_subnets && var.database_dedicated_network_acl\n}\n\nresource \"aws_network_acl\" \"database\" {\n  count = local.create_database_network_acl ? 1 : 0\n\n  region = var.region\n\n  vpc_id     = local.vpc_id\n  subnet_ids = aws_subnet.database[*].id\n\n  tags = merge(\n    { \"Name\" = \"${var.name}-${var.database_subnet_suffix}\" },\n    var.tags,\n    var.database_acl_tags,\n  )\n}\n\nresource \"aws_network_acl_rule\" \"database_inbound\" {\n  count = local.create_database_network_acl ? length(var.database_inbound_acl_rules) : 0\n\n  region = var.region\n\n  network_acl_id = aws_network_acl.database[0].id\n\n  egress          = false\n  rule_number     = var.database_inbound_acl_rules[count.index][\"rule_number\"]\n  rule_action     = var.database_inbound_acl_rules[count.index][\"rule_action\"]\n  from_port       = lookup(var.database_inbound_acl_rules[count.index], \"from_port\", null)\n  to_port         = lookup(var.database_inbound_acl_rules[count.index], \"to_port\", null)\n  icmp_code       = lookup(var.database_inbound_acl_rules[count.index], \"icmp_code\", null)\n  icmp_type       = lookup(var.database_inbound_acl_rules[count.index], \"icmp_type\", null)\n  protocol        = var.database_inbound_acl_rules[count.index][\"protocol\"]\n  cidr_block      = lookup(var.database_inbound_acl_rules[count.index], \"cidr_block\", null)\n  ipv6_cidr_block = lookup(var.database_inbound_acl_rules[count.index], \"ipv6_cidr_block\", null)\n}\n\nresource \"aws_network_acl_rule\" \"database_outbound\" {\n  count = local.create_database_network_acl ? length(var.database_outbound_acl_rules) : 0\n\n  region = var.region\n\n  network_acl_id = aws_network_acl.database[0].id\n\n  egress          = true\n  rule_number     = var.database_outbound_acl_rules[count.index][\"rule_number\"]\n  rule_action     = var.database_outbound_acl_rules[count.index][\"rule_action\"]\n  from_port       = lookup(var.database_outbound_acl_rules[count.index], \"from_port\", null)\n  to_port         = lookup(var.database_outbound_acl_rules[count.index], \"to_port\", null)\n  icmp_code       = lookup(var.database_outbound_acl_rules[count.index], \"icmp_code\", null)\n  icmp_type       = lookup(var.database_outbound_acl_rules[count.index], \"icmp_type\", null)\n  protocol        = var.database_outbound_acl_rules[count.index][\"protocol\"]\n  cidr_block      = lookup(var.database_outbound_acl_rules[count.index], \"cidr_block\", null)\n  ipv6_cidr_block = lookup(var.database_outbound_acl_rules[count.index], \"ipv6_cidr_block\", null)\n}\n\n################################################################################\n# Redshift Subnets\n################################################################################\n\nlocals {\n  create_redshift_subnets     = local.create_vpc && local.len_redshift_subnets > 0\n  create_redshift_route_table = local.create_redshift_subnets && var.create_redshift_subnet_route_table\n}\n\nresource \"aws_subnet\" \"redshift\" {\n  count = local.create_redshift_subnets ? local.len_redshift_subnets : 0\n\n  region = var.region\n\n  assign_ipv6_address_on_creation                = var.enable_ipv6 && var.redshift_subnet_ipv6_native ? true : var.redshift_subnet_assign_ipv6_address_on_creation\n  availability_zone                              = length(regexall(\"^[a-z]{2}-\", element(var.azs, count.index))) > 0 ? element(var.azs, count.index) : null\n  availability_zone_id                           = length(regexall(\"^[a-z]{2}-\", element(var.azs, count.index))) == 0 ? element(var.azs, count.index) : null\n  cidr_block                                     = var.redshift_subnet_ipv6_native ? null : element(concat(var.redshift_subnets, [\"\"]), count.index)\n  enable_dns64                                   = var.enable_ipv6 && var.redshift_subnet_enable_dns64\n  enable_resource_name_dns_aaaa_record_on_launch = var.enable_ipv6 && var.redshift_subnet_enable_resource_name_dns_aaaa_record_on_launch\n  enable_resource_name_dns_a_record_on_launch    = !var.redshift_subnet_ipv6_native && var.redshift_subnet_enable_resource_name_dns_a_record_on_launch\n  ipv6_cidr_block                                = var.enable_ipv6 && length(var.redshift_subnet_ipv6_prefixes) > 0 ? cidrsubnet(aws_vpc.this[0].ipv6_cidr_block, 8, var.redshift_subnet_ipv6_prefixes[count.index]) : null\n  ipv6_native                                    = var.enable_ipv6 && var.redshift_subnet_ipv6_native\n  private_dns_hostname_type_on_launch            = var.redshift_subnet_private_dns_hostname_type_on_launch\n  vpc_id                                         = local.vpc_id\n\n  tags = merge(\n    {\n      Name = try(\n        var.redshift_subnet_names[count.index],\n        format(\"${var.name}-${var.redshift_subnet_suffix}-%s\", element(var.azs, count.index))\n      )\n    },\n    var.tags,\n    var.redshift_subnet_tags,\n  )\n}\n\nresource \"aws_redshift_subnet_group\" \"redshift\" {\n  count = local.create_redshift_subnets && var.create_redshift_subnet_group ? 1 : 0\n\n  region = var.region\n\n  name        = lower(coalesce(var.redshift_subnet_group_name, var.name))\n  description = \"Redshift subnet group for ${var.name}\"\n  subnet_ids  = aws_subnet.redshift[*].id\n\n  tags = merge(\n    { \"Name\" = coalesce(var.redshift_subnet_group_name, var.name) },\n    var.tags,\n    var.redshift_subnet_group_tags,\n  )\n}\n\nresource \"aws_route_table\" \"redshift\" {\n  count = local.create_redshift_route_table ? 1 : 0\n\n  region = var.region\n\n  vpc_id = local.vpc_id\n\n  tags = merge(\n    { \"Name\" = \"${var.name}-${var.redshift_subnet_suffix}\" },\n    var.tags,\n    var.redshift_route_table_tags,\n  )\n}\n\nresource \"aws_route_table_association\" \"redshift\" {\n  count = local.create_redshift_subnets && !var.enable_public_redshift ? local.len_redshift_subnets : 0\n\n  region = var.region\n\n  subnet_id = element(aws_subnet.redshift[*].id, count.index)\n  route_table_id = element(\n    coalescelist(aws_route_table.redshift[*].id, aws_route_table.private[*].id),\n    var.single_nat_gateway || var.create_redshift_subnet_route_table ? 0 : count.index,\n  )\n}\n\nresource \"aws_route_table_association\" \"redshift_public\" {\n  count = local.create_redshift_subnets && var.enable_public_redshift ? local.len_redshift_subnets : 0\n\n  region = var.region\n\n  subnet_id = element(aws_subnet.redshift[*].id, count.index)\n  route_table_id = element(\n    coalescelist(aws_route_table.redshift[*].id, aws_route_table.public[*].id),\n    var.single_nat_gateway || var.create_redshift_subnet_route_table ? 0 : count.index,\n  )\n}\n\n################################################################################\n# Redshift Network ACLs\n################################################################################\n\nlocals {\n  create_redshift_network_acl = local.create_redshift_subnets && var.redshift_dedicated_network_acl\n}\n\nresource \"aws_network_acl\" \"redshift\" {\n  count = local.create_redshift_network_acl ? 1 : 0\n\n  region = var.region\n\n  vpc_id     = local.vpc_id\n  subnet_ids = aws_subnet.redshift[*].id\n\n  tags = merge(\n    { \"Name\" = \"${var.name}-${var.redshift_subnet_suffix}\" },\n    var.tags,\n    var.redshift_acl_tags,\n  )\n}\n\nresource \"aws_network_acl_rule\" \"redshift_inbound\" {\n  count = local.create_redshift_network_acl ? length(var.redshift_inbound_acl_rules) : 0\n\n  region = var.region\n\n  network_acl_id = aws_network_acl.redshift[0].id\n\n  egress          = false\n  rule_number     = var.redshift_inbound_acl_rules[count.index][\"rule_number\"]\n  rule_action     = var.redshift_inbound_acl_rules[count.index][\"rule_action\"]\n  from_port       = lookup(var.redshift_inbound_acl_rules[count.index], \"from_port\", null)\n  to_port         = lookup(var.redshift_inbound_acl_rules[count.index], \"to_port\", null)\n  icmp_code       = lookup(var.redshift_inbound_acl_rules[count.index], \"icmp_code\", null)\n  icmp_type       = lookup(var.redshift_inbound_acl_rules[count.index], \"icmp_type\", null)\n  protocol        = var.redshift_inbound_acl_rules[count.index][\"protocol\"]\n  cidr_block      = lookup(var.redshift_inbound_acl_rules[count.index], \"cidr_block\", null)\n  ipv6_cidr_block = lookup(var.redshift_inbound_acl_rules[count.index], \"ipv6_cidr_block\", null)\n}\n\nresource \"aws_network_acl_rule\" \"redshift_outbound\" {\n  count = local.create_redshift_network_acl ? length(var.redshift_outbound_acl_rules) : 0\n\n  region = var.region\n\n  network_acl_id = aws_network_acl.redshift[0].id\n\n  egress          = true\n  rule_number     = var.redshift_outbound_acl_rules[count.index][\"rule_number\"]\n  rule_action     = var.redshift_outbound_acl_rules[count.index][\"rule_action\"]\n  from_port       = lookup(var.redshift_outbound_acl_rules[count.index], \"from_port\", null)\n  to_port         = lookup(var.redshift_outbound_acl_rules[count.index], \"to_port\", null)\n  icmp_code       = lookup(var.redshift_outbound_acl_rules[count.index], \"icmp_code\", null)\n  icmp_type       = lookup(var.redshift_outbound_acl_rules[count.index], \"icmp_type\", null)\n  protocol        = var.redshift_outbound_acl_rules[count.index][\"protocol\"]\n  cidr_block      = lookup(var.redshift_outbound_acl_rules[count.index], \"cidr_block\", null)\n  ipv6_cidr_block = lookup(var.redshift_outbound_acl_rules[count.index], \"ipv6_cidr_block\", null)\n}\n\n################################################################################\n# Elasticache Subnets\n################################################################################\n\nlocals {\n  create_elasticache_subnets     = local.create_vpc && local.len_elasticache_subnets > 0\n  create_elasticache_route_table = local.create_elasticache_subnets && var.create_elasticache_subnet_route_table\n}\n\nresource \"aws_subnet\" \"elasticache\" {\n  count = local.create_elasticache_subnets ? local.len_elasticache_subnets : 0\n\n  region = var.region\n\n  assign_ipv6_address_on_creation                = var.enable_ipv6 && var.elasticache_subnet_ipv6_native ? true : var.elasticache_subnet_assign_ipv6_address_on_creation\n  availability_zone                              = length(regexall(\"^[a-z]{2}-\", element(var.azs, count.index))) > 0 ? element(var.azs, count.index) : null\n  availability_zone_id                           = length(regexall(\"^[a-z]{2}-\", element(var.azs, count.index))) == 0 ? element(var.azs, count.index) : null\n  cidr_block                                     = var.elasticache_subnet_ipv6_native ? null : element(concat(var.elasticache_subnets, [\"\"]), count.index)\n  enable_dns64                                   = var.enable_ipv6 && var.elasticache_subnet_enable_dns64\n  enable_resource_name_dns_aaaa_record_on_launch = var.enable_ipv6 && var.elasticache_subnet_enable_resource_name_dns_aaaa_record_on_launch\n  enable_resource_name_dns_a_record_on_launch    = !var.elasticache_subnet_ipv6_native && var.elasticache_subnet_enable_resource_name_dns_a_record_on_launch\n  ipv6_cidr_block                                = var.enable_ipv6 && length(var.elasticache_subnet_ipv6_prefixes) > 0 ? cidrsubnet(aws_vpc.this[0].ipv6_cidr_block, 8, var.elasticache_subnet_ipv6_prefixes[count.index]) : null\n  ipv6_native                                    = var.enable_ipv6 && var.elasticache_subnet_ipv6_native\n  private_dns_hostname_type_on_launch            = var.elasticache_subnet_private_dns_hostname_type_on_launch\n  vpc_id                                         = local.vpc_id\n\n  tags = merge(\n    {\n      Name = try(\n        var.elasticache_subnet_names[count.index],\n        format(\"${var.name}-${var.elasticache_subnet_suffix}-%s\", element(var.azs, count.index))\n      )\n    },\n    var.tags,\n    var.elasticache_subnet_tags,\n  )\n}\n\nresource \"aws_elasticache_subnet_group\" \"elasticache\" {\n  count = local.create_elasticache_subnets && var.create_elasticache_subnet_group ? 1 : 0\n\n  region = var.region\n\n  name        = coalesce(var.elasticache_subnet_group_name, var.name)\n  description = \"ElastiCache subnet group for ${var.name}\"\n  subnet_ids  = aws_subnet.elasticache[*].id\n\n  tags = merge(\n    { \"Name\" = coalesce(var.elasticache_subnet_group_name, var.name) },\n    var.tags,\n    var.elasticache_subnet_group_tags,\n  )\n}\n\nresource \"aws_route_table\" \"elasticache\" {\n  count = local.create_elasticache_route_table ? 1 : 0\n\n  region = var.region\n\n  vpc_id = local.vpc_id\n\n  tags = merge(\n    { \"Name\" = \"${var.name}-${var.elasticache_subnet_suffix}\" },\n    var.tags,\n    var.elasticache_route_table_tags,\n  )\n}\n\nresource \"aws_route_table_association\" \"elasticache\" {\n  count = local.create_elasticache_subnets ? local.len_elasticache_subnets : 0\n\n  region = var.region\n\n  subnet_id = element(aws_subnet.elasticache[*].id, count.index)\n  route_table_id = element(\n    coalescelist(\n      aws_route_table.elasticache[*].id,\n      aws_route_table.private[*].id,\n    ),\n    var.single_nat_gateway || var.create_elasticache_subnet_route_table ? 0 : count.index,\n  )\n}\n\n################################################################################\n# Elasticache Network ACLs\n################################################################################\n\nlocals {\n  create_elasticache_network_acl = local.create_elasticache_subnets && var.elasticache_dedicated_network_acl\n}\n\nresource \"aws_network_acl\" \"elasticache\" {\n  count = local.create_elasticache_network_acl ? 1 : 0\n\n  region = var.region\n\n  vpc_id     = local.vpc_id\n  subnet_ids = aws_subnet.elasticache[*].id\n\n  tags = merge(\n    { \"Name\" = \"${var.name}-${var.elasticache_subnet_suffix}\" },\n    var.tags,\n    var.elasticache_acl_tags,\n  )\n}\n\nresource \"aws_network_acl_rule\" \"elasticache_inbound\" {\n  count = local.create_elasticache_network_acl ? length(var.elasticache_inbound_acl_rules) : 0\n\n  region = var.region\n\n  network_acl_id = aws_network_acl.elasticache[0].id\n\n  egress          = false\n  rule_number     = var.elasticache_inbound_acl_rules[count.index][\"rule_number\"]\n  rule_action     = var.elasticache_inbound_acl_rules[count.index][\"rule_action\"]\n  from_port       = lookup(var.elasticache_inbound_acl_rules[count.index], \"from_port\", null)\n  to_port         = lookup(var.elasticache_inbound_acl_rules[count.index], \"to_port\", null)\n  icmp_code       = lookup(var.elasticache_inbound_acl_rules[count.index], \"icmp_code\", null)\n  icmp_type       = lookup(var.elasticache_inbound_acl_rules[count.index], \"icmp_type\", null)\n  protocol        = var.elasticache_inbound_acl_rules[count.index][\"protocol\"]\n  cidr_block      = lookup(var.elasticache_inbound_acl_rules[count.index], \"cidr_block\", null)\n  ipv6_cidr_block = lookup(var.elasticache_inbound_acl_rules[count.index], \"ipv6_cidr_block\", null)\n}\n\nresource \"aws_network_acl_rule\" \"elasticache_outbound\" {\n  count = local.create_elasticache_network_acl ? length(var.elasticache_outbound_acl_rules) : 0\n\n  region = var.region\n\n  network_acl_id = aws_network_acl.elasticache[0].id\n\n  egress          = true\n  rule_number     = var.elasticache_outbound_acl_rules[count.index][\"rule_number\"]\n  rule_action     = var.elasticache_outbound_acl_rules[count.index][\"rule_action\"]\n  from_port       = lookup(var.elasticache_outbound_acl_rules[count.index], \"from_port\", null)\n  to_port         = lookup(var.elasticache_outbound_acl_rules[count.index], \"to_port\", null)\n  icmp_code       = lookup(var.elasticache_outbound_acl_rules[count.index], \"icmp_code\", null)\n  icmp_type       = lookup(var.elasticache_outbound_acl_rules[count.index], \"icmp_type\", null)\n  protocol        = var.elasticache_outbound_acl_rules[count.index][\"protocol\"]\n  cidr_block      = lookup(var.elasticache_outbound_acl_rules[count.index], \"cidr_block\", null)\n  ipv6_cidr_block = lookup(var.elasticache_outbound_acl_rules[count.index], \"ipv6_cidr_block\", null)\n}\n\n################################################################################\n# Intra Subnets\n################################################################################\n\nlocals {\n  create_intra_subnets   = local.create_vpc && local.len_intra_subnets > 0\n  num_intra_route_tables = var.create_multiple_intra_route_tables ? local.len_intra_subnets : 1\n}\n\nresource \"aws_subnet\" \"intra\" {\n  count = local.create_intra_subnets ? local.len_intra_subnets : 0\n\n  region = var.region\n\n  assign_ipv6_address_on_creation                = var.enable_ipv6 && var.intra_subnet_ipv6_native ? true : var.intra_subnet_assign_ipv6_address_on_creation\n  availability_zone                              = length(regexall(\"^[a-z]{2}-\", element(var.azs, count.index))) > 0 ? element(var.azs, count.index) : null\n  availability_zone_id                           = length(regexall(\"^[a-z]{2}-\", element(var.azs, count.index))) == 0 ? element(var.azs, count.index) : null\n  cidr_block                                     = var.intra_subnet_ipv6_native ? null : element(concat(var.intra_subnets, [\"\"]), count.index)\n  enable_dns64                                   = var.enable_ipv6 && var.intra_subnet_enable_dns64\n  enable_resource_name_dns_aaaa_record_on_launch = var.enable_ipv6 && var.intra_subnet_enable_resource_name_dns_aaaa_record_on_launch\n  enable_resource_name_dns_a_record_on_launch    = !var.intra_subnet_ipv6_native && var.intra_subnet_enable_resource_name_dns_a_record_on_launch\n  ipv6_cidr_block                                = var.enable_ipv6 && length(var.intra_subnet_ipv6_prefixes) > 0 ? cidrsubnet(aws_vpc.this[0].ipv6_cidr_block, 8, var.intra_subnet_ipv6_prefixes[count.index]) : null\n  ipv6_native                                    = var.enable_ipv6 && var.intra_subnet_ipv6_native\n  private_dns_hostname_type_on_launch            = var.intra_subnet_private_dns_hostname_type_on_launch\n  vpc_id                                         = local.vpc_id\n\n  tags = merge(\n    {\n      Name = try(\n        var.intra_subnet_names[count.index],\n        format(\"${var.name}-${var.intra_subnet_suffix}-%s\", element(var.azs, count.index))\n      )\n    },\n    var.tags,\n    var.intra_subnet_tags,\n  )\n}\n\nresource \"aws_route_table\" \"intra\" {\n  count = local.create_intra_subnets ? local.num_intra_route_tables : 0\n\n  region = var.region\n\n  vpc_id = local.vpc_id\n\n  tags = merge(\n    {\n      \"Name\" = var.create_multiple_intra_route_tables ? format(\n        \"${var.name}-${var.intra_subnet_suffix}-%s\",\n        element(var.azs, count.index),\n      ) : \"${var.name}-${var.intra_subnet_suffix}\"\n    },\n    var.tags,\n    var.intra_route_table_tags,\n  )\n}\n\nresource \"aws_route_table_association\" \"intra\" {\n  count = local.create_intra_subnets ? local.len_intra_subnets : 0\n\n  region = var.region\n\n  subnet_id      = element(aws_subnet.intra[*].id, count.index)\n  route_table_id = element(aws_route_table.intra[*].id, var.create_multiple_intra_route_tables ? count.index : 0)\n}\n\n################################################################################\n# Intra Network ACLs\n################################################################################\n\nlocals {\n  create_intra_network_acl = local.create_intra_subnets && var.intra_dedicated_network_acl\n}\n\nresource \"aws_network_acl\" \"intra\" {\n  count = local.create_intra_network_acl ? 1 : 0\n\n  region = var.region\n\n  vpc_id     = local.vpc_id\n  subnet_ids = aws_subnet.intra[*].id\n\n  tags = merge(\n    { \"Name\" = \"${var.name}-${var.intra_subnet_suffix}\" },\n    var.tags,\n    var.intra_acl_tags,\n  )\n}\n\nresource \"aws_network_acl_rule\" \"intra_inbound\" {\n  count = local.create_intra_network_acl ? length(var.intra_inbound_acl_rules) : 0\n\n  region = var.region\n\n  network_acl_id = aws_network_acl.intra[0].id\n\n  egress          = false\n  rule_number     = var.intra_inbound_acl_rules[count.index][\"rule_number\"]\n  rule_action     = var.intra_inbound_acl_rules[count.index][\"rule_action\"]\n  from_port       = lookup(var.intra_inbound_acl_rules[count.index], \"from_port\", null)\n  to_port         = lookup(var.intra_inbound_acl_rules[count.index], \"to_port\", null)\n  icmp_code       = lookup(var.intra_inbound_acl_rules[count.index], \"icmp_code\", null)\n  icmp_type       = lookup(var.intra_inbound_acl_rules[count.index], \"icmp_type\", null)\n  protocol        = var.intra_inbound_acl_rules[count.index][\"protocol\"]\n  cidr_block      = lookup(var.intra_inbound_acl_rules[count.index], \"cidr_block\", null)\n  ipv6_cidr_block = lookup(var.intra_inbound_acl_rules[count.index], \"ipv6_cidr_block\", null)\n}\n\nresource \"aws_network_acl_rule\" \"intra_outbound\" {\n  count = local.create_intra_network_acl ? length(var.intra_outbound_acl_rules) : 0\n\n  region = var.region\n\n  network_acl_id = aws_network_acl.intra[0].id\n\n  egress          = true\n  rule_number     = var.intra_outbound_acl_rules[count.index][\"rule_number\"]\n  rule_action     = var.intra_outbound_acl_rules[count.index][\"rule_action\"]\n  from_port       = lookup(var.intra_outbound_acl_rules[count.index], \"from_port\", null)\n  to_port         = lookup(var.intra_outbound_acl_rules[count.index], \"to_port\", null)\n  icmp_code       = lookup(var.intra_outbound_acl_rules[count.index], \"icmp_code\", null)\n  icmp_type       = lookup(var.intra_outbound_acl_rules[count.index], \"icmp_type\", null)\n  protocol        = var.intra_outbound_acl_rules[count.index][\"protocol\"]\n  cidr_block      = lookup(var.intra_outbound_acl_rules[count.index], \"cidr_block\", null)\n  ipv6_cidr_block = lookup(var.intra_outbound_acl_rules[count.index], \"ipv6_cidr_block\", null)\n}\n\n################################################################################\n# Outpost Subnets\n################################################################################\n\nlocals {\n  create_outpost_subnets = local.create_vpc && local.len_outpost_subnets > 0\n}\n\nresource \"aws_subnet\" \"outpost\" {\n  count = local.create_outpost_subnets ? local.len_outpost_subnets : 0\n\n  region = var.region\n\n  assign_ipv6_address_on_creation                = var.enable_ipv6 && var.outpost_subnet_ipv6_native ? true : var.outpost_subnet_assign_ipv6_address_on_creation\n  availability_zone                              = var.outpost_az\n  cidr_block                                     = var.outpost_subnet_ipv6_native ? null : element(concat(var.outpost_subnets, [\"\"]), count.index)\n  customer_owned_ipv4_pool                       = var.customer_owned_ipv4_pool\n  enable_dns64                                   = var.enable_ipv6 && var.outpost_subnet_enable_dns64\n  enable_resource_name_dns_aaaa_record_on_launch = var.enable_ipv6 && var.outpost_subnet_enable_resource_name_dns_aaaa_record_on_launch\n  enable_resource_name_dns_a_record_on_launch    = !var.outpost_subnet_ipv6_native && var.outpost_subnet_enable_resource_name_dns_a_record_on_launch\n  ipv6_cidr_block                                = var.enable_ipv6 && length(var.outpost_subnet_ipv6_prefixes) > 0 ? cidrsubnet(aws_vpc.this[0].ipv6_cidr_block, 8, var.outpost_subnet_ipv6_prefixes[count.index]) : null\n  ipv6_native                                    = var.enable_ipv6 && var.outpost_subnet_ipv6_native\n  map_customer_owned_ip_on_launch                = var.map_customer_owned_ip_on_launch\n  outpost_arn                                    = var.outpost_arn\n  private_dns_hostname_type_on_launch            = var.outpost_subnet_private_dns_hostname_type_on_launch\n  vpc_id                                         = local.vpc_id\n\n  tags = merge(\n    {\n      Name = try(\n        var.outpost_subnet_names[count.index],\n        format(\"${var.name}-${var.outpost_subnet_suffix}-%s\", var.outpost_az)\n      )\n    },\n    var.tags,\n    var.outpost_subnet_tags,\n  )\n}\n\nresource \"aws_route_table_association\" \"outpost\" {\n  count = local.create_outpost_subnets ? local.len_outpost_subnets : 0\n\n  region = var.region\n\n  subnet_id = element(aws_subnet.outpost[*].id, count.index)\n  route_table_id = element(\n    aws_route_table.private[*].id,\n    var.single_nat_gateway ? 0 : count.index,\n  )\n}\n\n################################################################################\n# Outpost Network ACLs\n################################################################################\n\nlocals {\n  create_outpost_network_acl = local.create_outpost_subnets && var.outpost_dedicated_network_acl\n}\n\nresource \"aws_network_acl\" \"outpost\" {\n  count = local.create_outpost_network_acl ? 1 : 0\n\n  region = var.region\n\n  vpc_id     = local.vpc_id\n  subnet_ids = aws_subnet.outpost[*].id\n\n  tags = merge(\n    { \"Name\" = \"${var.name}-${var.outpost_subnet_suffix}\" },\n    var.tags,\n    var.outpost_acl_tags,\n  )\n}\n\nresource \"aws_network_acl_rule\" \"outpost_inbound\" {\n  count = local.create_outpost_network_acl ? length(var.outpost_inbound_acl_rules) : 0\n\n  region = var.region\n\n  network_acl_id = aws_network_acl.outpost[0].id\n\n  egress          = false\n  rule_number     = var.outpost_inbound_acl_rules[count.index][\"rule_number\"]\n  rule_action     = var.outpost_inbound_acl_rules[count.index][\"rule_action\"]\n  from_port       = lookup(var.outpost_inbound_acl_rules[count.index], \"from_port\", null)\n  to_port         = lookup(var.outpost_inbound_acl_rules[count.index], \"to_port\", null)\n  icmp_code       = lookup(var.outpost_inbound_acl_rules[count.index], \"icmp_code\", null)\n  icmp_type       = lookup(var.outpost_inbound_acl_rules[count.index], \"icmp_type\", null)\n  protocol        = var.outpost_inbound_acl_rules[count.index][\"protocol\"]\n  cidr_block      = lookup(var.outpost_inbound_acl_rules[count.index], \"cidr_block\", null)\n  ipv6_cidr_block = lookup(var.outpost_inbound_acl_rules[count.index], \"ipv6_cidr_block\", null)\n}\n\nresource \"aws_network_acl_rule\" \"outpost_outbound\" {\n  count = local.create_outpost_network_acl ? length(var.outpost_outbound_acl_rules) : 0\n\n  region = var.region\n\n  network_acl_id = aws_network_acl.outpost[0].id\n\n  egress          = true\n  rule_number     = var.outpost_outbound_acl_rules[count.index][\"rule_number\"]\n  rule_action     = var.outpost_outbound_acl_rules[count.index][\"rule_action\"]\n  from_port       = lookup(var.outpost_outbound_acl_rules[count.index], \"from_port\", null)\n  to_port         = lookup(var.outpost_outbound_acl_rules[count.index], \"to_port\", null)\n  icmp_code       = lookup(var.outpost_outbound_acl_rules[count.index], \"icmp_code\", null)\n  icmp_type       = lookup(var.outpost_outbound_acl_rules[count.index], \"icmp_type\", null)\n  protocol        = var.outpost_outbound_acl_rules[count.index][\"protocol\"]\n  cidr_block      = lookup(var.outpost_outbound_acl_rules[count.index], \"cidr_block\", null)\n  ipv6_cidr_block = lookup(var.outpost_outbound_acl_rules[count.index], \"ipv6_cidr_block\", null)\n}\n\n################################################################################\n# Internet Gateway\n################################################################################\n\nresource \"aws_internet_gateway\" \"this\" {\n  count = local.create_public_subnets && var.create_igw ? 1 : 0\n\n  region = var.region\n\n  vpc_id = local.vpc_id\n\n  tags = merge(\n    { \"Name\" = var.name },\n    var.tags,\n    var.igw_tags,\n  )\n}\n\nresource \"aws_egress_only_internet_gateway\" \"this\" {\n  count = local.create_vpc && var.create_egress_only_igw && var.enable_ipv6 && local.max_subnet_length > 0 ? 1 : 0\n\n  region = var.region\n\n  vpc_id = local.vpc_id\n\n  tags = merge(\n    { \"Name\" = var.name },\n    var.tags,\n    var.igw_tags,\n  )\n}\n\nresource \"aws_route\" \"private_ipv6_egress\" {\n  count = local.create_vpc && var.create_egress_only_igw && var.enable_ipv6 && local.len_private_subnets > 0 ? local.nat_gateway_count : 0\n\n  region = var.region\n\n  route_table_id              = element(aws_route_table.private[*].id, count.index)\n  destination_ipv6_cidr_block = \"::/0\"\n  egress_only_gateway_id      = element(aws_egress_only_internet_gateway.this[*].id, 0)\n}\n\n################################################################################\n# NAT Gateway\n################################################################################\n\nlocals {\n  nat_gateway_count = var.single_nat_gateway ? 1 : var.one_nat_gateway_per_az ? length(var.azs) : local.max_subnet_length\n  nat_gateway_ips   = var.reuse_nat_ips ? var.external_nat_ip_ids : aws_eip.nat[*].id\n}\n\nresource \"aws_eip\" \"nat\" {\n  count = local.create_vpc && var.enable_nat_gateway && !var.reuse_nat_ips ? local.nat_gateway_count : 0\n\n  region = var.region\n\n  domain = \"vpc\"\n\n  tags = merge(\n    {\n      \"Name\" = format(\n        \"${var.name}-%s\",\n        element(var.azs, var.single_nat_gateway ? 0 : count.index),\n      )\n    },\n    var.tags,\n    var.nat_eip_tags,\n  )\n\n  depends_on = [aws_internet_gateway.this]\n}\n\nresource \"aws_nat_gateway\" \"this\" {\n  count = local.create_vpc && var.enable_nat_gateway ? local.nat_gateway_count : 0\n\n  region = var.region\n\n  allocation_id = element(\n    local.nat_gateway_ips,\n    var.single_nat_gateway ? 0 : count.index,\n  )\n  subnet_id = element(\n    aws_subnet.public[*].id,\n    var.single_nat_gateway ? 0 : count.index,\n  )\n\n  tags = merge(\n    {\n      \"Name\" = format(\n        \"${var.name}-%s\",\n        element(var.azs, var.single_nat_gateway ? 0 : count.index),\n      )\n    },\n    var.tags,\n    var.nat_gateway_tags,\n  )\n\n  depends_on = [aws_internet_gateway.this]\n}\n\nresource \"aws_route\" \"private_nat_gateway\" {\n  count = local.create_vpc && var.enable_nat_gateway && var.create_private_nat_gateway_route ? local.nat_gateway_count : 0\n\n  region = var.region\n\n  route_table_id         = element(aws_route_table.private[*].id, count.index)\n  destination_cidr_block = var.nat_gateway_destination_cidr_block\n  nat_gateway_id         = element(aws_nat_gateway.this[*].id, count.index)\n\n  timeouts {\n    create = \"5m\"\n  }\n}\n\nresource \"aws_route\" \"private_dns64_nat_gateway\" {\n  count = local.create_vpc && var.enable_nat_gateway && var.enable_ipv6 && var.private_subnet_enable_dns64 ? local.nat_gateway_count : 0\n\n  region = var.region\n\n  route_table_id              = element(aws_route_table.private[*].id, count.index)\n  destination_ipv6_cidr_block = \"64:ff9b::/96\"\n  nat_gateway_id              = element(aws_nat_gateway.this[*].id, count.index)\n\n  timeouts {\n    create = \"5m\"\n  }\n}\n\n################################################################################\n# Customer Gateways\n################################################################################\n\nresource \"aws_customer_gateway\" \"this\" {\n  for_each = var.customer_gateways\n\n  region = var.region\n\n  bgp_asn     = each.value[\"bgp_asn\"]\n  ip_address  = each.value[\"ip_address\"]\n  device_name = lookup(each.value, \"device_name\", null)\n  type        = \"ipsec.1\"\n\n  tags = merge(\n    { Name = \"${var.name}-${each.key}\" },\n    var.tags,\n    var.customer_gateway_tags,\n  )\n\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n\n################################################################################\n# VPN Gateway\n################################################################################\n\nresource \"aws_vpn_gateway\" \"this\" {\n  count = local.create_vpc && var.enable_vpn_gateway ? 1 : 0\n\n  region = var.region\n\n  vpc_id            = local.vpc_id\n  amazon_side_asn   = var.amazon_side_asn\n  availability_zone = var.vpn_gateway_az\n\n  tags = merge(\n    { \"Name\" = var.name },\n    var.tags,\n    var.vpn_gateway_tags,\n  )\n}\n\nresource \"aws_vpn_gateway_attachment\" \"this\" {\n  count = var.vpn_gateway_id != \"\" ? 1 : 0\n\n  region = var.region\n\n  vpc_id         = local.vpc_id\n  vpn_gateway_id = var.vpn_gateway_id\n}\n\nresource \"aws_vpn_gateway_route_propagation\" \"public\" {\n  count = local.create_vpc && var.propagate_public_route_tables_vgw && (var.enable_vpn_gateway || var.vpn_gateway_id != \"\") ? 1 : 0\n\n  region = var.region\n\n  route_table_id = element(aws_route_table.public[*].id, count.index)\n  vpn_gateway_id = element(\n    concat(\n      aws_vpn_gateway.this[*].id,\n      aws_vpn_gateway_attachment.this[*].vpn_gateway_id,\n    ),\n    count.index,\n  )\n}\n\nresource \"aws_vpn_gateway_route_propagation\" \"private\" {\n  count = local.create_vpc && var.propagate_private_route_tables_vgw && (var.enable_vpn_gateway || var.vpn_gateway_id != \"\") ? local.len_private_subnets : 0\n\n  region = var.region\n\n  route_table_id = element(aws_route_table.private[*].id, count.index)\n  vpn_gateway_id = element(\n    concat(\n      aws_vpn_gateway.this[*].id,\n      aws_vpn_gateway_attachment.this[*].vpn_gateway_id,\n    ),\n    count.index,\n  )\n}\n\nresource \"aws_vpn_gateway_route_propagation\" \"intra\" {\n  count = local.create_vpc && var.propagate_intra_route_tables_vgw && (var.enable_vpn_gateway || var.vpn_gateway_id != \"\") ? local.len_intra_subnets : 0\n\n  region = var.region\n\n  route_table_id = element(aws_route_table.intra[*].id, count.index)\n  vpn_gateway_id = element(\n    concat(\n      aws_vpn_gateway.this[*].id,\n      aws_vpn_gateway_attachment.this[*].vpn_gateway_id,\n    ),\n    count.index,\n  )\n}\n\n################################################################################\n# Default VPC\n################################################################################\n\nresource \"aws_default_vpc\" \"this\" {\n  count = var.manage_default_vpc ? 1 : 0\n\n  region = var.region\n\n  enable_dns_support   = var.default_vpc_enable_dns_support\n  enable_dns_hostnames = var.default_vpc_enable_dns_hostnames\n\n  tags = merge(\n    { \"Name\" = coalesce(var.default_vpc_name, \"default\") },\n    var.tags,\n    var.default_vpc_tags,\n  )\n}\n\nresource \"aws_default_security_group\" \"this\" {\n  count = local.create_vpc && var.manage_default_security_group ? 1 : 0\n\n  region = var.region\n\n  vpc_id = aws_vpc.this[0].id\n\n  dynamic \"ingress\" {\n    for_each = var.default_security_group_ingress\n    content {\n      self             = lookup(ingress.value, \"self\", null)\n      cidr_blocks      = compact(split(\",\", lookup(ingress.value, \"cidr_blocks\", \"\")))\n      ipv6_cidr_blocks = compact(split(\",\", lookup(ingress.value, \"ipv6_cidr_blocks\", \"\")))\n      prefix_list_ids  = compact(split(\",\", lookup(ingress.value, \"prefix_list_ids\", \"\")))\n      security_groups  = compact(split(\",\", lookup(ingress.value, \"security_groups\", \"\")))\n      description      = lookup(ingress.value, \"description\", null)\n      from_port        = lookup(ingress.value, \"from_port\", 0)\n      to_port          = lookup(ingress.value, \"to_port\", 0)\n      protocol         = lookup(ingress.value, \"protocol\", \"-1\")\n    }\n  }\n\n  dynamic \"egress\" {\n    for_each = var.default_security_group_egress\n    content {\n      self             = lookup(egress.value, \"self\", null)\n      cidr_blocks      = compact(split(\",\", lookup(egress.value, \"cidr_blocks\", \"\")))\n      ipv6_cidr_blocks = compact(split(\",\", lookup(egress.value, \"ipv6_cidr_blocks\", \"\")))\n      prefix_list_ids  = compact(split(\",\", lookup(egress.value, \"prefix_list_ids\", \"\")))\n      security_groups  = compact(split(\",\", lookup(egress.value, \"security_groups\", \"\")))\n      description      = lookup(egress.value, \"description\", null)\n      from_port        = lookup(egress.value, \"from_port\", 0)\n      to_port          = lookup(egress.value, \"to_port\", 0)\n      protocol         = lookup(egress.value, \"protocol\", \"-1\")\n    }\n  }\n\n  tags = merge(\n    { \"Name\" = coalesce(var.default_security_group_name, \"${var.name}-default\") },\n    var.tags,\n    var.default_security_group_tags,\n  )\n}\n\n################################################################################\n# Default Network ACLs\n################################################################################\n\nresource \"aws_default_network_acl\" \"this\" {\n  count = local.create_vpc && var.manage_default_network_acl ? 1 : 0\n\n  region = var.region\n\n  default_network_acl_id = aws_vpc.this[0].default_network_acl_id\n\n  # subnet_ids is using lifecycle ignore_changes, so it is not necessary to list\n  # any explicitly. See https://github.com/terraform-aws-modules/terraform-aws-vpc/issues/736\n  subnet_ids = null\n\n  dynamic \"ingress\" {\n    for_each = var.default_network_acl_ingress\n    content {\n      action          = ingress.value.action\n      cidr_block      = lookup(ingress.value, \"cidr_block\", null)\n      from_port       = ingress.value.from_port\n      icmp_code       = lookup(ingress.value, \"icmp_code\", null)\n      icmp_type       = lookup(ingress.value, \"icmp_type\", null)\n      ipv6_cidr_block = lookup(ingress.value, \"ipv6_cidr_block\", null)\n      protocol        = ingress.value.protocol\n      rule_no         = ingress.value.rule_no\n      to_port         = ingress.value.to_port\n    }\n  }\n  dynamic \"egress\" {\n    for_each = var.default_network_acl_egress\n    content {\n      action          = egress.value.action\n      cidr_block      = lookup(egress.value, \"cidr_block\", null)\n      from_port       = egress.value.from_port\n      icmp_code       = lookup(egress.value, \"icmp_code\", null)\n      icmp_type       = lookup(egress.value, \"icmp_type\", null)\n      ipv6_cidr_block = lookup(egress.value, \"ipv6_cidr_block\", null)\n      protocol        = egress.value.protocol\n      rule_no         = egress.value.rule_no\n      to_port         = egress.value.to_port\n    }\n  }\n\n  tags = merge(\n    { \"Name\" = coalesce(var.default_network_acl_name, \"${var.name}-default\") },\n    var.tags,\n    var.default_network_acl_tags,\n  )\n\n  lifecycle {\n    ignore_changes = [subnet_ids]\n  }\n}\n\n################################################################################\n# Default Route\n################################################################################\n\nresource \"aws_default_route_table\" \"default\" {\n  count = local.create_vpc && var.manage_default_route_table ? 1 : 0\n\n  region = var.region\n\n  default_route_table_id = aws_vpc.this[0].default_route_table_id\n  propagating_vgws       = var.default_route_table_propagating_vgws\n\n  dynamic \"route\" {\n    for_each = var.default_route_table_routes\n    content {\n      # One of the following destinations must be provided\n      cidr_block      = route.value.cidr_block\n      ipv6_cidr_block = lookup(route.value, \"ipv6_cidr_block\", null)\n\n      # One of the following targets must be provided\n      egress_only_gateway_id    = lookup(route.value, \"egress_only_gateway_id\", null)\n      gateway_id                = lookup(route.value, \"gateway_id\", null)\n      instance_id               = lookup(route.value, \"instance_id\", null)\n      nat_gateway_id            = lookup(route.value, \"nat_gateway_id\", null)\n      network_interface_id      = lookup(route.value, \"network_interface_id\", null)\n      transit_gateway_id        = lookup(route.value, \"transit_gateway_id\", null)\n      vpc_endpoint_id           = lookup(route.value, \"vpc_endpoint_id\", null)\n      vpc_peering_connection_id = lookup(route.value, \"vpc_peering_connection_id\", null)\n    }\n  }\n\n  timeouts {\n    create = \"5m\"\n    update = \"5m\"\n  }\n\n  tags = merge(\n    { \"Name\" = coalesce(var.default_route_table_name, \"${var.name}-default\") },\n    var.tags,\n    var.default_route_table_tags,\n  )\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-vpc/main.tf", "hash": "b70b0310cfaeb09a", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "# privileged-psp-users gives the privileged-psp-user role\n# to the group privileged-psp-users.\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n    name: privileged-psp-users\nsubjects:\n- kind: Group\n  apiGroup: rbac.authorization.k8s.io\n  name: privileged-psp-users\nroleRef:\n   apiGroup: rbac.authorization.k8s.io\n   kind: ClusterRole\n   name: privileged-psp-user\n---\n# restricted-psp-users grants the restricted-psp-user role to\n# the groups restricted-psp-users and privileged-psp-users.\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n    name: restricted-psp-users\nsubjects:\n- kind: Group\n  apiGroup: rbac.authorization.k8s.io\n  name: restricted-psp-users\n- kind: Group\n  apiGroup: rbac.authorization.k8s.io\n  name: privileged-psp-users\nroleRef:\n   apiGroup: rbac.authorization.k8s.io\n   kind: ClusterRole\n   name: restricted-psp-user\n---\n# edit grants edit role to the groups\n# restricted-psp-users and privileged-psp-users.\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n    name: edit\nsubjects:\n- kind: Group\n  apiGroup: rbac.authorization.k8s.io\n  name: privileged-psp-users\n- kind: Group\n  apiGroup: rbac.authorization.k8s.io\n  name: restricted-psp-users\nroleRef:\n   apiGroup: rbac.authorization.k8s.io\n   kind: ClusterRole\n   name: edit\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/podsecuritypolicy/rbac/bindings.yaml", "hash": "1b8b3aeb3dfca1fa", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: namespace-namespace\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/Namespace/namespace.yaml", "hash": "16d31bb9adf0b622", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: rbd2\nspec:\n  containers:\n    - image: kubernetes/pause\n      name: rbd-rw\n      volumeMounts:\n      - name: rbdpd\n        mountPath: /mnt/rbd\n  volumes:\n    - name: rbdpd\n      rbd:\n        monitors:\n        - '10.16.154.78:6789'\n        - '10.16.154.82:6789'\n        - '10.16.154.83:6789'\n        pool: kube\n        image: foo\n        fsType: ext4\n        readOnly: true\n        user: admin\n        secretRef:\n          name: ceph-secret\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/rbd/rbd-with-secret.yaml:5"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container rbd-rw is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/rbd/rbd-with-secret.yaml:8"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/volumes/rbd/rbd-with-secret.yaml", "hash": "ffcfd6292f0cf688", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: resource-request-pod\nspec:\n  containers:\n    - name: resource-request-container\n      image: busybox\n      args:\n        - sleep\n        - \"600\"\n      livenessProbe:\n        exec:\n          command:\n            - cat\n            - /tmp/healthy\n        initialDelaySeconds: 5\n        periodSeconds: 5\n      resources:\n        requests:\n          memory: \"20000Mi\"\n          cpu: \"99999m\"\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Pod/spec.containers.resources/resource-request.yaml:6"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container resource-request-container is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Pod/spec.containers.resources/resource-request.yaml:8"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/Pod/spec.containers.resources/resource-request.yaml", "hash": "24747087c2590c97", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: be\nspec:\n  containers:\n  - image: quay.io/connordoyle/cpuset-visualizer\n    name: be\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/cpu-manager/be.yaml:5"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container be is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/cpu-manager/be.yaml:8"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/cpu-manager/be.yaml", "hash": "468560bdc92e00dc", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "kind: Pod\napiVersion: v1\nmetadata:\n  name: pod-uses-shared-hdd-5g\n  labels:\n    name: storage\nspec:\n  containers:\n  - image: nginx\n    name: az-c-01\n    command:\n    - /bin/sh\n    - -c\n    - while true; do echo $(date) >> /mnt/blobdisk/outfile; sleep 1; done\n    volumeMounts:\n    - name: blobdisk01\n      mountPath: /mnt/blobdisk\n  volumes:\n  - name: blobdisk01\n    persistentVolumeClaim:\n      claimName: pv-dd-shared-hdd-5g\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/azure_disk/claim/blob-based-disk/shared-hdd/pod-uses-shared-hdd.yaml:7"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container az-c-01 is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/azure_disk/claim/blob-based-disk/shared-hdd/pod-uses-shared-hdd.yaml:10"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/volumes/azure_disk/claim/blob-based-disk/shared-hdd/pod-uses-shared-hdd.yaml", "hash": "214054905a10a66e", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "terraform {\n  required_version = \">= 1.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \">= 5.92\"\n    }\n  }\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-rds/versions.tf", "hash": "6212b8f03764bcee", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "terraform {\n  required_version = \">= 1.5.7\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \">= 6.5\"\n    }\n  }\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-vpc/wrappers/flow-log/versions.tf", "hash": "29bf8ed1617232a7", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "variable \"create\" {\n  description = \"Determines whether to create a DB instance role association\"\n  type        = bool\n  default     = true\n}\n\nvariable \"feature_name\" {\n  description = \"Name of the feature for association\"\n  type        = string\n  default     = null\n}\n\nvariable \"role_arn\" {\n  description = \"Amazon Resource Name (ARN) of the IAM Role to associate with the DB Instance\"\n  type        = string\n  default     = null\n}\n\nvariable \"db_instance_identifier\" {\n  description = \"The database instance identifier to associate the role\"\n  type        = string\n  default     = null\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-rds/modules/db_instance_role_association/variables.tf", "hash": "17287f574ab5377f", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cinder-web\nspec:\n  containers:\n    - name: web\n      image: nginx\n      ports:\n        - name: web\n          containerPort: 80\n          protocol: tcp\n      volumeMounts:\n        - name: html-volume\n          mountPath: \"/usr/share/nginx/html\"\n  volumes:\n    - name: html-volume\n      cinder:\n        # Enter the volume ID below\n        volumeID: volume_ID\n        fsType: ext4\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/cinder/cinder-web.yaml:5"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container web is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/cinder/cinder-web.yaml:7"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/volumes/cinder/cinder-web.yaml", "hash": "339c2c98be8c3536", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "kind: Service\napiVersion: v1\nmetadata:\n  name: nfs-web\nspec:\n  ports:\n    - port: 80\n  selector:\n    role: web-frontend\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "dangling-service", "severity": "medium", "msg": "Confirm that your service's selector correctly matches the labels on one of your deployments.", "loc": ""}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/volumes/nfs/nfs-web-service.yaml", "hash": "0de4d9416d223caf", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n{{- if .Values.currencyService.create }}\n{{- if .Values.serviceAccounts.create }}\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: {{ .Values.currencyService.name }}\n  namespace: {{.Release.Namespace}}\n  {{- if not .Values.serviceAccounts.annotationsOnlyForCartservice }}\n  {{- with .Values.serviceAccounts.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- end }}\n---\n{{- end }}\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Values.currencyService.name }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    app: {{ .Values.currencyService.name }}\nspec:\n  selector:\n    matchLabels:\n      app: {{ .Values.currencyService.name }}\n  template:\n    metadata:\n      labels:\n        app: {{ .Values.currencyService.name }}\n    spec:\n      {{- if .Values.serviceAccounts.create }}\n      serviceAccountName: {{ .Values.currencyService.name }}\n      {{- else }}\n      serviceAccountName: default\n      {{- end }}\n      terminationGracePeriodSeconds: 5\n      {{- if .Values.securityContext.enable }}\n      securityContext:\n        fsGroup: 1000\n        runAsGroup: 1000\n        runAsNonRoot: true\n        runAsUser: 1000\n        {{- if .Values.seccompProfile.enable }}\n        seccompProfile:\n          type: {{ .Values.seccompProfile.type }}\n        {{- end }}\n      {{- end }}\n      containers:\n      - name: server\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n              - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n        image: {{ .Values.images.repository }}/{{ .Values.currencyService.name }}:{{ .Values.images.tag | default .Chart.AppVersion }}\n        ports:\n        - name: grpc\n          containerPort: 7000\n        env:\n        - name: PORT\n          value: \"7000\"\n        {{- if .Values.opentelemetryCollector.create }}\n        - name: COLLECTOR_SERVICE_ADDR\n          value: \"{{ .Values.opentelemetryCollector.name }}:4317\"\n        - name: OTEL_SERVICE_NAME\n          value: \"{{ .Values.currencyService.name }}\"\n        {{- end }}\n        {{- if .Values.googleCloudOperations.tracing }}\n        - name: ENABLE_TRACING\n          value: \"1\"\n        {{- end }}\n        {{- if not .Values.googleCloudOperations.profiler }}\n        - name: DISABLE_PROFILER\n          value: \"1\"\n        {{- end }}\n        readinessProbe:\n          grpc:\n            port: 7000\n        livenessProbe:\n          grpc:\n            port: 7000\n        resources:\n          {{- toYaml .Values.currencyService.resources | nindent 10 }}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Values.currencyService.name }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    app: {{ .Values.currencyService.name }}\nspec:\n  type: ClusterIP\n  selector:\n    app: {{ .Values.currencyService.name }}\n  ports:\n  - name: grpc\n    port: 7000\n    targetPort: 7000\n{{- if .Values.networkPolicies.create }}\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: {{ .Values.currencyService.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  podSelector:\n    matchLabels:\n      app: {{ .Values.currencyService.name }}\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: {{ .Values.frontend.name }}\n    - podSelector:\n        matchLabels:\n          app: {{ .Values.checkoutService.name }}\n    ports:\n     - port: 7000\n       protocol: TCP\n  egress:\n  - {}\n{{- end }}\n{{- if .Values.sidecars.create }}\n---\napiVersion: networking.istio.io/v1beta1\nkind: Sidecar\nmetadata:\n  name: {{ .Values.currencyService.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  workloadSelector:\n    labels:\n      app: {{ .Values.currencyService.name }}\n  egress:\n  - hosts:\n    - istio-system/*\n    {{- if .Values.opentelemetryCollector.create }}\n    - ./{{ .Values.opentelemetryCollector.name }}.{{ .Release.Namespace }}.svc.cluster.local\n    {{- end }}\n{{- end }}\n{{- if .Values.authorizationPolicies.create }}\n---\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: {{ .Values.currencyService.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  selector:\n    matchLabels:\n      app: {{ .Values.currencyService.name }}\n  rules:\n  - from:\n    - source:\n        principals:\n        {{- if .Values.serviceAccounts.create }}\n        - cluster.local/ns/{{ .Release.Namespace }}/sa/{{ .Values.frontend.name }}\n        - cluster.local/ns/{{ .Release.Namespace }}/sa/{{ .Values.checkoutService.name }}\n        {{- else }}\n        - cluster.local/ns/{{ .Release.Namespace }}/sa/default\n        {{- end }}\n    to:\n    - operation:\n        paths:\n        - /hipstershop.CurrencyService/Convert\n        - /hipstershop.CurrencyService/GetSupportedCurrencies\n        methods:\n        - POST\n        ports:\n        - \"7000\"\n{{- end }}\n{{- end }}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/microservices-demo/helm-chart/templates/currencyservice.yaml", "hash": "5b3b6ae0391ac2b2", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "kind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n   name: portworx-io-priority-high\nprovisioner: kubernetes.io/portworx-volume\nparameters:\n  repl: \"1\"\n  snap_interval:   \"70\"\n  io_priority:  \"high\"\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/volumes/portworx/portworx-volume-sc-high.yaml", "hash": "9e1198792a4904c1", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pvpod\nspec:\n  containers:\n  - name: test-container\n    image: registry.k8s.io/test-webserver\n    volumeMounts:\n    - name: test-volume\n      mountPath: /test-vmdk\n  volumes:\n  - name: test-volume\n    persistentVolumeClaim:\n      claimName: pvcsc001\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/vsphere/vsphere-volume-pvcscpod.yaml:5"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container test-container is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/volumes/vsphere/vsphere-volume-pvcscpod.yaml:7"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/volumes/vsphere/vsphere-volume-pvcscpod.yaml", "hash": "e754d13993aacf9a", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "module \"wrapper\" {\n  source = \"../../modules/flow-log\"\n\n  for_each = var.items\n\n  cloudwatch_log_group_class             = try(each.value.cloudwatch_log_group_class, var.defaults.cloudwatch_log_group_class, null)\n  cloudwatch_log_group_kms_key_id        = try(each.value.cloudwatch_log_group_kms_key_id, var.defaults.cloudwatch_log_group_kms_key_id, null)\n  cloudwatch_log_group_name              = try(each.value.cloudwatch_log_group_name, var.defaults.cloudwatch_log_group_name, null)\n  cloudwatch_log_group_retention_in_days = try(each.value.cloudwatch_log_group_retention_in_days, var.defaults.cloudwatch_log_group_retention_in_days, 90)\n  cloudwatch_log_group_tags              = try(each.value.cloudwatch_log_group_tags, var.defaults.cloudwatch_log_group_tags, {})\n  cloudwatch_log_group_use_name_prefix   = try(each.value.cloudwatch_log_group_use_name_prefix, var.defaults.cloudwatch_log_group_use_name_prefix, true)\n  create                                 = try(each.value.create, var.defaults.create, true)\n  create_cloudwatch_log_group            = try(each.value.create_cloudwatch_log_group, var.defaults.create_cloudwatch_log_group, true)\n  create_iam_role                        = try(each.value.create_iam_role, var.defaults.create_iam_role, true)\n  deliver_cross_account_role             = try(each.value.deliver_cross_account_role, var.defaults.deliver_cross_account_role, null)\n  destination_options                    = try(each.value.destination_options, var.defaults.destination_options, null)\n  eni_id                                 = try(each.value.eni_id, var.defaults.eni_id, null)\n  flow_log_tags                          = try(each.value.flow_log_tags, var.defaults.flow_log_tags, {})\n  iam_role_arn                           = try(each.value.iam_role_arn, var.defaults.iam_role_arn, null)\n  iam_role_description                   = try(each.value.iam_role_description, var.defaults.iam_role_description, null)\n  iam_role_name                          = try(each.value.iam_role_name, var.defaults.iam_role_name, null)\n  iam_role_path                          = try(each.value.iam_role_path, var.defaults.iam_role_path, null)\n  iam_role_permissions                   = try(each.value.iam_role_permissions, var.defaults.iam_role_permissions, null)\n  iam_role_permissions_boundary          = try(each.value.iam_role_permissions_boundary, var.defaults.iam_role_permissions_boundary, null)\n  iam_role_tags                          = try(each.value.iam_role_tags, var.defaults.iam_role_tags, {})\n  iam_role_trust_policy_permissions      = try(each.value.iam_role_trust_policy_permissions, var.defaults.iam_role_trust_policy_permissions, null)\n  iam_role_use_name_prefix               = try(each.value.iam_role_use_name_prefix, var.defaults.iam_role_use_name_prefix, true)\n  kinesis_data_firehose_arn              = try(each.value.kinesis_data_firehose_arn, var.defaults.kinesis_data_firehose_arn, null)\n  log_destination                        = try(each.value.log_destination, var.defaults.log_destination, null)\n  log_destination_type                   = try(each.value.log_destination_type, var.defaults.log_destination_type, \"cloud-watch-logs\")\n  log_format                             = try(each.value.log_format, var.defaults.log_format, null)\n  max_aggregation_interval               = try(each.value.max_aggregation_interval, var.defaults.max_aggregation_interval, null)\n  name                                   = try(each.value.name, var.defaults.name, \"\")\n  region                                 = try(each.value.region, var.defaults.region, null)\n  subnet_id                              = try(each.value.subnet_id, var.defaults.subnet_id, null)\n  tags                                   = try(each.value.tags, var.defaults.tags, {})\n  traffic_type                           = try(each.value.traffic_type, var.defaults.traffic_type, \"ALL\")\n  transit_gateway_attachment_id          = try(each.value.transit_gateway_attachment_id, var.defaults.transit_gateway_attachment_id, null)\n  transit_gateway_id                     = try(each.value.transit_gateway_id, var.defaults.transit_gateway_id, null)\n  vpc_id                                 = try(each.value.vpc_id, var.defaults.vpc_id, null)\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-vpc/wrappers/flow-log/main.tf", "hash": "bf0559c9f60cb10b", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "terraform {\n  required_version = \">= 1.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \">= 5.92\"\n    }\n  }\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-rds/wrappers/db_instance_automated_backups_replication/versions.tf", "hash": "6212b8f03764bcee", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: server\nspec:\n  replicas: 1\n  selector:\n    role: server\n  template:\n    metadata:\n      labels:\n        role: server\n    spec:\n      containers:\n      - name: server\n        image: nginx\n        volumeMounts:\n          - mountPath: /var/lib/www/html\n            name: cinderpvc\n      volumes:\n        - name: cinderpvc\n          persistentVolumeClaim:\n            claimName: claim1\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/persistent-volume-provisioning/cinder/example-pod.yaml:13"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container server is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/persistent-volume-provisioning/cinder/example-pod.yaml:15"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/persistent-volume-provisioning/cinder/example-pod.yaml", "hash": "6b1a854d81fa1f60", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "---\n# https://kubernetes.io/docs/concepts/services-networking/service/#externalname\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-external-name-service\nspec:\n  type: ExternalName\n  externalName: my.database.example.com\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/Service/spec.externalName/external-name.yaml", "hash": "4ec02d002ebf7aaf", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\n$patch: delete\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "mismatching-selector", "severity": "medium", "msg": "Confirm that your deployment selector correctly matches the labels in its pod template.", "loc": ""}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/microservices-demo/kustomize/components/without-loadgenerator/delete-loadgenerator.patch.yaml", "hash": "6d9964a977266ee6", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "provider \"aws\" {\n  region = local.region\n}\n\ndata \"aws_availability_zones\" \"available\" {}\n\nlocals {\n  name   = \"ex-${basename(path.cwd)}\"\n  region = \"eu-west-1\"\n\n  vpc_cidr = \"10.0.0.0/16\"\n  azs      = slice(data.aws_availability_zones.available.names, 0, 3)\n\n  tags = {\n    Example    = local.name\n    GithubRepo = \"terraform-aws-vpc\"\n    GithubOrg  = \"terraform-aws-modules\"\n  }\n}\n\n################################################################################\n# VPC Module\n################################################################################\n\nmodule \"vpc\" {\n  source = \"../../\"\n\n  name = local.name\n  cidr = local.vpc_cidr\n\n  azs             = local.azs\n  private_subnets = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 4, k)]\n\n  tags = local.tags\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-vpc/examples/simple/main.tf", "hash": "03a1104dac56d099", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "output \"wrapper\" {\n  description = \"Map of outputs of a wrapper.\"\n  value       = module.wrapper\n  # sensitive = false # No sensitive module output found\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-vpc/wrappers/flow-log/outputs.tf", "hash": "602ebe5b9cff0659", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: network-policy-default-allow-ingress\nspec:\n  podSelector: {}\n  ingress:\n    - {}\n  policyTypes:\n    - Ingress\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/NetworkPolicy/spec.ingress/default-allow-ingress.yaml", "hash": "51e2088fb54e64da", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "# Copyright 2023 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\napiVersion: kustomize.config.k8s.io/v1alpha1\nkind: Component\nresources:\n  - allow-egress-googleapis.yaml\n  - frontend-gateway.yaml\n  - frontend.yaml\npatches:\n# frontend - delete frontend-external service (same as non-public-frontend component)\n- patch: |-\n    apiVersion: v1\n    kind: Service\n    metadata:\n      name: frontend-external\n    $patch: delete\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/microservices-demo/kustomize/components/service-mesh-istio/kustomization.yaml", "hash": "1d735cc45b557fe0", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: network-policy-default-deny-ingress\nspec:\n  podSelector: {}\n  policyTypes:\n    - Ingress\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/NetworkPolicy/spec.policyTypes/default-deny-ingress.yaml", "hash": "ead509946dd083c4", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n    securityContext:\n      privileged: true\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "privilege-escalation-container", "severity": "medium", "msg": "Ensure containers do not allow privilege escalation by setting allowPrivilegeEscalation=false, privileged=false and removing CAP_SYS_ADMIN capability. See https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for more details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "privileged-container", "severity": "medium", "msg": "Do not run your container as privileged unless it is required.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/podsecuritypolicy/rbac/pod_priv.yaml:7"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.privileged-container.privileged-container", "severity": "WARNING", "msg": "Container or pod is running in privileged mode. This grants the container the equivalent of root capabilities on the host machine. This can lead to container escapes, privilege escalation, and other security concerns. Remove the 'privileged' key to disable this capability.", "loc": "scripts/data/sources/kubernetes/examples/_archived/podsecuritypolicy/rbac/pod_priv.yaml:9"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container nginx is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/podsecuritypolicy/rbac/pod_priv.yaml:9"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.allow-privilege-escalation.allow-privilege-escalation", "severity": "WARNING", "msg": "In Kubernetes, each pod runs in its own isolated environment with its own set of security policies. However, certain container images may contain `setuid` or `setgid` binaries that could allow an attacker to perform privilege escalation and gain access to sensitive resources. To mitigate this risk, it's recommended to add a `securityContext` to the container in the pod, with the parameter `allowPrivilegeEscalation` set to `false`. This will prevent the container from running any privileged processes and limit the impact of any potential attacks. By adding the `allowPrivilegeEscalation` parameter to your the `securityContext`, you can help to ensure that your containerized applications are more secure and less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/podsecuritypolicy/rbac/pod_priv.yaml:13"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/podsecuritypolicy/rbac/pod_priv.yaml", "hash": "ca7b4113ec9d2fc8", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "---\n# https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: jobs-timeout-job\nspec:\n  activeDeadlineSeconds: 100\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n        - command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n          image: perl\n          name: jobs-timeout-container\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "job-ttl-seconds-after-finished", "severity": "medium", "msg": "Set Job.spec.ttlSecondsAfterFinished. Unset CronJob.Spec.JobTemplate.Spec.ttlSecondsAfterFinished.", "loc": ""}, {"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Job/spec.activeDeadlineSeconds/timeout.yaml:10"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container jobs-timeout-container is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Job/spec.activeDeadlineSeconds/timeout.yaml:15"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/Job/spec.activeDeadlineSeconds/timeout.yaml", "hash": "c68afa1d0261df9c", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "---\n# https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-load-balancer-service\nspec:\n  selector:\n    app: MyApp\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n  type: LoadBalancer\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "dangling-service", "severity": "medium", "msg": "Confirm that your service's selector correctly matches the labels on one of your deployments.", "loc": ""}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/Service/spec.type/load-balancer.yaml", "hash": "ef963a88d8003d76", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: dns-backend\n  labels:\n    name: dns-backend\nspec:\n  replicas: 1\n  selector:\n    name: dns-backend\n  template:\n    metadata:\n      labels:\n        name: dns-backend\n    spec:\n      containers:\n        - name: dns-backend\n          image: registry.k8s.io/example-dns-backend:v2\n          ports:\n            - name: backend-port\n              containerPort: 8000\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/cluster-dns/dns-backend-rc.yaml:15"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container dns-backend is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/cluster-dns/dns-backend-rc.yaml:17"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/cluster-dns/dns-backend-rc.yaml", "hash": "bdc17780f775a0d4", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: apps/v1  # for k8s versions before 1.9.0 use apps/v1beta2  and before 1.8.0 use extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: storm-worker-controller\n  labels:\n     name: storm-worker\nspec:\n  replicas: 2\n  selector:\n     matchLabels:\n        name: storm-worker\n        uses: nimbus\n  template:\n    metadata:\n      labels:\n        name: storm-worker\n        uses: nimbus\n    spec:\n      containers:\n      - name: storm-worke\n        image: mattf/storm-worker\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        ports:\n        - hostPort: 6700\n          containerPort: 6700\n        - hostPort: 6701\n          containerPort: 6701\n        - hostPort: 6702\n          containerPort: 6702\n        - hostPort: 6703\n          containerPort: 6703\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-anti-affinity", "severity": "medium", "msg": "Specify anti-affinity in your pod specification to ensure that the orchestrator attempts to schedule replicas on different nodes. Using podAntiAffinity, specify a labelSelector that matches pods for the deployment, and set the topologyKey to kubernetes.io/hostname. Refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/storm/storm-worker-controller.yaml:18"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container storm-worke is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/storm/storm-worker-controller.yaml:20"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/storm/storm-worker-controller.yaml", "hash": "25b2e1f5e8b41ec7", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: v1\nkind: Service\nmetadata:\n  name: pxc-node1\n  labels:\n    node: pxc-node1\nspec:\n  ports:\n    - port: 3306\n      name: mysql\n    - port: 4444\n      name: state-snapshot-transfer\n    - port: 4567\n      name: replication-traffic\n    - port: 4568\n      name: incremental-state-transfer\n  selector:\n    node: pxc-node1\n---\napiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: pxc-node1\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        node: pxc-node1\n        unit: pxc-cluster\n    spec:\n      containers:\n        - resources:\n            limits:\n              cpu: 0.3\n          image: capttofu/percona_xtradb_cluster_5_6:beta\n          name: pxc-node1\n          ports:\n            - containerPort: 3306\n            - containerPort: 4444\n            - containerPort: 4567\n            - containerPort: 4568\n          env:\n            - name: GALERA_CLUSTER\n              value: \"true\"\n            - name: WSREP_CLUSTER_ADDRESS\n              value: gcomm://\n            - name: WSREP_SST_USER\n              value: sst\n            - name: WSREP_SST_PASSWORD\n              value: sst\n            - name: MYSQL_USER\n              value: mysql\n            - name: MYSQL_PASSWORD\n              value: mysql\n            - name: MYSQL_ROOT_PASSWORD\n              value: c-krit\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/storage/mysql-galera/pxc-node1.yaml:31"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container pxc-node1 is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/storage/mysql-galera/pxc-node1.yaml:37"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/storage/mysql-galera/pxc-node1.yaml", "hash": "e9d12a6660dc26ba", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\napiVersion: kustomize.config.k8s.io/v1alpha1\nkind: Component\nimages:\n- name: us-central1-docker.pkg.dev/google-samples/microservices-demo/adservice\n  newTag: CONTAINER_IMAGES_TAG\n- name: us-central1-docker.pkg.dev/google-samples/microservices-demo/cartservice\n  newTag: CONTAINER_IMAGES_TAG\n- name: us-central1-docker.pkg.dev/google-samples/microservices-demo/checkoutservice\n  newTag: CONTAINER_IMAGES_TAG\n- name: us-central1-docker.pkg.dev/google-samples/microservices-demo/currencyservice\n  newTag: CONTAINER_IMAGES_TAG\n- name: us-central1-docker.pkg.dev/google-samples/microservices-demo/emailservice\n  newTag: CONTAINER_IMAGES_TAG\n- name: us-central1-docker.pkg.dev/google-samples/microservices-demo/frontend\n  newTag: CONTAINER_IMAGES_TAG\n- name: us-central1-docker.pkg.dev/google-samples/microservices-demo/loadgenerator\n  newTag: CONTAINER_IMAGES_TAG\n- name: us-central1-docker.pkg.dev/google-samples/microservices-demo/paymentservice\n  newTag: CONTAINER_IMAGES_TAG\n- name: us-central1-docker.pkg.dev/google-samples/microservices-demo/productcatalogservice\n  newTag: CONTAINER_IMAGES_TAG\n- name: us-central1-docker.pkg.dev/google-samples/microservices-demo/recommendationservice\n  newTag: CONTAINER_IMAGES_TAG\n- name: us-central1-docker.pkg.dev/google-samples/microservices-demo/shippingservice\n  newTag: CONTAINER_IMAGES_TAG\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/microservices-demo/kustomize/components/container-images-tag/kustomization.yaml", "hash": "0bd7b79a55f9a5d1", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n{{- if .Values.paymentService.create }}\n{{- if .Values.serviceAccounts.create }}\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: {{ .Values.paymentService.name }}\n  namespace: {{.Release.Namespace}}\n  {{- if not .Values.serviceAccounts.annotationsOnlyForCartservice }}\n  {{- with .Values.serviceAccounts.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- end }}\n---\n{{- end }}\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Values.paymentService.name }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    app: {{ .Values.paymentService.name }}\nspec:\n  selector:\n    matchLabels:\n      app: {{ .Values.paymentService.name }}\n  template:\n    metadata:\n      labels:\n        app: {{ .Values.paymentService.name }}\n    spec:\n      {{- if .Values.serviceAccounts.create }}\n      serviceAccountName: {{ .Values.paymentService.name }}\n      {{- else }}\n      serviceAccountName: default\n      {{- end }}\n      terminationGracePeriodSeconds: 5\n      {{- if .Values.securityContext.enable }}\n      securityContext:\n        fsGroup: 1000\n        runAsGroup: 1000\n        runAsNonRoot: true\n        runAsUser: 1000\n        {{- if .Values.seccompProfile.enable }}\n        seccompProfile:\n          type: {{ .Values.seccompProfile.type }}\n        {{- end }}\n      {{- end }}\n      containers:\n      - name: server\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n              - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n        image: {{ .Values.images.repository }}/{{ .Values.paymentService.name }}:{{ .Values.images.tag | default .Chart.AppVersion }}\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: \"50051\"\n        {{- if .Values.opentelemetryCollector.create }}\n        - name: COLLECTOR_SERVICE_ADDR\n          value: \"{{ .Values.opentelemetryCollector.name }}:4317\"\n        - name: OTEL_SERVICE_NAME\n          value: \"{{ .Values.paymentService.name }}\"\n        {{- end }}\n        {{- if .Values.googleCloudOperations.tracing }}\n        - name: ENABLE_TRACING\n          value: \"1\"\n        {{- end }}\n        {{- if not .Values.googleCloudOperations.profiler }}\n        - name: DISABLE_PROFILER\n          value: \"1\"\n        {{- end }}\n        readinessProbe:\n          grpc:\n            port: 50051\n        livenessProbe:\n          grpc:\n            port: 50051\n        resources:\n          {{- toYaml .Values.paymentService.resources | nindent 10 }}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Values.paymentService.name }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    app: {{ .Values.paymentService.name }}\nspec:\n  type: ClusterIP\n  selector:\n    app: {{ .Values.paymentService.name }}\n  ports:\n  - name: grpc\n    port: 50051\n    targetPort: 50051\n{{- if .Values.networkPolicies.create }}\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: {{ .Values.paymentService.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  podSelector:\n    matchLabels:\n      app: {{ .Values.paymentService.name }}\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: {{ .Values.checkoutService.name }}\n    ports:\n     - port: 50051\n       protocol: TCP\n  egress:\n  - {}\n{{- end }}\n{{- if .Values.sidecars.create }}\n---\napiVersion: networking.istio.io/v1beta1\nkind: Sidecar\nmetadata:\n  name: {{ .Values.paymentService.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  workloadSelector:\n    labels:\n      app: {{ .Values.paymentService.name }}\n  egress:\n  - hosts:\n    - istio-system/*\n    {{- if .Values.opentelemetryCollector.create }}\n    - ./{{ .Values.opentelemetryCollector.name }}.{{ .Release.Namespace }}.svc.cluster.local\n    {{- end }}\n{{- end }}\n{{- if .Values.authorizationPolicies.create }}\n---\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: {{ .Values.paymentService.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  selector:\n    matchLabels:\n      app: {{ .Values.paymentService.name }}\n  rules:\n  - from:\n    - source:\n        principals:\n        {{- if .Values.serviceAccounts.create }}\n        - cluster.local/ns/{{ .Release.Namespace }}/sa/{{ .Values.checkoutService.name }}\n        {{- else }}\n        - cluster.local/ns/{{ .Release.Namespace }}/sa/default\n        {{- end }}\n    to:\n    - operation:\n        paths:\n        - /hipstershop.PaymentService/Charge\n        methods:\n        - POST\n        ports:\n        - \"50051\"\n{{- end }}\n{{- end }}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/microservices-demo/helm-chart/templates/paymentservice.yaml", "hash": "c0fb2b9602a27990", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "kind: ReplicationController\napiVersion: v1\nmetadata:\n  name: vtctld\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        component: vtctld\n        app: vitess\n    spec:\n      containers:\n        - name: vtctld\n          image: vitess/lite:v2.0.0-alpha5\n          volumeMounts:\n            - name: syslog\n              mountPath: /dev/log\n            - name: vtdataroot\n              mountPath: /vt/vtdataroot\n            - name: certs\n              readOnly: true\n              mountPath: /etc/ssl/certs\n          resources:\n            limits:\n              memory: \"128Mi\"\n              cpu: \"100m\"\n          command:\n            - sh\n            - \"-c\"\n            - >-\n              mkdir -p $VTDATAROOT/tmp &&\n              chown -R vitess /vt &&\n              su -p -c \"/vt/bin/vtctld\n              -debug\n              -templates $VTTOP/go/cmd/vtctld/templates\n              -web_dir $VTTOP/web/vtctld\n              -log_dir $VTDATAROOT/tmp\n              -alsologtostderr\n              -port 15000\n              -grpc_port 15001\n              -service_map 'grpc-vtctl'\n              -topo_implementation etcd\n              -tablet_protocol grpc\n              -tablet_manager_protocol grpc\n              -etcd_global_addrs http://$ETCD_GLOBAL_SERVICE_HOST:$ETCD_GLOBAL_SERVICE_PORT\n              {{backup_flags}}\" vitess\n      volumes:\n        - name: syslog\n          hostPath: {path: /dev/log}\n        - name: vtdataroot\n          emptyDir: {}\n        - name: certs\n          hostPath: {path: /etc/ssl/certs}\n\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/storage/vitess/vtctld-controller-template.yaml:12"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container vtctld is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/storage/vitess/vtctld-controller-template.yaml:14"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/storage/vitess/vtctld-controller-template.yaml", "hash": "2de6df31116c26ea", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: exclusive-4\nspec:\n  containers:\n  - image: quay.io/connordoyle/cpuset-visualizer\n    name: exclusive-4\n    resources:\n      requests:\n        cpu: 4\n        memory: \"256M\"\n      limits:\n        cpu: 4\n        memory: \"256M\"\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/cpu-manager/exclusive-4.yaml:5"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container exclusive-4 is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/cpu-manager/exclusive-4.yaml:8"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/cpu-manager/exclusive-4.yaml", "hash": "2f2b859e4f83a08b", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "module \"wrapper\" {\n  source = \"../\"\n\n  for_each = var.items\n\n  allocated_storage                                      = try(each.value.allocated_storage, var.defaults.allocated_storage, null)\n  allow_major_version_upgrade                            = try(each.value.allow_major_version_upgrade, var.defaults.allow_major_version_upgrade, false)\n  apply_immediately                                      = try(each.value.apply_immediately, var.defaults.apply_immediately, false)\n  auto_minor_version_upgrade                             = try(each.value.auto_minor_version_upgrade, var.defaults.auto_minor_version_upgrade, true)\n  availability_zone                                      = try(each.value.availability_zone, var.defaults.availability_zone, null)\n  backup_retention_period                                = try(each.value.backup_retention_period, var.defaults.backup_retention_period, null)\n  backup_window                                          = try(each.value.backup_window, var.defaults.backup_window, null)\n  blue_green_update                                      = try(each.value.blue_green_update, var.defaults.blue_green_update, {})\n  ca_cert_identifier                                     = try(each.value.ca_cert_identifier, var.defaults.ca_cert_identifier, null)\n  character_set_name                                     = try(each.value.character_set_name, var.defaults.character_set_name, null)\n  cloudwatch_log_group_class                             = try(each.value.cloudwatch_log_group_class, var.defaults.cloudwatch_log_group_class, null)\n  cloudwatch_log_group_kms_key_id                        = try(each.value.cloudwatch_log_group_kms_key_id, var.defaults.cloudwatch_log_group_kms_key_id, null)\n  cloudwatch_log_group_retention_in_days                 = try(each.value.cloudwatch_log_group_retention_in_days, var.defaults.cloudwatch_log_group_retention_in_days, 7)\n  cloudwatch_log_group_skip_destroy                      = try(each.value.cloudwatch_log_group_skip_destroy, var.defaults.cloudwatch_log_group_skip_destroy, null)\n  cloudwatch_log_group_tags                              = try(each.value.cloudwatch_log_group_tags, var.defaults.cloudwatch_log_group_tags, {})\n  copy_tags_to_snapshot                                  = try(each.value.copy_tags_to_snapshot, var.defaults.copy_tags_to_snapshot, false)\n  create_cloudwatch_log_group                            = try(each.value.create_cloudwatch_log_group, var.defaults.create_cloudwatch_log_group, false)\n  create_db_instance                                     = try(each.value.create_db_instance, var.defaults.create_db_instance, true)\n  create_db_option_group                                 = try(each.value.create_db_option_group, var.defaults.create_db_option_group, true)\n  create_db_parameter_group                              = try(each.value.create_db_parameter_group, var.defaults.create_db_parameter_group, true)\n  create_db_subnet_group                                 = try(each.value.create_db_subnet_group, var.defaults.create_db_subnet_group, false)\n  create_monitoring_role                                 = try(each.value.create_monitoring_role, var.defaults.create_monitoring_role, false)\n  custom_iam_instance_profile                            = try(each.value.custom_iam_instance_profile, var.defaults.custom_iam_instance_profile, null)\n  database_insights_mode                                 = try(each.value.database_insights_mode, var.defaults.database_insights_mode, null)\n  db_instance_role_associations                          = try(each.value.db_instance_role_associations, var.defaults.db_instance_role_associations, {})\n  db_instance_tags                                       = try(each.value.db_instance_tags, var.defaults.db_instance_tags, {})\n  db_name                                                = try(each.value.db_name, var.defaults.db_name, null)\n  db_option_group_tags                                   = try(each.value.db_option_group_tags, var.defaults.db_option_group_tags, {})\n  db_parameter_group_tags                                = try(each.value.db_parameter_group_tags, var.defaults.db_parameter_group_tags, {})\n  db_subnet_group_description                            = try(each.value.db_subnet_group_description, var.defaults.db_subnet_group_description, null)\n  db_subnet_group_name                                   = try(each.value.db_subnet_group_name, var.defaults.db_subnet_group_name, null)\n  db_subnet_group_tags                                   = try(each.value.db_subnet_group_tags, var.defaults.db_subnet_group_tags, {})\n  db_subnet_group_use_name_prefix                        = try(each.value.db_subnet_group_use_name_prefix, var.defaults.db_subnet_group_use_name_prefix, true)\n  dedicated_log_volume                                   = try(each.value.dedicated_log_volume, var.defaults.dedicated_log_volume, false)\n  delete_automated_backups                               = try(each.value.delete_automated_backups, var.defaults.delete_automated_backups, true)\n  deletion_protection                                    = try(each.value.deletion_protection, var.defaults.deletion_protection, false)\n  domain                                                 = try(each.value.domain, var.defaults.domain, null)\n  domain_auth_secret_arn                                 = try(each.value.domain_auth_secret_arn, var.defaults.domain_auth_secret_arn, null)\n  domain_dns_ips                                         = try(each.value.domain_dns_ips, var.defaults.domain_dns_ips, null)\n  domain_fqdn                                            = try(each.value.domain_fqdn, var.defaults.domain_fqdn, null)\n  domain_iam_role_name                                   = try(each.value.domain_iam_role_name, var.defaults.domain_iam_role_name, null)\n  domain_ou                                              = try(each.value.domain_ou, var.defaults.domain_ou, null)\n  enabled_cloudwatch_logs_exports                        = try(each.value.enabled_cloudwatch_logs_exports, var.defaults.enabled_cloudwatch_logs_exports, [])\n  engine                                                 = try(each.value.engine, var.defaults.engine, null)\n  engine_lifecycle_support                               = try(each.value.engine_lifecycle_support, var.defaults.engine_lifecycle_support, null)\n  engine_version                                         = try(each.value.engine_version, var.defaults.engine_version, null)\n  family                                                 = try(each.value.family, var.defaults.family, null)\n  final_snapshot_identifier_prefix                       = try(each.value.final_snapshot_identifier_prefix, var.defaults.final_snapshot_identifier_prefix, \"final\")\n  iam_database_authentication_enabled                    = try(each.value.iam_database_authentication_enabled, var.defaults.iam_database_authentication_enabled, false)\n  identifier                                             = try(each.value.identifier, var.defaults.identifier)\n  instance_class                                         = try(each.value.instance_class, var.defaults.instance_class, null)\n  instance_use_identifier_prefix                         = try(each.value.instance_use_identifier_prefix, var.defaults.instance_use_identifier_prefix, false)\n  iops                                                   = try(each.value.iops, var.defaults.iops, null)\n  kms_key_id                                             = try(each.value.kms_key_id, var.defaults.kms_key_id, null)\n  license_model                                          = try(each.value.license_model, var.defaults.license_model, null)\n  maintenance_window                                     = try(each.value.maintenance_window, var.defaults.maintenance_window, null)\n  major_engine_version                                   = try(each.value.major_engine_version, var.defaults.major_engine_version, null)\n  manage_master_user_password                            = try(each.value.manage_master_user_password, var.defaults.manage_master_user_password, true)\n  manage_master_user_password_rotation                   = try(each.value.manage_master_user_password_rotation, var.defaults.manage_master_user_password_rotation, false)\n  master_user_password_rotate_immediately                = try(each.value.master_user_password_rotate_immediately, var.defaults.master_user_password_rotate_immediately, null)\n  master_user_password_rotation_automatically_after_days = try(each.value.master_user_password_rotation_automatically_after_days, var.defaults.master_user_password_rotation_automatically_after_days, null)\n  master_user_password_rotation_duration                 = try(each.value.master_user_password_rotation_duration, var.defaults.master_user_password_rotation_duration, null)\n  master_user_password_rotation_schedule_expression      = try(each.value.master_user_password_rotation_schedule_expression, var.defaults.master_user_password_rotation_schedule_expression, null)\n  master_user_secret_kms_key_id                          = try(each.value.master_user_secret_kms_key_id, var.defaults.master_user_secret_kms_key_id, null)\n  max_allocated_storage                                  = try(each.value.max_allocated_storage, var.defaults.max_allocated_storage, 0)\n  monitoring_interval                                    = try(each.value.monitoring_interval, var.defaults.monitoring_interval, 0)\n  monitoring_role_arn                                    = try(each.value.monitoring_role_arn, var.defaults.monitoring_role_arn, null)\n  monitoring_role_description                            = try(each.value.monitoring_role_description, var.defaults.monitoring_role_description, null)\n  monitoring_role_name                                   = try(each.value.monitoring_role_name, var.defaults.monitoring_role_name, \"rds-monitoring-role\")\n  monitoring_role_permissions_boundary                   = try(each.value.monitoring_role_permissions_boundary, var.defaults.monitoring_role_permissions_boundary, null)\n  monitoring_role_use_name_prefix                        = try(each.value.monitoring_role_use_name_prefix, var.defaults.monitoring_role_use_name_prefix, false)\n  multi_az                                               = try(each.value.multi_az, var.defaults.multi_az, false)\n  nchar_character_set_name                               = try(each.value.nchar_character_set_name, var.defaults.nchar_character_set_name, null)\n  network_type                                           = try(each.value.network_type, var.defaults.network_type, null)\n  option_group_description                               = try(each.value.option_group_description, var.defaults.option_group_description, null)\n  option_group_name                                      = try(each.value.option_group_name, var.defaults.option_group_name, null)\n  option_group_skip_destroy                              = try(each.value.option_group_skip_destroy, var.defaults.option_group_skip_destroy, null)\n  option_group_timeouts                                  = try(each.value.option_group_timeouts, var.defaults.option_group_timeouts, {})\n  option_group_use_name_prefix                           = try(each.value.option_group_use_name_prefix, var.defaults.option_group_use_name_prefix, true)\n  options                                                = try(each.value.options, var.defaults.options, [])\n  parameter_group_description                            = try(each.value.parameter_group_description, var.defaults.parameter_group_description, null)\n  parameter_group_name                                   = try(each.value.parameter_group_name, var.defaults.parameter_group_name, null)\n  parameter_group_skip_destroy                           = try(each.value.parameter_group_skip_destroy, var.defaults.parameter_group_skip_destroy, null)\n  parameter_group_use_name_prefix                        = try(each.value.parameter_group_use_name_prefix, var.defaults.parameter_group_use_name_prefix, true)\n  parameters                                             = try(each.value.parameters, var.defaults.parameters, [])\n  password                                               = try(each.value.password, var.defaults.password, null)\n  performance_insights_enabled                           = try(each.value.performance_insights_enabled, var.defaults.performance_insights_enabled, false)\n  performance_insights_kms_key_id                        = try(each.value.performance_insights_kms_key_id, var.defaults.performance_insights_kms_key_id, null)\n  performance_insights_retention_period                  = try(each.value.performance_insights_retention_period, var.defaults.performance_insights_retention_period, 7)\n  port                                                   = try(each.value.port, var.defaults.port, null)\n  publicly_accessible                                    = try(each.value.publicly_accessible, var.defaults.publicly_accessible, false)\n  putin_khuylo                                           = try(each.value.putin_khuylo, var.defaults.putin_khuylo, true)\n  replica_mode                                           = try(each.value.replica_mode, var.defaults.replica_mode, null)\n  replicate_source_db                                    = try(each.value.replicate_source_db, var.defaults.replicate_source_db, null)\n  restore_to_point_in_time                               = try(each.value.restore_to_point_in_time, var.defaults.restore_to_point_in_time, null)\n  s3_import                                              = try(each.value.s3_import, var.defaults.s3_import, null)\n  skip_final_snapshot                                    = try(each.value.skip_final_snapshot, var.defaults.skip_final_snapshot, false)\n  snapshot_identifier                                    = try(each.value.snapshot_identifier, var.defaults.snapshot_identifier, null)\n  storage_encrypted                                      = try(each.value.storage_encrypted, var.defaults.storage_encrypted, true)\n  storage_throughput                                     = try(each.value.storage_throughput, var.defaults.storage_throughput, null)\n  storage_type                                           = try(each.value.storage_type, var.defaults.storage_type, null)\n  subnet_ids                                             = try(each.value.subnet_ids, var.defaults.subnet_ids, [])\n  tags                                                   = try(each.value.tags, var.defaults.tags, {})\n  timeouts                                               = try(each.value.timeouts, var.defaults.timeouts, {})\n  timezone                                               = try(each.value.timezone, var.defaults.timezone, null)\n  upgrade_storage_config                                 = try(each.value.upgrade_storage_config, var.defaults.upgrade_storage_config, null)\n  username                                               = try(each.value.username, var.defaults.username, null)\n  vpc_security_group_ids                                 = try(each.value.vpc_security_group_ids, var.defaults.vpc_security_group_ids, [])\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-rds/wrappers/main.tf", "hash": "a02893a620a9a484", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "---\n# Taken from: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-hostname-and-subdomain-fields\n# Currently when a pod is created, its hostname is the Pod's metadata.name value.\n# The Pod spec has an optional hostname field, which can be used to specify the Pod's hostname.\n# When specified, it takes precedence over the Pod's name to be the hostname of the pod.\n# For example, given a Pod with hostname set to \"my-host\", the Pod will have its hostname set to \"my-host\".\n# The Pod spec also has an optional subdomain field which can be used to specify its subdomain.\n# For example, a Pod with hostname set to \"foo\", and subdomain set to \"bar\", in namespace\n# \"default\", will have the fully qualified domain name (FQDN) \"foo.bar.default.svc.cluster-domain.example\".\n#\n# If there exists a headless service in the same namespace as the pod and with the same name as the subdomain,\n# the cluster's DNS Server also returns an A or AAAA record for the Pod's fully qualified hostname.\n# For example, given a Pod with the hostname set to \"subdomain-simple-hostname-1\" and the subdomain\n# set to \"subdomain-simple-subdomain-service\", and a headless Service named \"subdomain-simple-subdomain-service\"\n# in the same namespace, the pod will see its own FQDN as\n# \"subdomain-simple-hostname-1.subdomain-simple-subdomain-service.default.svc.cluster-domain.example\".\n# DNS serves an A or AAAA record at that name, pointing to the Pod's IP.\n# Both pods \"subdomain-simple-pod-1\" and \"subdomain-simple-pod-2\" can have their distinct A or AAAA records.\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: subdomain-simple-subdomain-service\nspec:\n  clusterIP: None  # A headless service\n  ports:\n    - name: subdomain-simple-port-name  # Actually, no port is needed.\n      port: 1234\n      targetPort: 1234\n  selector:\n    name: subdomain-simple-selector\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: subdomain-simple-selector\n  name: subdomain-simple-pod-1\nspec:\n  containers:\n    - command:\n        - sleep\n        - \"3600\"\n      image: busybox\n      name: subdomain-simple-container-1\n  hostname: subdomain-simple-hostname-1\n  subdomain: subdomain-simple-subdomain-service\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: subdomain-simple-pod-2\n  labels:\n    name: subdomain-simple-selector\nspec:\n  containers:\n    - command:\n        - sleep\n        - \"3600\"\n      image: busybox\n      name: subdomain-simple-container-2\n  hostname: subdomain-simple-hostname-2\n  subdomain: subdomain-simple-subdomain-service\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Service/Pod.spec.subdomain/subdomain.yaml:40"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container subdomain-simple-container-1 is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Service/Pod.spec.subdomain/subdomain.yaml:46"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Service/Pod.spec.subdomain/subdomain.yaml:56"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container subdomain-simple-container-2 is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Service/Pod.spec.subdomain/subdomain.yaml:62"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/Service/Pod.spec.subdomain/subdomain.yaml", "hash": "fcfcb75f58622e4d", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "################################################################################\n# Node IAM Role\n################################################################################\n\noutput \"name\" {\n  description = \"The name of the node IAM role\"\n  value       = try(aws_iam_role.this[0].name, null)\n}\n\noutput \"arn\" {\n  description = \"The Amazon Resource Name (ARN) specifying the node IAM role\"\n  value       = try(aws_iam_role.this[0].arn, null)\n}\n\noutput \"unique_id\" {\n  description = \"Stable and unique string identifying the node IAM role\"\n  value       = try(aws_iam_role.this[0].unique_id, null)\n}\n\n################################################################################\n# Intermedaite IAM Role\n################################################################################\n\noutput \"intermediate_role_name\" {\n  description = \"The name of the node IAM role\"\n  value       = try(aws_iam_role.intermediate[0].name, null)\n}\n\noutput \"intermediate_role_arn\" {\n  description = \"The Amazon Resource Name (ARN) specifying the node IAM role\"\n  value       = try(aws_iam_role.intermediate[0].arn, null)\n}\n\noutput \"intermediate_role_unique_id\" {\n  description = \"Stable and unique string identifying the node IAM role\"\n  value       = try(aws_iam_role.intermediate[0].unique_id, null)\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-eks/modules/hybrid-node-role/outputs.tf", "hash": "3477098b74c65bf3", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
{"question": "---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: debug-network-pod\nspec:\n  containers:\n    - command:\n        - sleep\n        - \"3600\"\n      image: praqma/network-multitool\n      name: debug-network-container\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Pod/debug-network.yaml:6"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container debug-network-container is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Pod/debug-network.yaml:12"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/Pod/debug-network.yaml", "hash": "b2cf6d9fe714db86", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-model-pvc\nspec:\n  accessModes:\n    - ReadOnlyMany\n  resources:\n    requests:\n      storage: 1Gi\n  volumeName: my-model-pv\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/AI/model-serving-tensorflow/pvc.yaml", "hash": "c918e443bdf2c746", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: frontend\n  annotations:\n    cloud.google.com/neg: '{\"ingress\": true}'\n    cloud.google.com/backend-config: '{\"default\": \"frontend-backend-config\"}'\nspec:\n  type: ClusterIP\n  selector:\n    app: frontend\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080", "info": {"violations": [{"tool": "kube-linter", "rule_id": "dangling-service", "severity": "medium", "msg": "Confirm that your service's selector correctly matches the labels on one of your deployments.", "loc": ""}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/microservices-demo/.github/release-cluster/frontend-service.yaml", "hash": "39ec955c83f5bc3c", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "# Copyright 2022 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: opentelemetrycollector\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: opentelemetrycollector\n  template:\n    metadata:\n      labels:\n        app: opentelemetrycollector\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsGroup: 1000\n        runAsNonRoot: true\n        runAsUser: 1000\n      # Init container retrieves the current cloud project id from the metadata server\n      # and inserts it into the collector config template\n      # https://cloud.google.com/compute/docs/storing-retrieving-metadata\n      initContainers:\n      - name: otel-gateway-init\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n              - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n        image: busybox:latest@sha256:d82f458899c9696cb26a7c02d5568f81c8c8223f8661bb2a7988b269c8b9051e\n        command:\n        - '/bin/sh'\n        - '-c'\n        - |\n           sed \"s/{{PROJECT_ID}}/$(curl -H 'Metadata-Flavor: Google' http://metadata.google.internal/computeMetadata/v1/project/project-id)/\" /template/collector-gateway-config-template.yaml >> /conf/collector-gateway-config.yaml\n        volumeMounts:\n        - name: collector-gateway-config-template\n          mountPath: /template\n        - name: collector-gateway-config\n          mountPath: /conf\n      containers:\n      # This gateway container will receive traces and metrics from each microservice\n      # and forward it to GCP\n      - name: otel-gateway\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n              - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n        args:\n        - --config=/conf/collector-gateway-config.yaml\n        image: otel/opentelemetry-collector-contrib:0.135.0@sha256:89107a3a8f4636a396927edf7025bb9614b8da2d92f4cc3f43109e8d115736e2\n        volumeMounts:\n        - name: collector-gateway-config\n          mountPath: /conf\n      volumes:\n      # Simple ConfigMap volume with template file\n      - name: collector-gateway-config-template\n        configMap:\n          items:\n          - key: collector-gateway-config-template.yaml\n            path: collector-gateway-config-template.yaml\n          name: collector-gateway-config-template\n      # Create a volume to store the expanded template (with correct cloud project ID)\n      - name: collector-gateway-config\n        emptyDir: {}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: opentelemetrycollector\nspec:\n  ports:\n  - name: grpc-otlp\n    port: 4317\n    protocol: TCP\n    targetPort: 4317\n  selector:\n    app: opentelemetrycollector\n  type: ClusterIP\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: collector-gateway-config-template\n# Open Telemetry Collector config\n# https://opentelemetry.io/docs/collector/configuration/\ndata:\n  collector-gateway-config-template.yaml: |\n    receivers:\n      otlp:\n        protocols: \n          grpc:\n    processors:\n    exporters:\n      googlecloud:\n        project: {{PROJECT_ID}}\n    service:\n      pipelines:\n        traces:\n          receivers: [otlp] # Receive otlp-formatted data from other collector instances\n          processors: []\n          exporters: [googlecloud] # Export traces directly to Google Cloud\n        metrics:\n          receivers: [otlp]\n          processors: []\n          exporters: [googlecloud] # Export metrics to Google Cloud", "info": {"violations": [{"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/microservices-demo/kustomize/components/google-cloud-operations/otel-collector.yaml", "hash": "c6836e6088e90502", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: apps/v1 #  for k8s versions before 1.9.0 use apps/v1beta2  and before 1.8.0 use extensions/v1beta1\nkind: Deployment\nmetadata:\n  labels:\n    name: openshift\n  name: openshift\nspec:\n  selector:\n    matchLabels:\n      name: openshift\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openshift\n  template:\n    metadata:\n      labels:\n        name: openshift\n    spec:\n      containers:\n        - args:\n            - start\n            - master\n            - --config=/config/master-config.yaml\n          image: \"openshift/origin\"\n          name: origin\n          ports:\n            - containerPort: 8443\n              name: openshift\n          volumeMounts:\n            - mountPath: /config\n              name: config\n              readOnly: true\n      volumes:\n        - name: config\n          secret:\n            secretName: openshift-config\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/openshift-origin/openshift-controller.yaml:19"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container origin is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/openshift-origin/openshift-controller.yaml:26"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/openshift-origin/openshift-controller.yaml", "hash": "7cdef84fac558824", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "# https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pods-image-pull-secrets-pod\nspec:\n  containers:\n    - command:\n        - sleep\n        - \"3600\"\n      image: busybox\n      name: pods-image-pull-secrets-container\n  imagePullSecrets:\n    - name: regcred  # does not exist, create with instructions above\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Pod/spec.imagePullSecrets/image-pull-secrets.yaml:7"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container pods-image-pull-secrets-container is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Pod/spec.imagePullSecrets/image-pull-secrets.yaml:13"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/Pod/spec.imagePullSecrets/image-pull-secrets.yaml", "hash": "968626538da4a4d3", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv0001\nspec:\n  capacity:\n    storage: 5Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Delete\n  storageClassName: fast\n  storageos:\n    # This volume must already exist within StorageOS\n    volumeName: pv0001\n    # volumeNamespace is optional, and specifies the volume scope within\n    # StorageOS.  Set to `default` or leave blank if you are not using\n    # namespaces.\n    #volumeNamespace: default\n    # The filesystem type to create on the volume, if required.\n    fsType: ext4\n    # The secret name for API credentials\n    secretName: storageos-secret\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/volumes/storageos/storageos-pv.yaml", "hash": "5288100e0ffd9b7c", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "apiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: newrelic-infra-agent\n  labels:\n    tier: monitoring\n    app: newrelic-infra-agent\n    version: v1\nspec:\n  template:\n    metadata:\n      labels:\n        name: newrelic\n    spec:\n      # Filter to specific nodes:\n      # nodeSelector:\n      #  app: newrelic\n      hostPID: true\n      hostIPC: true\n      hostNetwork: true\n      containers:\n        - resources:\n            requests:\n              cpu: 0.15\n          securityContext:\n            privileged: true\n          image: newrelic/infrastructure\n          name: newrelic\n          command: [ \"bash\", \"-c\", \"source /etc/kube-nr-infra/config && /usr/bin/newrelic-infra\" ]\n          volumeMounts:\n            - name: newrelic-config\n              mountPath: /etc/kube-nr-infra\n              readOnly: true\n            - name: dev\n              mountPath: /dev\n            - name: run\n              mountPath: /var/run/docker.sock\n            - name: log\n              mountPath: /var/log\n            - name: host-root\n              mountPath: /host\n              readOnly: true\n      volumes:\n        - name: newrelic-config\n          secret:\n            secretName: newrelic-config\n        - name: dev\n          hostPath:\n              path: /dev\n        - name: run\n          hostPath:\n              path: /var/run/docker.sock\n        - name: log\n          hostPath:\n              path: /var/log\n        - name: host-root\n          hostPath:\n              path: /\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "no-extensions-v1beta", "severity": "medium", "msg": "Migrate using the apps/v1 API versions for the objects. Refer to https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/ for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/newrelic-infrastructure/newrelic-infra-daemonset.yaml:14"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.hostpid-pod.hostpid-pod", "severity": "WARNING", "msg": "Pod is sharing the host process ID namespace. When paired with ptrace this can be used to escalate privileges outside of the container. Remove the 'hostPID' key to disable this functionality.", "loc": "scripts/data/sources/kubernetes/examples/_archived/newrelic-infrastructure/newrelic-infra-daemonset.yaml:18"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.hostipc-pod.hostipc-pod", "severity": "WARNING", "msg": "Pod is sharing the host IPC namespace. This allows container processes to communicate with processes on the host which reduces isolation and bypasses container protection models. Remove the 'hostIPC' key to disable this functionality.", "loc": "scripts/data/sources/kubernetes/examples/_archived/newrelic-infrastructure/newrelic-infra-daemonset.yaml:19"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.hostnetwork-pod.hostnetwork-pod", "severity": "WARNING", "msg": "Pod may use the node network namespace. This gives the pod access to the loopback device, services listening on localhost, and could be used to snoop on network activity of other pods on the same node. Remove the 'hostNetwork' key to disable this functionality.", "loc": "scripts/data/sources/kubernetes/examples/_archived/newrelic-infrastructure/newrelic-infra-daemonset.yaml:20"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.privileged-container.privileged-container", "severity": "WARNING", "msg": "Container or pod is running in privileged mode. This grants the container the equivalent of root capabilities on the host machine. This can lead to container escapes, privilege escalation, and other security concerns. Remove the 'privileged' key to disable this capability.", "loc": "scripts/data/sources/kubernetes/examples/_archived/newrelic-infrastructure/newrelic-infra-daemonset.yaml:22"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.allow-privilege-escalation.allow-privilege-escalation", "severity": "WARNING", "msg": "In Kubernetes, each pod runs in its own isolated environment with its own set of security policies. However, certain container images may contain `setuid` or `setgid` binaries that could allow an attacker to perform privilege escalation and gain access to sensitive resources. To mitigate this risk, it's recommended to add a `securityContext` to the container in the pod, with the parameter `allowPrivilegeEscalation` set to `false`. This will prevent the container from running any privileged processes and limit the impact of any potential attacks. By adding the `allowPrivilegeEscalation` parameter to your the `securityContext`, you can help to ensure that your containerized applications are more secure and less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/newrelic-infrastructure/newrelic-infra-daemonset.yaml:25"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container newrelic is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/newrelic-infrastructure/newrelic-infra-daemonset.yaml:28"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.exposing-docker-socket-hostpath.exposing-docker-socket-hostpath", "severity": "WARNING", "msg": "Exposing host's Docker socket to containers via a volume. The owner of this socket is root. Giving someone access to it is equivalent to giving unrestricted root access to your host. Remove 'docker.sock' from hostpath to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/newrelic-infrastructure/newrelic-infra-daemonset.yaml:51"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/newrelic-infrastructure/newrelic-infra-daemonset.yaml", "hash": "fcba27549de0c58e", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "#Use this sysdig.yaml when Daemon Sets are NOT enabled on Kubernetes (minimum version 1.1.1). If Daemon Sets are available, use the other example sysdig.yaml - that is the recommended method.\n\napiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: sysdig-agent\n  labels:\n    app: sysdig-agent\nspec:\n  replicas: 100             #REQUIRED - replace with the maximum number of replica nodes in the cluster\n  template:\n    spec:\n      volumes:\n      - name: docker-sock\n        hostPath:\n         path: /var/run/docker.sock\n         type: Socket\n      - name: dev-vol\n        hostPath:\n         path: /dev\n      - name: proc-vol\n        hostPath:\n         path: /proc\n      - name: boot-vol\n        hostPath:\n         path: /boot\n      - name: modules-vol\n        hostPath:\n         path: /lib/modules\n      - name: usr-vol\n        hostPath:\n          path: /usr\n      hostNetwork: true\n      hostPID: true\n      containers:\n      - name: sysdig-agent\n        image: sysdig/agent\n        ports:\n        - containerPort: 6666\n          hostPort: 6666\n        securityContext:\n         privileged: true\n        env:\n        - name: ACCESS_KEY                                  #REQUIRED - replace with your Sysdig Cloud access key\n          value: 8312341g-5678-abcd-4a2b2c-33bcsd655\n#        - name: K8S_DELEGATED_NODE                         #OPTIONAL - only necessary when connecting remotely to API server\n#          value: <DELEGATED NODE IP>\n#        - name: K8S_API_URI                                #OPTIONAL - only necessary when connecting remotely to API server\n#          value: \"http[s]://[username:passwd@]host[:port]\"\n#        - name: TAGS                                       #OPTIONAL\n#          value: linux:ubuntu,dept:dev,local:nyc\n#        - name: COLLECTOR                                  #OPTIONAL\n#          value: 192.168.183.200\n#        - name: SECURE                                     #OPTIONAL\n#          value: false\n#        - name: CHECK_CERTIFICATE                          #OPTIONAL\n#          value: false\n#        - name: ADDITIONAL_CONF                            #OPTIONAL\n#          value: \"app_checks:\\n  - name: nginx\\n    check_module: nginx\\n    pattern:\\n      comm: nginx\\n    conf:\\n      nginx_status_url: \"http://localhost:{port}/nginx_status\\\"\"\n        volumeMounts:\n        - mountPath: /host/var/run/docker.sock\n          name: docker-sock\n          readOnly: false\n        - mountPath: /host/dev\n          name: dev-vol\n          readOnly: false\n        - mountPath: /host/proc\n          name: proc-vol\n          readOnly: true\n        - mountPath: /host/boot\n          name: boot-vol\n          readOnly: true\n        - mountPath: /host/lib/modules\n          name: modules-vol\n          readOnly: true\n        - mountPath: /host/usr\n          name: usr-vol\n          readOnly: true\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "docker-sock", "severity": "medium", "msg": "Ensure the Docker socket is not mounted inside any containers by removing the associated  Volume and VolumeMount in deployment yaml specification. If the Docker socket is mounted inside a container it could allow processes running within  the container to execute Docker commands which would effectively allow for full control of the host.", "loc": ""}, {"tool": "kube-linter", "rule_id": "host-network", "severity": "medium", "msg": "Ensure the host's network namespace is not shared.", "loc": ""}, {"tool": "kube-linter", "rule_id": "host-pid", "severity": "medium", "msg": "Ensure the host's process namespace is not shared.", "loc": ""}, {"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-anti-affinity", "severity": "medium", "msg": "Specify anti-affinity in your pod specification to ensure that the orchestrator attempts to schedule replicas on different nodes. Using podAntiAffinity, specify a labelSelector that matches pods for the deployment, and set the topologyKey to kubernetes.io/hostname. Refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "privilege-escalation-container", "severity": "medium", "msg": "Ensure containers do not allow privilege escalation by setting allowPrivilegeEscalation=false, privileged=false and removing CAP_SYS_ADMIN capability. See https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for more details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "privileged-container", "severity": "medium", "msg": "Do not run your container as privileged unless it is required.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "sensitive-host-mounts", "severity": "medium", "msg": "Ensure sensitive host system directories are not mounted in containers by removing those Volumes and VolumeMounts.", "loc": ""}, {"tool": "kube-linter", "rule_id": "sensitive-host-mounts", "severity": "medium", "msg": "Ensure sensitive host system directories are not mounted in containers by removing those Volumes and VolumeMounts.", "loc": ""}, {"tool": "kube-linter", "rule_id": "sensitive-host-mounts", "severity": "medium", "msg": "Ensure sensitive host system directories are not mounted in containers by removing those Volumes and VolumeMounts.", "loc": ""}, {"tool": "kube-linter", "rule_id": "sensitive-host-mounts", "severity": "medium", "msg": "Ensure sensitive host system directories are not mounted in containers by removing those Volumes and VolumeMounts.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/sysdig-cloud/sysdig-rc.yaml:12"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.exposing-docker-socket-hostpath.exposing-docker-socket-hostpath", "severity": "WARNING", "msg": "Exposing host's Docker socket to containers via a volume. The owner of this socket is root. Giving someone access to it is equivalent to giving unrestricted root access to your host. Remove 'docker.sock' from hostpath to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/sysdig-cloud/sysdig-rc.yaml:15"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.hostnetwork-pod.hostnetwork-pod", "severity": "WARNING", "msg": "Pod may use the node network namespace. This gives the pod access to the loopback device, services listening on localhost, and could be used to snoop on network activity of other pods on the same node. Remove the 'hostNetwork' key to disable this functionality.", "loc": "scripts/data/sources/kubernetes/examples/_archived/sysdig-cloud/sysdig-rc.yaml:33"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.hostpid-pod.hostpid-pod", "severity": "WARNING", "msg": "Pod is sharing the host process ID namespace. When paired with ptrace this can be used to escalate privileges outside of the container. Remove the 'hostPID' key to disable this functionality.", "loc": "scripts/data/sources/kubernetes/examples/_archived/sysdig-cloud/sysdig-rc.yaml:34"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.privileged-container.privileged-container", "severity": "WARNING", "msg": "Container or pod is running in privileged mode. This grants the container the equivalent of root capabilities on the host machine. This can lead to container escapes, privilege escalation, and other security concerns. Remove the 'privileged' key to disable this capability.", "loc": "scripts/data/sources/kubernetes/examples/_archived/sysdig-cloud/sysdig-rc.yaml:36"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container sysdig-agent is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/sysdig-cloud/sysdig-rc.yaml:36"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.allow-privilege-escalation.allow-privilege-escalation", "severity": "WARNING", "msg": "In Kubernetes, each pod runs in its own isolated environment with its own set of security policies. However, certain container images may contain `setuid` or `setgid` binaries that could allow an attacker to perform privilege escalation and gain access to sensitive resources. To mitigate this risk, it's recommended to add a `securityContext` to the container in the pod, with the parameter `allowPrivilegeEscalation` set to `false`. This will prevent the container from running any privileged processes and limit the impact of any potential attacks. By adding the `allowPrivilegeEscalation` parameter to your the `securityContext`, you can help to ensure that your containerized applications are more secure and less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/sysdig-cloud/sysdig-rc.yaml:41"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/sysdig-cloud/sysdig-rc.yaml", "hash": "60962d0404d02f18", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "kind: Service\napiVersion: v1\nmetadata:\n  name: etcd-{{cell}}\n  labels:\n    component: etcd\n    cell: {{cell}}\n    app: vitess\nspec:\n  ports:\n    - port: 4001\n  selector:\n    component: etcd\n    cell: {{cell}}\n    app: vitess\n\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/storage/vitess/etcd-service-template.yaml", "hash": "bb63345397c1f021", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# ----------------------------------------------------------\n# WARNING: This file is autogenerated. Do not manually edit.\n# ----------------------------------------------------------\n\n# [START servicemesh_release_istio_manifests_microservices_demo]\n---\napiVersion: gateway.networking.k8s.io/v1beta1\nkind: Gateway\nmetadata:\n  name: istio-gateway\nspec:\n  gatewayClassName: istio\n  listeners:\n  - name: http\n    port: 80\n    protocol: HTTP\n    allowedRoutes:\n      namespaces:\n        from: Same\n---\napiVersion: gateway.networking.k8s.io/v1beta1\nkind: HTTPRoute\nmetadata:\n  name: frontend-route\nspec:\n  parentRefs:\n  - name: istio-gateway\n  rules:\n  - matches:\n    - path:\n        value: /\n    backendRefs:\n    - name: frontend\n      port: 80\n---\napiVersion: networking.istio.io/v1alpha3\nkind: ServiceEntry\nmetadata:\n  name: allow-egress-googleapis\nspec:\n  hosts:\n  - \"accounts.google.com\" # Used to get token\n  - \"*.googleapis.com\"\n  ports:\n  - number: 80\n    protocol: HTTP\n    name: http\n  - number: 443\n    protocol: HTTPS\n    name: https\n---\napiVersion: networking.istio.io/v1alpha3\nkind: ServiceEntry\nmetadata:\n  name: allow-egress-google-metadata\nspec:\n  hosts:\n  - metadata.google.internal\n  addresses:\n  - 169.254.169.254 # GCE metadata server\n  ports:\n  - number: 80\n    name: http\n    protocol: HTTP\n  - number: 443\n    name: https\n    protocol: HTTPS\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: frontend\nspec:\n  hosts:\n  - \"frontend.default.svc.cluster.local\"\n  http:\n  - route:\n    - destination:\n        host: frontend\n        port:\n          number: 80\n# [END servicemesh_release_istio_manifests_microservices_demo]\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/microservices-demo/release/istio-manifests.yaml", "hash": "69f3b008df41593f", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kafka-config-map\ndata:\n  topic: kafka-config-map-topic\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: database-secrets\ntype: Opaque\ndata:\n  password: cGFzc3dvcmQ=  # password here is 'password' in base64\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deployments-simple-deployment-with-environment-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: deployments-simple-deployment-with-environment-app\n  template:\n    metadata:\n      labels:\n        app: deployments-simple-deployment-with-environment-app\n    spec:\n      containers:\n        - name: busybox\n          image: busybox\n          command:\n            - sleep\n            - \"3600\"\n          env:\n            # Plain Text ENV\n            - name: DEMO_GREETING\n              value: \"Hello from the environment\"\n            # Load from a secret\n            - name: DATABASE_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: database-secrets\n                  key: password\n            # Load from a configMap\n            - name: KAFKA_TOPIC\n              valueFrom:\n                configMapKeyRef:\n                  name: kafka-config-map\n                  key: topic\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-anti-affinity", "severity": "medium", "msg": "Specify anti-affinity in your pod specification to ensure that the orchestrator attempts to schedule replicas on different nodes. Using podAntiAffinity, specify a labelSelector that matches pods for the deployment, and set the topologyKey to kubernetes.io/hostname. Refer to https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Deployment/simple-deployment-with-environment.yaml:30"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container busybox is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/kubernetes-examples/Deployment/simple-deployment-with-environment.yaml:32"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/Deployment/simple-deployment-with-environment.yaml", "hash": "d161638c702dcb01", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "---\napiVersion: rbac.authorization.k8s.io/v1\n# This role binding allows \"jane\" to read pods in the \"default\" namespace.\n# You need to already have a Role named \"pod-reader\" in that namespace.\nkind: RoleBinding\nmetadata:\n  name: rbac-role-binding-role-binding\nsubjects:\n  # You can specify more than one \"subject\"\n  - kind: User\n    name: jane  # \"name\" is case sensitive\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  # \"roleRef\" specifies the binding to a Role / ClusterRole\n  kind: Role  # this must be Role or ClusterRole\n  # this must match the name of the Role or ClusterRole you wish to bind to\n  name: rbac-role-binding-role\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: rbac-role-binding-role\nrules:\n  - apiGroups: [\"\"]  # \"\" indicates the core API group\n    resources: [\"pods\"]\n    verbs: [\"get\", \"watch\", \"list\"]\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/kubernetes-examples/RoleBinding/role-binding.yaml", "hash": "f09ff27579ee1e17", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\napiVersion: networking.istio.io/v1alpha3\nkind: ServiceEntry\nmetadata:\n  name: allow-egress-googleapis\nspec:\n  hosts:\n  - \"accounts.google.com\" # Used to get token\n  - \"*.googleapis.com\"\n  ports:\n  - number: 80\n    protocol: HTTP\n    name: http\n  - number: 443\n    protocol: HTTPS\n    name: https\n---\napiVersion: networking.istio.io/v1alpha3\nkind: ServiceEntry\nmetadata:\n  name: allow-egress-google-metadata\nspec:\n  hosts:\n  - metadata.google.internal\n  addresses:\n  - 169.254.169.254 # GCE metadata server\n  ports:\n  - number: 80\n    name: http\n    protocol: HTTP\n  - number: 443\n    name: https\n    protocol: HTTPS\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/microservices-demo/kustomize/components/service-mesh-istio/allow-egress-googleapis.yaml", "hash": "4e3489bfc0bfc055", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n{{- if .Values.productCatalogService.create }}\n{{- if .Values.serviceAccounts.create }}\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: {{ .Values.productCatalogService.name }}\n  namespace: {{.Release.Namespace}}\n  {{- if not .Values.serviceAccounts.annotationsOnlyForCartservice }}\n  {{- with .Values.serviceAccounts.annotations }}\n  annotations:\n    {{- toYaml . | nindent 4 }}\n  {{- end }}\n  {{- end }}\n---\n{{- end }}\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Values.productCatalogService.name }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    app: {{ .Values.productCatalogService.name }}\nspec:\n  selector:\n    matchLabels:\n      app: {{ .Values.productCatalogService.name }}\n  template:\n    metadata:\n      labels:\n        app: {{ .Values.productCatalogService.name }}\n    spec:\n      {{- if .Values.serviceAccounts.create }}\n      serviceAccountName: {{ .Values.productCatalogService.name }}\n      {{- else }}\n      serviceAccountName: default\n      {{- end }}\n      terminationGracePeriodSeconds: 5\n      {{- if .Values.securityContext.enable }}\n      securityContext:\n        fsGroup: 1000\n        runAsGroup: 1000\n        runAsNonRoot: true\n        runAsUser: 1000\n        {{- if .Values.seccompProfile.enable }}\n        seccompProfile:\n          type: {{ .Values.seccompProfile.type }}\n        {{- end }}\n      {{- end }}\n      containers:\n      - name: server\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n              - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n        image: {{ .Values.images.repository }}/{{ .Values.productCatalogService.name }}:{{ .Values.images.tag | default .Chart.AppVersion }}\n        ports:\n        - containerPort: 3550\n        env:\n        - name: PORT\n          value: \"3550\"\n        {{- if .Values.opentelemetryCollector.create }}\n        - name: COLLECTOR_SERVICE_ADDR\n          value: \"{{ .Values.opentelemetryCollector.name }}:4317\"\n        - name: OTEL_SERVICE_NAME\n          value: \"{{ .Values.productCatalogService.name }}\"\n        {{- end }}\n        {{- if .Values.googleCloudOperations.tracing }}\n        - name: ENABLE_TRACING\n          value: \"1\"\n        {{- end }}\n        {{- if not .Values.googleCloudOperations.profiler }}\n        - name: DISABLE_PROFILER\n          value: \"1\"\n        {{- end }}\n        - name: EXTRA_LATENCY\n          value: {{ .Values.productCatalogService.extraLatency }}\n        readinessProbe:\n          grpc:\n            port: 3550\n        livenessProbe:\n          grpc:\n            port: 3550\n        resources:\n          {{- toYaml .Values.productCatalogService.resources | nindent 10 }}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: {{ .Values.productCatalogService.name }}\n  namespace: {{ .Release.Namespace }}\n  labels:\n    app: {{ .Values.productCatalogService.name }}\nspec:\n  type: ClusterIP\n  selector:\n    app: {{ .Values.productCatalogService.name }}\n  ports:\n  - name: grpc\n    port: 3550\n    targetPort: 3550\n{{- if .Values.networkPolicies.create }}\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: {{ .Values.productCatalogService.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  podSelector:\n    matchLabels:\n      app: {{ .Values.productCatalogService.name }}\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: {{ .Values.frontend.name }}\n    - podSelector:\n        matchLabels:\n          app: {{ .Values.checkoutService.name }}\n    - podSelector:\n        matchLabels:\n          app: {{ .Values.recommendationService.name }}\n    ports:\n     - port: 3550\n       protocol: TCP\n  egress:\n  - {}\n{{- end }}\n{{- if .Values.sidecars.create }}\n---\napiVersion: networking.istio.io/v1beta1\nkind: Sidecar\nmetadata:\n  name: {{ .Values.productCatalogService.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  workloadSelector:\n    labels:\n      app: {{ .Values.productCatalogService.name }}\n  egress:\n  - hosts:\n    - istio-system/*\n    {{- if .Values.opentelemetryCollector.create }}\n    - ./{{ .Values.opentelemetryCollector.name }}.{{ .Release.Namespace }}.svc.cluster.local\n    {{- end }}\n{{- end }}\n{{- if .Values.authorizationPolicies.create }}\n---\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: {{ .Values.productCatalogService.name }}\n  namespace: {{ .Release.Namespace }}\nspec:\n  selector:\n    matchLabels:\n      app: {{ .Values.productCatalogService.name }}\n  rules:\n  - from:\n    - source:\n        principals:\n        {{- if .Values.serviceAccounts.create }}\n        - cluster.local/ns/{{ .Release.Namespace }}/sa/{{ .Values.frontend.name }}\n        - cluster.local/ns/{{ .Release.Namespace }}/sa/{{ .Values.checkoutService.name }}\n        - cluster.local/ns/{{ .Release.Namespace }}/sa/{{ .Values.recommendationService.name }}\n        {{- else }}\n        - cluster.local/ns/{{ .Release.Namespace }}/sa/default\n        {{- end }}\n    to:\n    - operation:\n        paths:\n        - /hipstershop.ProductCatalogService/GetProduct\n        - /hipstershop.ProductCatalogService/ListProducts\n        methods:\n        - POST\n        ports:\n        - \"3550\"\n{{- end }}\n{{- end }}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/microservices-demo/helm-chart/templates/productcatalogservice.yaml", "hash": "be8181a2939a44c3", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cartservice\n  labels:\n    app: cartservice\nspec:\n  selector:\n    matchLabels:\n      app: cartservice\n  template:\n    metadata:\n      labels:\n        app: cartservice\n    spec:\n      serviceAccountName: cartservice\n      terminationGracePeriodSeconds: 5\n      securityContext:\n        fsGroup: 1000\n        runAsGroup: 1000\n        runAsNonRoot: true\n        runAsUser: 1000\n      containers:\n      - name: server\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n              - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n        image: us-central1-docker.pkg.dev/google-samples/microservices-demo/cartservice:v0.10.3\n        ports:\n        - containerPort: 7070\n        env:\n        - name: REDIS_ADDR\n          value: \"redis-cart:6379\"\n        resources:\n          requests:\n            cpu: 200m\n            memory: 64Mi\n          limits:\n            cpu: 300m\n            memory: 128Mi\n        readinessProbe:\n          initialDelaySeconds: 15\n          grpc:\n            port: 7070\n        livenessProbe:\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          grpc:\n            port: 7070\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: cartservice\n  labels:\n    app: cartservice\nspec:\n  type: ClusterIP\n  selector:\n    app: cartservice\n  ports:\n  - name: grpc\n    port: 7070\n    targetPort: 7070\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: cartservice\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-cart\n  labels:\n    app: redis-cart\nspec:\n  selector:\n    matchLabels:\n      app: redis-cart\n  template:\n    metadata:\n      labels:\n        app: redis-cart\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsGroup: 1000\n        runAsNonRoot: true\n        runAsUser: 1000\n      containers:\n      - name: redis\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n              - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n        image: redis:alpine\n        ports:\n        - containerPort: 6379\n        readinessProbe:\n          periodSeconds: 5\n          tcpSocket:\n            port: 6379\n        livenessProbe:\n          periodSeconds: 5\n          tcpSocket:\n            port: 6379\n        volumeMounts:\n        - mountPath: /data\n          name: redis-data\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 125m\n          requests:\n            cpu: 70m\n            memory: 200Mi\n      volumes:\n      - name: redis-data\n        emptyDir: {}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: redis-cart\n  labels:\n    app: redis-cart\nspec:\n  type: ClusterIP\n  selector:\n    app: redis-cart\n  ports:\n  - name: tcp-redis\n    port: 6379\n    targetPort: 6379\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/microservices-demo/kustomize/base/cartservice.yaml", "hash": "db30e5f368cd4888", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "#Use this sysdig.yaml when Daemon Sets are enabled on Kubernetes (minimum version 1.1.1). Otherwise use the RC method.\n\napiVersion: apps/v1  #for k8s versions before 1.9.0 use apps/v1beta2  and before 1.8.0 use extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: sysdig-agent\n  labels:\n    app: sysdig-agent\nspec:\n  selector:\n    matchLabels:\n      name: sysdig-agent\n  template:\n    metadata:\n      labels:\n        name: sysdig-agent\n    spec:\n      volumes:\n      - name: docker-sock\n        hostPath:\n         path: /var/run/docker.sock\n         type: Socket\n      - name: dev-vol\n        hostPath:\n         path: /dev\n      - name: proc-vol\n        hostPath:\n         path: /proc\n      - name: boot-vol\n        hostPath:\n         path: /boot\n      - name: modules-vol\n        hostPath:\n         path: /lib/modules\n      - name: usr-vol\n        hostPath:\n          path: /usr\n      hostNetwork: true\n      hostPID: true\n      containers:\n      - name: sysdig-agent\n        image: sysdig/agent\n        securityContext:\n         privileged: true\n        env:\n        - name: ACCESS_KEY                                  #REQUIRED - replace with your Sysdig Cloud access key\n          value: 8312341g-5678-abcd-4a2b2c-33bcsd655\n#        - name: TAGS                                       #OPTIONAL\n#          value: linux:ubuntu,dept:dev,local:nyc\n#        - name: COLLECTOR                                  #OPTIONAL - on-prem install only\n#          value: 192.168.183.200\n#        - name: SECURE                                     #OPTIONAL - on-prem install only\n#          value: false\n#        - name: CHECK_CERTIFICATE                          #OPTIONAL - on-prem install only\n#          value: false\n#        - name: ADDITIONAL_CONF                            #OPTIONAL pass additional parameters to the agent such as authentication example provided here\n#          value: \"k8s_uri: https://myacct:mypass@localhost:4430\\nk8s_ca_certificate: k8s-ca.crt\\nk8s_ssl_verify_certificate: true\"\n        volumeMounts:\n        - mountPath: /host/var/run/docker.sock\n          name: docker-sock\n          readOnly: false\n        - mountPath: /host/dev\n          name: dev-vol\n          readOnly: false\n        - mountPath: /host/proc\n          name: proc-vol\n          readOnly: true\n        - mountPath: /host/boot\n          name: boot-vol\n          readOnly: true\n        - mountPath: /host/lib/modules\n          name: modules-vol\n          readOnly: true\n        - mountPath: /host/usr\n          name: usr-vol\n          readOnly: true\n", "info": {"violations": [{"tool": "kube-linter", "rule_id": "docker-sock", "severity": "medium", "msg": "Ensure the Docker socket is not mounted inside any containers by removing the associated  Volume and VolumeMount in deployment yaml specification. If the Docker socket is mounted inside a container it could allow processes running within  the container to execute Docker commands which would effectively allow for full control of the host.", "loc": ""}, {"tool": "kube-linter", "rule_id": "host-network", "severity": "medium", "msg": "Ensure the host's network namespace is not shared.", "loc": ""}, {"tool": "kube-linter", "rule_id": "host-pid", "severity": "medium", "msg": "Ensure the host's process namespace is not shared.", "loc": ""}, {"tool": "kube-linter", "rule_id": "latest-tag", "severity": "medium", "msg": "Use a container image with a specific tag other than latest.", "loc": ""}, {"tool": "kube-linter", "rule_id": "no-read-only-root-fs", "severity": "medium", "msg": "Set readOnlyRootFilesystem to true in the container securityContext.", "loc": ""}, {"tool": "kube-linter", "rule_id": "privilege-escalation-container", "severity": "medium", "msg": "Ensure containers do not allow privilege escalation by setting allowPrivilegeEscalation=false, privileged=false and removing CAP_SYS_ADMIN capability. See https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for more details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "privileged-container", "severity": "medium", "msg": "Do not run your container as privileged unless it is required.", "loc": ""}, {"tool": "kube-linter", "rule_id": "run-as-non-root", "severity": "medium", "msg": "Set runAsUser to a non-zero number and runAsNonRoot to true in your pod or container securityContext. Refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "sensitive-host-mounts", "severity": "medium", "msg": "Ensure sensitive host system directories are not mounted in containers by removing those Volumes and VolumeMounts.", "loc": ""}, {"tool": "kube-linter", "rule_id": "sensitive-host-mounts", "severity": "medium", "msg": "Ensure sensitive host system directories are not mounted in containers by removing those Volumes and VolumeMounts.", "loc": ""}, {"tool": "kube-linter", "rule_id": "sensitive-host-mounts", "severity": "medium", "msg": "Ensure sensitive host system directories are not mounted in containers by removing those Volumes and VolumeMounts.", "loc": ""}, {"tool": "kube-linter", "rule_id": "sensitive-host-mounts", "severity": "medium", "msg": "Ensure sensitive host system directories are not mounted in containers by removing those Volumes and VolumeMounts.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-cpu-requirements", "severity": "medium", "msg": "Set CPU requests for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "kube-linter", "rule_id": "unset-memory-requirements", "severity": "medium", "msg": "Set memory limits for your container based on its requirements. Refer to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits for details.", "loc": ""}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.run-as-non-root.run-as-non-root", "severity": "INFO", "msg": "When running containers in Kubernetes, it's important to ensure that they  are properly secured to prevent privilege escalation attacks.  One potential vulnerability is when a container is allowed to run  applications as the root user, which could allow an attacker to gain  access to sensitive resources. To mitigate this risk, it's recommended to  add a `securityContext` to the container, with the parameter `runAsNonRoot`  set to `true`. This will ensure that the container runs as a non-root user,  limiting the damage that could be caused by any potential attacks. By  adding a `securityContext` to the container in your Kubernetes pod, you can  help to ensure that your containerized applications are more secure and  less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/sysdig-cloud/sysdig-daemonset.yaml:17"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.exposing-docker-socket-hostpath.exposing-docker-socket-hostpath", "severity": "WARNING", "msg": "Exposing host's Docker socket to containers via a volume. The owner of this socket is root. Giving someone access to it is equivalent to giving unrestricted root access to your host. Remove 'docker.sock' from hostpath to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/sysdig-cloud/sysdig-daemonset.yaml:20"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.hostnetwork-pod.hostnetwork-pod", "severity": "WARNING", "msg": "Pod may use the node network namespace. This gives the pod access to the loopback device, services listening on localhost, and could be used to snoop on network activity of other pods on the same node. Remove the 'hostNetwork' key to disable this functionality.", "loc": "scripts/data/sources/kubernetes/examples/_archived/sysdig-cloud/sysdig-daemonset.yaml:38"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.hostpid-pod.hostpid-pod", "severity": "WARNING", "msg": "Pod is sharing the host process ID namespace. When paired with ptrace this can be used to escalate privileges outside of the container. Remove the 'hostPID' key to disable this functionality.", "loc": "scripts/data/sources/kubernetes/examples/_archived/sysdig-cloud/sysdig-daemonset.yaml:39"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.privileged-container.privileged-container", "severity": "WARNING", "msg": "Container or pod is running in privileged mode. This grants the container the equivalent of root capabilities on the host machine. This can lead to container escapes, privilege escalation, and other security concerns. Remove the 'privileged' key to disable this capability.", "loc": "scripts/data/sources/kubernetes/examples/_archived/sysdig-cloud/sysdig-daemonset.yaml:41"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.writable-filesystem-container.writable-filesystem-container", "severity": "WARNING", "msg": "Container sysdig-agent is running with a writable root filesystem. This may allow malicious applications to download and run additional payloads, or modify container files. If an application inside a container has to save something temporarily consider using a tmpfs. Add 'readOnlyRootFilesystem: true' to this container to prevent this.", "loc": "scripts/data/sources/kubernetes/examples/_archived/sysdig-cloud/sysdig-daemonset.yaml:41"}, {"tool": "semgrep", "rule_id": "yaml.kubernetes.security.allow-privilege-escalation.allow-privilege-escalation", "severity": "WARNING", "msg": "In Kubernetes, each pod runs in its own isolated environment with its own set of security policies. However, certain container images may contain `setuid` or `setgid` binaries that could allow an attacker to perform privilege escalation and gain access to sensitive resources. To mitigate this risk, it's recommended to add a `securityContext` to the container in the pod, with the parameter `allowPrivilegeEscalation` set to `false`. This will prevent the container from running any privileged processes and limit the impact of any potential attacks. By adding the `allowPrivilegeEscalation` parameter to your the `securityContext`, you can help to ensure that your containerized applications are more secure and less vulnerable to privilege escalation attacks.", "loc": "scripts/data/sources/kubernetes/examples/_archived/sysdig-cloud/sysdig-daemonset.yaml:43"}], "patch": null}, "meta": {"lang": "k8s", "source": "scripts/data/sources/kubernetes/examples/_archived/sysdig-cloud/sysdig-daemonset.yaml", "hash": "218a0600241de154", "public_mini": true, "source_dataset": "k8s-labeled-v1.jsonl"}}
{"question": "################################################################################\n# Issue 44\n################################################################################\n\n# VPC\noutput \"issue_44_vpc_id\" {\n  description = \"The ID of the VPC\"\n  value       = module.vpc_issue_44.vpc_id\n}\n\n# Subnets\noutput \"issue_44_private_subnets\" {\n  description = \"List of IDs of private subnets\"\n  value       = module.vpc_issue_44.private_subnets\n}\n\noutput \"issue_44_public_subnets\" {\n  description = \"List of IDs of public subnets\"\n  value       = module.vpc_issue_44.public_subnets\n}\n\noutput \"issue_44_database_subnets\" {\n  description = \"List of IDs of database subnets\"\n  value       = module.vpc_issue_44.database_subnets\n}\n\noutput \"issue_44_elasticache_subnets\" {\n  description = \"List of IDs of elasticache subnets\"\n  value       = module.vpc_issue_44.elasticache_subnets\n}\n\n# NAT gateways\noutput \"issue_44_nat_public_ips\" {\n  description = \"List of public Elastic IPs created for AWS NAT Gateway\"\n  value       = module.vpc_issue_44.nat_public_ips\n}\n\n################################################################################\n# Issue 46\n################################################################################\n\n# VPC\noutput \"issue_46_vpc_id\" {\n  description = \"The ID of the VPC\"\n  value       = module.vpc_issue_46.vpc_id\n}\n\n# Subnets\noutput \"issue_46_private_subnets\" {\n  description = \"List of IDs of private subnets\"\n  value       = module.vpc_issue_46.private_subnets\n}\n\noutput \"issue_46_public_subnets\" {\n  description = \"List of IDs of public subnets\"\n  value       = module.vpc_issue_46.public_subnets\n}\n\noutput \"issue_46_database_subnets\" {\n  description = \"List of IDs of database subnets\"\n  value       = module.vpc_issue_46.database_subnets\n}\n\noutput \"issue_46_elasticache_subnets\" {\n  description = \"List of IDs of elasticache subnets\"\n  value       = module.vpc_issue_46.elasticache_subnets\n}\n\n# NAT gateways\noutput \"issue_46_nat_public_ips\" {\n  description = \"List of public Elastic IPs created for AWS NAT Gateway\"\n  value       = module.vpc_issue_46.nat_public_ips\n}\n\n################################################################################\n# Issue 108\n################################################################################\n\n# VPC\noutput \"issue_108_vpc_id\" {\n  description = \"The ID of the VPC\"\n  value       = module.vpc_issue_108.vpc_id\n}\n\n# Subnets\noutput \"issue_108_private_subnets\" {\n  description = \"List of IDs of private subnets\"\n  value       = module.vpc_issue_108.private_subnets\n}\n\noutput \"issue_108_public_subnets\" {\n  description = \"List of IDs of public subnets\"\n  value       = module.vpc_issue_108.public_subnets\n}\n\noutput \"issue_108_database_subnets\" {\n  description = \"List of IDs of database subnets\"\n  value       = module.vpc_issue_108.database_subnets\n}\n\noutput \"issue_108_elasticache_subnets\" {\n  description = \"List of IDs of elasticache subnets\"\n  value       = module.vpc_issue_108.elasticache_subnets\n}\n\n# NAT gateways\noutput \"issue_108_nat_public_ips\" {\n  description = \"List of public Elastic IPs created for AWS NAT Gateway\"\n  value       = module.vpc_issue_108.nat_public_ips\n}\n", "info": {"violations": [], "patch": null}, "meta": {"lang": "tf", "source": "scripts/data/sources/terraform/terraform-aws-vpc/examples/issues/outputs.tf", "hash": "d2e5bd2f41d14e81", "public_mini": true, "source_dataset": "terraform-labeled-v1.jsonl"}}
