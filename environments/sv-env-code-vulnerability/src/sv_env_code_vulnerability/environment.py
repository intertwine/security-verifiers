"""CodeVulnerabilityEnvironment implementation for code vulnerability assessment."""

from __future__ import annotations

import logging
from typing import Any, Mapping

import verifiers as vf
from datasets import Dataset

from .verifier import CodeVulnerabilityVerifier

logger = logging.getLogger(__name__)


class CodeVulnerabilityEnvironment:
    """ToolEnv/MultiTurnEnv environment for code vulnerability assessment.

    This environment implements PRD Environment #5: A ToolEnv/MultiTurnEnv where models
    inspect source code for security vulnerabilities and suggest fixes. The model can
    call static analysis tools and propose corrected code snippets that address
    security issues.
    """

    def __init__(
        self,
        verifier: CodeVulnerabilityVerifier | None = None,
        max_examples: int = 1000,
        system_prompt: str | None = None,
    ):
        """Initialize the code vulnerability assessment environment.

        Args:
            verifier: Custom verifier instance (uses default if None)
            max_examples: Maximum number of examples to use from dataset
            system_prompt: Custom system prompt (uses default if None)
        """
        self.verifier = verifier or CodeVulnerabilityVerifier()
        self.max_examples = max_examples

        self.system_prompt = system_prompt or (
            "You are a security code reviewer. Analyze the provided code snippet "
            "for potential security vulnerabilities. Identify any security issues, "
            "explain their impact, and suggest secure code fixes. You have access "
            "to static analysis tools to help with your assessment."
        )

        self._dataset: Dataset | None = None
        self._env: vf.ToolEnv | None = None

    def evaluate(self, code: str, execution_log: str) -> tuple[float, Mapping[str, Any]]:
        """Evaluate a model's vulnerability assessment of a code snippet.

        Args:
            code: The code snippet that was analyzed
            execution_log: Log from any tool executions or analysis

        Returns:
            tuple: (reward, info) where reward is the RL signal and info contains
                   auxiliary data about the evaluation
        """
        # Score the vulnerability assessment
        vulnerability_score = self.verifier.score(code, execution_log)

        # Higher vulnerability score means less secure code
        # For RL training, we want to reward finding vulnerabilities
        # So we use the vulnerability score as the reward
        reward = vulnerability_score

        info = {
            **self.verifier.details(),
            "vulnerability_score": vulnerability_score,
            "execution_log": execution_log[:1000] + "..." if len(execution_log) > 1000 else execution_log,
        }

        return reward, info

    def get_dataset(self) -> Dataset:
        """Get the dataset of code snippets for vulnerability assessment.

        Returns:
            Dataset containing code snippets with known vulnerabilities
        """
        if self._dataset is None:
            logger.info("Creating synthetic vulnerability dataset")
            self._dataset = self._create_synthetic_dataset()

        return self._dataset

    def get_verifiers_env(self) -> vf.ToolEnv:
        """Get the underlying Verifiers ToolEnv for RL training.

        Returns:
            vf.ToolEnv: Configured environment ready for RL training
        """
        if self._env is None:
            self._env = self._create_verifiers_env()
        return self._env

    def _create_verifiers_env(self) -> vf.ToolEnv:
        """Create the underlying Verifiers ToolEnv."""
        dataset = self.get_dataset()

        def vulnerability_assessment_tool(code: str) -> str:
            """Tool for analyzing code vulnerabilities."""
            # Run the verifier and return analysis
            vulnerabilities = self.verifier._analyze_code(code)

            if not vulnerabilities:
                return "No obvious vulnerabilities detected in the code."

            result = f"Found {len(vulnerabilities)} potential security issues:\n\n"
            for i, vuln in enumerate(vulnerabilities, 1):
                result += f"{i}. {vuln.get('type', 'Unknown')} "
                result += f"(Severity: {vuln.get('severity', 'UNKNOWN')}) "
                if "line" in vuln:
                    result += f"at line {vuln['line']} "
                if "message" in vuln:
                    result += f"- {vuln['message']}"
                result += "\n"

            return result.strip()

        def bandit_analysis_tool(code: str) -> str:
            """Tool for running Bandit static analysis."""
            try:
                import json
                import subprocess
                import tempfile
                from pathlib import Path

                # Create temporary file
                with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as f:
                    f.write(code)
                    temp_file = f.name

                try:
                    # Run bandit
                    result = subprocess.run(
                        ["bandit", "-f", "json", temp_file],
                        capture_output=True,
                        text=True,
                        timeout=30,
                    )

                    if result.returncode == 0:
                        return "Bandit analysis: No issues found."

                    # Parse bandit output
                    try:
                        bandit_data = json.loads(result.stdout)
                        if "results" in bandit_data and bandit_data["results"]:
                            issues = []
                            for issue in bandit_data["results"]:
                                severity = issue.get("issue_severity", "UNKNOWN")
                                test_name = issue.get("test_name", "Unknown Test")
                                line_num = issue.get("line_number", "Unknown")
                                message = issue.get("issue_text", "No description")
                                issues.append(f"Line {line_num}: {test_name} ({severity}) - {message}")
                            return f"Bandit found {len(issues)} issues:\n" + "\n".join(issues)
                        else:
                            return "Bandit analysis completed - no issues found."
                    except json.JSONDecodeError:
                        return f"Bandit analysis completed with warnings: {result.stderr}"

                finally:
                    Path(temp_file).unlink(missing_ok=True)

            except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:
                return f"Bandit analysis failed: {str(e)}"

        def code_fix_validator(original_code: str, fixed_code: str) -> str:
            """Tool for validating security fixes."""
            original_score = self.verifier.score(original_code, "")
            fixed_score = self.verifier.score(fixed_code, "")

            improvement = original_score - fixed_score

            if improvement > 0.1:
                return f"Security improvement detected: vulnerability score reduced by {improvement:.2f}"
            elif improvement > 0:
                return f"Minor security improvement: vulnerability score reduced by {improvement:.2f}"
            elif improvement == 0:
                return "No change in security assessment"
            else:
                return f"Warning: Security may have gotten worse (score increased by {abs(improvement):.2f})"

        tools = [
            vf.Tool(
                name="analyze_vulnerabilities",
                description=(
                    "Analyze code for security vulnerabilities using pattern matching and AST analysis"
                ),
                function=vulnerability_assessment_tool,
            ),
            vf.Tool(
                name="run_bandit",
                description="Run Bandit static analysis security tool on the code",
                function=bandit_analysis_tool,
            ),
            vf.Tool(
                name="validate_fix",
                description="Validate that a proposed security fix actually improves security",
                function=code_fix_validator,
            ),
        ]

        def reward_vulnerability_detection(prompt: str, completion: str, answer: str, **kwargs) -> float:
            """Reward function for vulnerability detection accuracy."""
            # Extract code from prompt
            code = self._extract_code_from_prompt(prompt)

            # Score based on whether vulnerabilities were correctly identified
            vulnerability_score = self.verifier.score(code, completion)

            # Reward finding vulnerabilities and providing good analysis
            analysis_quality = self._assess_analysis_quality(completion, answer)

            return (vulnerability_score * 0.7) + (analysis_quality * 0.3)

        rubric = vf.Rubric(
            funcs=[reward_vulnerability_detection],
            weights=[1.0],
        )

        return vf.ToolEnv(
            dataset=dataset,
            rubric=rubric,
            tools=tools,
            system_prompt=self.system_prompt,
        )

    def _create_synthetic_dataset(self) -> Dataset:
        """Create a synthetic dataset for testing purposes."""
        examples = [
            {
                "prompt": """Analyze this Python code for security vulnerabilities:

```python
import os
import sqlite3

def login(username, password):
    conn = sqlite3.connect('users.db')
    cursor = conn.cursor()
    query = "SELECT * FROM users WHERE username='%s' AND password='%s'" % (username, password)
    cursor.execute(query)
    result = cursor.fetchone()
    conn.close()
    return result is not None
```""",
                "description": (
                    "SQL injection vulnerability: The query uses string formatting which "
                    "allows attackers to inject malicious SQL. Use parameterized queries instead."
                ),
            },
            {
                "prompt": """Analyze this Python code for security vulnerabilities:

```python
import subprocess

def run_command(cmd):
    return subprocess.call(cmd, shell=True)

def process_file(filename):
    result = run_command("cat " + filename)
    return result
```""",
                "description": (
                    "Command injection: User inputs directly concatenated into shell "
                    "commands. Use subprocess with argument lists instead of shell=True."
                ),
            },
            {
                "prompt": """Analyze this Python code for security vulnerabilities:

```python
import pickle

def load_data(data_string):
    return pickle.loads(data_string)

def save_user_session(session_data):
    with open('session.pkl', 'wb') as f:
        pickle.dump(session_data, f)
```""",
                "description": (
                    "Pickle usage: pickle.loads() can execute arbitrary code. Use JSON "
                    "or other safe serialization formats for untrusted data."
                ),
            },
            {
                "prompt": """Analyze this Python code for security vulnerabilities:

```python
import hashlib

def hash_password(password):
    return hashlib.md5(password.encode()).hexdigest()

def verify_password(password, hash_value):
    return hash_password(password) == hash_value
```""",
                "description": (
                    "Weak hashing algorithm: MD5 is cryptographically broken. Use bcrypt, "
                    "scrypt, or Argon2 for password hashing."
                ),
            },
            {
                "prompt": """Analyze this Python code for security vulnerabilities:

```python
def read_file(filename):
    if "../" in filename:
        return "Access denied"
    with open(filename, 'r') as f:
        return f.read()

def serve_file(user_path):
    return read_file(user_path)
```""",
                "description": (
                    "Path traversal: Simple check for '../' is insufficient. Use "
                    "os.path.abspath() and validate against allowed directories."
                ),
            },
            {
                "prompt": """Analyze this Python code for security vulnerabilities:

```python
API_KEY = "sk-1234567890abcdef"
SECRET_TOKEN = "abc123def456"

def authenticate(request):
    if request.headers.get('Authorization') == f"Bearer {API_KEY}":
        return True
    return False
```""",
                "description": (
                    "Hardcoded secrets: API tokens are hardcoded in source code. Use "
                    "environment variables or secure key management systems."
                ),
            },
            {
                "prompt": """Analyze this Python code for security vulnerabilities:

```python
def process_input(user_input):
    try:
        result = eval(user_input)
        return result
    except Exception as e:
        return "Error: " + str(e)
```""",
                "description": (
                    "Code execution: eval() can execute arbitrary Python code from "
                    "user input. Use ast.literal_eval() for safe evaluation or "
                    "implement proper input validation."
                ),
            },
            {
                "prompt": """Analyze this Python code for security vulnerabilities:

```python
import requests

def fetch_url(url):
    response = requests.get(url, verify=False)
    return response.text

def process_api(endpoint):
    return fetch_url("https://api.example.com/" + endpoint)
```""",
                "description": (
                    "TLS verification disabled: verify=False disables SSL certificate "
                    "validation, making the connection vulnerable to man-in-the-middle attacks."
                ),
            },
        ]

        logger.info(f"Created synthetic dataset with {len(examples)} examples")
        return Dataset.from_list(examples)

    def _extract_code_from_prompt(self, prompt: str) -> str:
        """Extract code snippet from prompt text."""
        # Simple extraction - look for code blocks
        import re

        code_match = re.search(r"```(?:python)?\n(.*?)\n```", prompt, re.DOTALL)
        if code_match:
            return code_match.group(1)
        return prompt

    def _assess_analysis_quality(self, completion: str, expected_answer: str) -> float:
        """Assess the quality of vulnerability analysis."""
        # Simple scoring based on keyword matching
        completion_lower = completion.lower()
        expected_lower = expected_answer.lower()

        # Check if key vulnerability terms are mentioned
        vuln_keywords = ["vulnerability", "injection", "attack", "security", "exploit", "unsafe"]
        mentioned_keywords = sum(1 for kw in vuln_keywords if kw in completion_lower)

        # Check if the specific vulnerability type is mentioned
        type_mentioned = any(word in completion_lower for word in expected_lower.split()[:3])

        # Basic scoring
        score = 0.0
        if mentioned_keywords > 0:
            score += 0.4
        if mentioned_keywords > 2:
            score += 0.2
        if type_mentioned:
            score += 0.3
        if len(completion) > 50:  # Detailed analysis
            score += 0.1

        return min(1.0, score)
