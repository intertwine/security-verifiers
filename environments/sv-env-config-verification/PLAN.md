# E2 Configuration Auditing — Implementation Canvas
**Date:** 2025-09-12
**Audience:** Human developers & LLM agent developers
**Goal:** A practical, end‑to‑end plan with concrete wrapper signatures, data models, reward logic, tests, and CI so Environment **E2** meets the PRD: *tool‑grounded config auditing for K8s & Terraform using OPA/Rego, KubeLinter, and Semgrep, with executable rewards and patch verification.*

---

## 0) TL;DR (What to build)
- **Adapters** that actually run **OPA**, **KubeLinter**, and **Semgrep**, parse JSON, and normalize findings to the E2 schema.
- **Dataset + Oracle** generated by the adapters (not heuristics), including **Kubernetes** and **Terraform** fixtures.
- **Reward** = severity‑weighted precision/recall against the oracle **plus patch verification** (re‑run tools after applying patch).
- **Baselines**: tools‑only reporter; LLM explainer without tools; RLFT config.
- **Risk controls**: pin tool + ruleset versions; golden‑file tests in CI; deterministic Docker image.

---

## 1) PRD slice for E2 (one page)
**Inputs**: Kubernetes manifests (YAML) and Terraform (HCL).
**Tools**: OPA/Rego, KubeLinter, Semgrep.
**Model Output schema** (enforced):
```json
{
  "violations": [{"id":"string","severity":"low|med|high"}],
  "patch": "string",
  "confidence": 0.0
}
```
**Reward**: Weighted TP/FP/FN on `violations` (tool‑derived oracle) **+** positive credit for patches that actually eliminate oracle violations when applied, **–** penalties for format drift / unverifiable claims.
**Risks**: Tool/rule drift; flaky CI; non‑applicable patches ⇒ **pin versions, golden files, and run post‑patch verification.**

---

## 2) Repository layout (proposed)
```
security-verifiers/
  e2_config_auditing/
    adapters/
      opa_adapter.py
      kubelinter_adapter.py
      semgrep_adapter.py
      types.py
    mapping.py                 # tool→E2 normalization (IDs, severity)
    patching.py                # apply unified-diff or JSONPatch
    oracle.py                  # run tools to build oracle labels
    reward.py                  # severity-weighted P/R + patch delta
    schema.py                  # Pydantic models; schema validation
    env.py                     # E2 gym-style environment
    baselines/
      tools_only.py
      llm_explainer.py
    dataset/
      fixtures/
        k8s/...                # YAML examples
        tf/...                 # Terraform (HCL) examples
      oracle/
        *.json                 # golden tool outputs (pinned versions)
    tests/
      test_opa_adapter.py
      test_kubelinter_adapter.py
      test_semgrep_adapter.py
      test_mapping.py
      test_patching.py
      test_reward.py
      test_oracle_regen_stability.py
      test_env_end2end.py
    docker/
      Dockerfile.e2
    ci/
      e2_verify_rules.yml
    README.md
    Makefile
```

---

## 3) Core data models (internal)

```python
# e2_config_auditing/adapters/types.py
from dataclasses import dataclass
from typing import Optional, Literal, Dict, Any, List

E2Severity = Literal["low", "med", "high"]

@dataclass
class ToolFinding:
    tool: Literal["opa","kube-linter","semgrep"]
    rule_id: str                    # native tool rule/check id
    severity: str                   # native tool severity (string)
    message: str
    file: Optional[str] = None
    start_line: Optional[int] = None
    end_line: Optional[int] = None
    extra: Optional[Dict[str, Any]] = None

@dataclass
class Violation:  # E2-normalized unit
    id: str
    severity: E2Severity
    # optional internal fields for explainability/debug
    message: Optional[str] = None
    file: Optional[str] = None
    start_line: Optional[int] = None
    end_line: Optional[int] = None
    source_tool: Optional[str] = None
    native_rule_id: Optional[str] = None

@dataclass
class AuditOutput:  # model/env IO
    violations: List[Violation]
    patch: Optional[str]
    confidence: float
    tools_used: Optional[List[str]] = None  # non-PRD, internal only
```
**Rule/ID namespace:** normalize IDs to `{tool}/{native_rule_id}` (e.g., `kube-linter/run-as-root`).

---

## 4) Tool adapters — wrapper signatures (final)

> These wrappers must be **pure, synchronous, timeout‑bounded**, and return structured results. The adapters own CLI invocation and JSON parsing; they never mutate repo state.

### 4.1 OPA / Rego
```python
# e2_config_auditing/adapters/opa_adapter.py
from pathlib import Path
from typing import Sequence, Optional, Dict, Any, List
from .types import ToolFinding

class OPAError(RuntimeError): pass

def opa_eval(
    input_data: str | Dict[str, Any],
    policy_paths: Sequence[str | Path],
    query: str = "data.security.deny",   # policy pkg exports a list of violation objects
    opa_bin: str | Path = "opa",
    timeout_s: int = 15,
    env: Optional[Dict[str,str]] = None,
) -> List[ToolFinding]:
    """
    Run `opa eval --format=json` over input & policies, expecting the query to
    evaluate to a list of violation objects with fields: id, message, severity, file?, line?.
    - input_data: JSON string or Python dict
    - policy_paths: .rego files or directories
    - query: Rego data path that returns a list of violation objects
    Returns: list of ToolFinding(tool='opa', ...)
    Raises: OPAError on non-zero exit, invalid JSON, or unexpected shape.
    """
    ...  # subprocess.run([...]); parse JSON; map to ToolFinding
```
**Policy contract:** Your Rego should emit objects like:
```rego
package security

deny[{"id": "rego/no-default-allow", "severity": "high", "message": "default allow is true", "file": input.__meta.file, "line": 1}] {
  input.kind == "rego"
  default allow = true
}
```

### 4.2 KubeLinter
```python
# e2_config_auditing/adapters/kubelinter_adapter.py
from pathlib import Path
from typing import Sequence, Optional, Dict, Any, List
from .types import ToolFinding

class KubeLinterError(RuntimeError): pass

def kubelinter_lint(
    k8s_paths: Sequence[str | Path],
    kube_linter_bin: str | Path = "kube-linter",
    timeout_s: int = 30,
    config_yaml: Optional[str | Path] = None,  # optional custom checks/config
    env: Optional[Dict[str,str]] = None,
) -> List[ToolFinding]:
    """
    Run `kube-linter lint --format json` on provided files/dirs.
    Return: ToolFinding list (tool='kube-linter') with rule_id, severity, message, file, line range.
    Raises: KubeLinterError on CLI failures or invalid output.
    """
    ...
```

### 4.3 Semgrep (for Terraform & generic code patterns)
```python
# e2_config_auditing/adapters/semgrep_adapter.py
from pathlib import Path
from typing import Sequence, Optional, Dict, Any, List
from .types import ToolFinding

class SemgrepError(RuntimeError): pass

def semgrep_scan(
    targets: Sequence[str | Path],
    rules: Sequence[str | Path] | str = ("p/ci",),  # can be a registry pack or local yaml
    semgrep_bin: str | Path = "semgrep",
    timeout_s: int = 60,
    include: Optional[Sequence[str]] = None,        # glob patterns
    exclude: Optional[Sequence[str]] = None,
    env: Optional[Dict[str,str]] = None,
) -> List[ToolFinding]:
    """
    Run `semgrep --json` with the provided rules.
    Return: ToolFinding list (tool='semgrep') with rule_id, severity, message, file, start/end lines.
    Raises: SemgrepError on CLI failures or invalid output.
    """
    ...
```

> **Terraform note:** point Semgrep at Terraform directories (HCL). You can use a curated ruleset for IaC/HCL or bundle a minimal rules file focused on PRD‑relevant misconfigs.

---

## 5) Normalization & mapping

```python
# e2_config_auditing/mapping.py
from typing import List
from .adapters.types import ToolFinding, Violation, E2Severity

TOOL_SEV_TO_E2: dict[str, dict[str, E2Severity]] = {
    "semgrep": {"INFO":"low", "WARNING":"med", "ERROR":"high"},
    "kube-linter": {"Info":"low", "Warning":"med", "Error":"high"},
    "opa": {"low":"low","medium":"med","med":"med","high":"high"},
}

def normalize_findings(findings: List[ToolFinding]) -> List[Violation]:
    out: List[Violation] = []
    for f in findings:
        sev_map = TOOL_SEV_TO_E2.get(f.tool, {})
        e2_sev = sev_map.get((f.severity or "").strip(), "med")
        v = Violation(
            id=f"{f.tool}/{f.rule_id}",
            severity=e2_sev,
            message=f.message,
            file=f.file,
            start_line=f.start_line,
            end_line=f.end_line,
            source_tool=f.tool,
            native_rule_id=f.rule_id,
        )
        out.append(v)
    return out

def to_prd_schema(violations: List[Violation]) -> list[dict]:
    # Strip to PRD-minimal form
    return [{"id": v.id, "severity": v.severity} for v in violations]
```

---

## 6) Patch application & verification

We support two patch formats:
1) **Unified diff** (text) over a single file (K8s YAML / Terraform HCL)
2) **JSON Patch (RFC6902)** where applicable (YAML/JSON configs) — applied after parsing YAML/HCL to dict where possible.

```python
# e2_config_auditing/patching.py
from typing import Literal, Tuple

PatchFormat = Literal["unified-diff","json-patch"]

class PatchError(RuntimeError): pass

def detect_patch_format(patch: str) -> PatchFormat: ...
def apply_patch_to_text(original: str, patch: str) -> str: ...
def apply_json_patch(obj: dict, patch_ops: list[dict]) -> dict: ...
def try_apply_patch(path: str, patch: str) -> Tuple[bool, str]:
    """
    Load file at `path`, attempt to apply patch, return (applied_ok, new_text).
    Does not modify files on disk; caller decides what to do with new_text.
    """
    ...
```

**Verification loop (per example artifact):**
1) Run tool(s) on original → `oracle_findings` (normalized → E2 `violations_oracle`)
2) Apply proposed `patch` virtually to the artifact(s) → produce candidate text(s)
3) Re‑run tool(s) on candidate → `post_findings` (normalized)
4) **Reward patch delta:** credit for each `(id, severity)` present in oracle but **absent** post‑patch (with severity weights).

---

## 7) Reward function (final form)

```python
# e2_config_auditing/reward.py
from typing import List, Tuple
from .adapters.types import Violation, E2Severity

SEV_WEIGHT = {"low": 0.3, "med": 0.6, "high": 1.0}

def score_detection(pred: List[Violation], oracle: List[Violation]) -> Tuple[float,float,float]:
    """
    Returns weighted (precision, recall, f1). Matching by violation.id.
    """
    o_ids = {v.id: SEV_WEIGHT[v.severity] for v in oracle}
    p_ids = {v.id: SEV_WEIGHT[v.severity] for v in pred}
    # weighted TP
    tp = sum(SEV_WEIGHT[v.severity] for v in pred if v.id in o_ids)
    fp = sum(SEV_WEIGHT[v.severity] for v in pred if v.id not in o_ids)
    fn = sum(w for vid, w in o_ids.items() if vid not in p_ids)
    precision = tp / (tp + fp) if (tp + fp) else 0.0
    recall = tp / (tp + fn) if (tp + fn) else 0.0
    f1 = 2*precision*recall / (precision+recall) if (precision+recall) else 0.0
    return precision, recall, f1

def score_patch_delta(oracle: List[Violation], post: List[Violation]) -> float:
    """
    Weighted count of oracle violations that disappeared after applying the patch.
    """
    o = {v.id: SEV_WEIGHT[v.severity] for v in oracle}
    p = {v.id for v in post}
    removed = [w for vid, w in o.items() if vid not in p]
    return sum(removed)

def final_reward(
    pred: List[Violation],
    oracle: List[Violation],
    patch_removed_weight: float = 1.0,
    format_bonus: float = 0.05,
    invalid_penalty: float = -0.25,
    had_valid_json: bool = True,
    post_patch: List[Violation] | None = None,
) -> float:
    prec, rec, f1 = score_detection(pred, oracle)
    reward = f1
    if post_patch is not None:
        reward += patch_removed_weight * score_patch_delta(oracle, post_patch)
    reward += (format_bonus if had_valid_json else invalid_penalty)
    return max(-1.0, min(2.0, reward))  # clamp for stability
```

---

## 8) Oracle generation (dataset build)

```python
# e2_config_auditing/oracle.py
from typing import Sequence, Dict, Any
from .adapters.opa_adapter import opa_eval
from .adapters.kubelinter_adapter import kubelinter_lint
from .adapters.semgrep_adapter import semgrep_scan
from .mapping import normalize_findings, to_prd_schema

def build_oracle_for_k8s(paths: Sequence[str]) -> list[dict]:
    kl = kubelinter_lint(paths)
    # Optionally, OPA policies for org-specific rules:
    # opa = opa_eval(input_data=..., policy_paths=[...], query="data.security.deny")
    findings = kl  # + opa if used
    return to_prd_schema(normalize_findings(findings))

def build_oracle_for_tf(paths: Sequence[str], semgrep_rules=("p/ci",)) -> list[dict]:
    sg = semgrep_scan(paths, rules=semgrep_rules)
    return to_prd_schema(normalize_findings(sg))
```
Store the oracle JSON next to fixtures with **pinned tool/rule versions** recorded in metadata (e.g., `oracle/_versions.json`).

---

## 9) Environment glue (gym-style)

```python
# e2_config_auditing/env.py
from typing import Dict, Any
from .schema import parse_model_output  # validates PRD schema
from .oracle import build_oracle_for_k8s, build_oracle_for_tf
from .mapping import normalize_findings
from .reward import final_reward
from .patching import try_apply_patch
from .adapters.kubelinter_adapter import kubelinter_lint
from .adapters.semgrep_adapter import semgrep_scan

class ConfigAuditingEnv:
    def __init__(self, fixture: Dict[str, Any], tool_versions: Dict[str,str]): ...
    def reset(self) -> Dict[str, Any]: ...
    def step(self, model_json: Dict[str, Any]) -> Dict[str, Any]:
        # 1) validate format
        violations_pred, patch, confidence = parse_model_output(model_json)
        # 2) oracle from tools
        oracle = self.oracle  # precomputed for fixture
        # 3) optional patch verification
        post = None
        if patch:
            ok, new_text = try_apply_patch(self.fixture_path, patch)
            if ok:
                # re-run relevant tool(s)
                post_findings = kubelinter_lint([self.fixture_path])  # or semgrep for TF
                post = normalize_findings(post_findings)
        # 4) reward
        r = final_reward(violations_pred, oracle, post_patch=post, had_valid_json=True)
        return { "reward": r, "done": True, "info": {"oracle": oracle, "post": post} }
```

---

## 10) Tests (what to cover)

### 10.1 Unit tests (fast)
- **Schema**: valid/invalid model outputs; partial/extra fields → format bonus/penalty paths.
- **Mapping**: severity & ID mapping tables are stable (snapshot).
- **Patching**: apply unified diff & JSON Patch happy/edge paths; idempotency (re‑apply no‑op).

### 10.2 Integration tests (tools required)
- **OPA**: run on tiny input with a minimal policy that emits 1 high & 1 med finding; assert parsed `ToolFinding` shape.
- **KubeLinter**: run on a purposely insecure Deployment; assert known check IDs and count (golden file).
- **Semgrep (Terraform)**: run on small TF with one obvious misconfig; assert known rule id & severity.

### 10.3 Oracle stability
- For each fixture, regenerate oracle and compare to **golden JSON**. If the diff is non‑empty, fail with an explanation that versions drifted.

### 10.4 End‑to‑end environment
- Given a fixture and a canned **tools‑only** submission (i.e., output equals oracle), reward ≈ **1.0** (plus format bonus).
- Given a patch known to eliminate a high‑severity issue, reward **increases** by `patch_removed_weight * 1.0`.

### 10.5 Error handling
- Timeouts produce clean exceptions; malformed tool JSON raises adapter‑specific errors; uncertain severities map to `'med'` and log warnings.

---

## 11) CI & reproducibility

**GitHub Actions (`ci/e2_verify_rules.yml`)** (outline):
- Matrix on `ubuntu-latest` with Python versions.
- Install pinned tool versions (OPA, KubeLinter, Semgrep). Keep the version pins in one file (`ci/versions.txt`).
- Cache tool downloads & pip deps.
- Run `make e2-test` (unit + integration). Fail if oracle regen differs from golden files.
- Upload artifacts on failure: adapter stdout/stderr, regenerated oracle, and diffs.

**Dockerfile (docker/Dockerfile.e2)** (outline):
- Slim base → install pinned CLIs → add repo → set `ENTRYPOINT make e2-test`.
- Publish image used by CI for determinism.

**Makefile targets:**
```
make e2-setup-tools      # installs pinned CLIs locally or in venv
make e2-test             # run all tests
make e2-regenerate-oracle FIXTURE=dataset/fixtures/k8s/bad_deploy.yaml
make e2-baseline-tools   # emit tools-only predictions for a fixture
```

---

## 12) Baselines & LLM agent usage

### 12.1 Tools‑only baseline
```python
# e2_config_auditing/baselines/tools_only.py
def emit_oracle_as_prediction(fixture_path: str, type_: str) -> dict:
    oracle = build_oracle_for_k8s([fixture_path]) if type_=="k8s" else build_oracle_for_tf([fixture_path])
    return {"violations": oracle, "patch": "", "confidence": 0.99}
```

### 12.2 LLM explainer baseline (no tools)
- Prompt the model with the raw file and ask for likely issues; still must output PRD JSON. Reward should be lower than tools‑only on average.

### 12.3 Agent tool‑use (function calling)
Expose safe tool functions to the agent runtime:
```json
[
  {"name":"run_kubelinter","parameters":{"type":"object","properties":{"paths":{"type":"array","items":{"type":"string"}}},"required":["paths"]}},
  {"name":"run_semgrep","parameters":{"type":"object","properties":{"paths":{"type":"array"}, "rules":{"type":"array","items":{"type":"string"}}},"required":["paths"]}},
  {"name":"run_opa_eval","parameters":{"type":"object","properties":{"input":{"type":"string"}, "policies":{"type":"array","items":{"type":"string"}}, "query":{"type":"string"}},"required":["input","policies"]}}
]
```
**Agent policy:** encourage **at most 2 tool calls** before finalizing; return only PRD JSON fields in the final message.

---

## 13) Acceptance criteria (copy/paste into PR)
- [ ] Adapters execute **OPA/KubeLinter/Semgrep** with pinned versions and parse JSON into `ToolFinding`.
- [ ] Oracle for **K8s** and **Terraform** fixtures is derived from adapters and checked into `dataset/oracle/` with version metadata.
- [ ] Reward includes **post‑patch re‑scan** and credits removed violations by severity.
- [ ] A **tools‑only baseline** reproduces oracle outputs.
- [ ] CI enforces golden files and fails on version drift.
- [ ] README documents tool versions, rulesets, and how to regenerate the oracle.

---

## 14) Appendix A — example version pinning (fill in exact versions centrally)
```
# ci/versions.txt
OPA_VERSION=<<PIN_ME>>
KUBELINTER_VERSION=<<PIN_ME>>
SEMGREP_VERSION=<<PIN_ME>>
PYTHON_VERSION=3.11.9
```
Update once, used by both Makefile and CI.

---

## 15) Appendix B — sample OPA policy contract
```rego
package security

default deny = []

deny[{"id": "k8s/run-as-root", "severity": "high", "message": "container runs as root", "file": input.file, "line": input.line}] {
  input.kind == "Pod"
  input.spec.securityContext.runAsNonRoot == false
}
```

---

## 16) Appendix C — modeling prompts (for LLM devs)
- **System**: “You audit Kubernetes/Terraform configs. Use tools if available. Return only the JSON schema fields. Prefer high‑confidence, high‑severity findings.”
- **Instruction**: “Identify violations by rule ID if possible. If you propose a patch, keep it minimal and valid (unified diff).”
- **Output constraint**: JSON only; match E2 schema.

---

### That’s it
This canvas is designed to be copied into the repo (`e2_config_auditing/README.md`) and implemented module‑by‑module. Start with **adapters** and **oracle**, then **reward + patching**, then **baselines + CI**.
